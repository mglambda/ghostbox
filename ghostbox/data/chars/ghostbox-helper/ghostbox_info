## README.md

```md
# Ghostbox

Do you love comprehensive LLM frameworks? Me neither.

I like things like this:

```python
import ghostbox
box = ghostbox.from_llamacpp(character_folder="unix_philosopher")

with box.options(temperature=0.6, samplers=["min_p", "temperature"]):
    answer = box.text("How do you make developing with LLMs easy and painless?")
    
box.tts_say(answer)
```

You would hear a voice saying "ghostbox". Probably.

## What it is
<a href="https://raw.github.com/mglambda/ghostbox/master/screenshots/terminal.png"><img alt="A screenshot displaying ghostbox used in a terminal." width="300" align="right" src="https://raw.github.com/mglambda/ghostbox/master/screenshots/terminal.png"></a>
Ghostbox is a python library and toolkit for querying LLMs (large language models), both locally hosted and remote. It let's you use AI independent of any particular provider and backend. It wants to make developing applications with tightly integrated AI as painless as possible, without tieing you down to some kind of framework.

Ghostbox ships with the `ghostbox` CLI program, a fully featured terminal client that let's you interact and chat with LLMs from the comfort of your own shell, as well as monitor and debug AIs that are running in your program.

It also includes `ghostbox-tts`, which allows text-to-speech synthesis with various SOTA speech models. This is coupled with the library, but serves as a standalone TTS client in its own right.

I wrote this because I wanted to build stuff with LLMs, while still understanding what's going on under the hood. And also because I wanted an actually good, blind accessible terminal client. Ghostbox is those things.

## Who this is for

If you

 - are a developer who wants to add AI assistance to your application
 - are a developer who wants to add structured AI generated content to your program
 - are anyone who wants a non-trivial terminal client for LLMs
 - are blind and looking for fully accessible software to engage with AI 

ghostbox might be for you.

## Features
<a href="https://raw.github.com/mglambda/ghostbox/master/screenshots/webui.png"><img alt="A screenshot displaying ghostbox used in the Chrome web browser through the Web UI" width="300" align="right" src="https://raw.github.com/mglambda/ghostbox/master/screenshots/webui.png"></a>
 - Generate text, json, or pydantic class instances with LLMs
 - Support for OpenAI, Llama.cpp, Lllama-box, and anyone who supports the OAI API.
 - Interruptible, streaming TTS with orpheus, Kokoro, Zonos, Amazon Polli, and others.
 - Continuous, voice activated transcription with OpenAI's whisper model
 - Include images for multimodal models (OAI and Llama-box only)
 - Create, configure, and switch between AI characters 
 - Tool calling for dummys: Write python functions in tools.py, ghostbox does the rest.
 - Track, save, and reload chat history (with branching, retries, and all the usual features in the CLI)
 - Integrated HTTP webserver using websockets, with a basic web UI that let's you monitor your applications AI while it is running
 - Prompt format templates if you want them (you probably don't, everyone uses Jinja now)
 - Self documenting: Try `ghostbox -cghostbox-helper` to have a friendly chat with an expert on the project
 - Much more. This is a work in progress.


## Examples

### Connecting to backends

```python
import ghostbox

# ghostbox can work with various backends.
# the generic adapter will work with anything that supports the OAI API
# it is the recommended way to make a ghostbox instance
box = ghostbox.from_generic(character_folder="ghost", endpoint="localhost:8080")

# this one is specific to OpenAI
# you don't have to specify the endpoint
cloud_box = ghostbox.from_openai(character_folder="ghost", api_key="...")

# using the llamacpp backend unlocks certain features specific to llama.cpp, e.g. setting your own samplers
# or getting better timing statistics
llama_box = ghostbox.from_llamacpp(
    character_folder="ghost", samplers=["min_p", "dry", "xtc", "temperature"]
)
```

### Using the terminal client

Here's a tiny example of the CLI.

```bash
marius@interlock ghostbox Î» ghostbox -cghost
 # Prompt format template set to 'auto': Formatting is handled server side.
 # Loaded config /home/marius/.config/ghostbox.conf
 # Loaded config /home/marius/prog/ai/ghostbox/ghostbox/data/chars/ghost/config.json
 # Found vars chat_user, system_msg, current_tokens
 # Ok. Loaded /home/marius/prog/ai/ghostbox/ghostbox/data/chars/ghost
 0 ðŸ‘» Write a haiku about cats.
Whiskers twitching soft,
Purring in the moonlight's glow,
Cats rule the night.
 43 ðŸ‘» /time
 # generated: 22, evaluated: 17, cached: 42
 # context: 39 / 32768, exceeded: False
 # 0.48s spent evaluating prompt.
 # 3.75s spent generating.
 # 4.23s total processing time.
 # 5.87T/s, 0.17s/T

 43 ðŸ‘» /set temperature 1.8

 43 ðŸ‘» /retry
 # Now on branch 1
Whiskers twitch softly
Silent hunters in the night
Purring hearts' lullaby
 42 ðŸ‘» 
```

You can do much more with the CLI. Try `/help`, or consult the [full list of commands](https://github.com/mglambda/ghostbox/blob/master/COMMANDS.md).

### Structured Output

Getting structured output for use in applications is fun and easy using [pydantic](https://docs.pydantic.dev/latest/). If you are familiar with the OpenAI python library, you might already know this. Thanks to llama.cpp and its grammar constraints, we can also do structured output locally.

```python
from pydantic import BaseModel
from typing import *
import ghostbox, json

box = ghostbox.from_generic(character_folder="ghost-writer")

# this is the type for the object that we will let the LLM create
# how we name things here really matters
class BlogPost(BaseModel):
    title: str
    content: str
    tags: List[str]


post = box.new(
    BlogPost,  # this tells ghostbox and the backend what the structure should be
    "Write an extremely argumentative post about how an overabundance of busking is ruining berlin.",
)  # the prompt will provide context for filling in the python object
print(json.dumps(post.model_dump(), indent=4))
```

And here's the output (shortened a bit):

```
{
    "title": "Berkelbach on Buskers: When the Music Stops the Music Stops! ",
    "content": "As die hard city dwellers we are always asked for tolerance to what is perceived by outsiders to be  'fleetng and trivial inconveniences'. But there comes a time where we must stand up for a livability standard that was Berlin at one point,  not just Berlin that is . When you come into Alexander Platz on the 15 bus, the first thing that should come to mind isn\u2019t the thought 'What would make my walk home less appealing today?', but, maybe it was!  And we should really ask, to whose detriment and why?   If you think there isn\u2019t a limit on busking or noise ...",
    "tags": [
        "berlin",
        "city-life",
        "music",
        "public-space"
    ]
}
```

As a german I guarantee this is 100% authentic.

### Multimodality (images)

Make sure you use a backend and model that actually supports multimodal inputs. I use Llama-box.

```python
import ghostbox

# assume we have these
image1, image2 = "bard.jpeg", "gnome.jpeg"

box = ghostbox.from_generic(character_folder="art_critic",
                            stderr=False) # don't spam

# we need both images in context before asking the art critic
with box.images([image1, image2]):
    criticism = box.text("Which of these images is more aesthetically pleasing, and why?")

# note that once we exited the context, the images got dropped
# though they remain in the chat history.
# This is usually what you want to not resend the same image over and over, racking up tokens
print(criticism)
```

Here's `art_critic/system_msg`:

```
You are an art critic. You are an expert on art, painting, visual design, photography, and sculpture.
Your opinions on art are nuanced and sophisticated. Your sense of aesthetics is impeccable. Your artistic intuition rivaled only by your hard earned erudition.
When asked your opinion, you react extremely snobbish and arrogantly. You love to criticise bad art in creative ways, and you do so often, and with great intensity.
```

Here's the output (using qwen2-vl): 
```
I am indeed a discerning critic, and these artworks certainly do not meet my standards of artistry and design.

The first image depicts a scene with a character dressed in vibrant medieval attire. However, the composition is chaotic and disjointed, with a lack of depth and coherence in the scene. The characters and objects seem to be placed randomly, lacking any real narrative or sense of space. The color palette, while eye-catching, seems to lack harmony and balance. It seems to lack the subtlety and nuance that I would expect from a well-crafted piece of art.

The second image, featuring a dragon flying over a castle, is equally lacking in sophistication. The use of color is overly bright and garish, lacking subtlety and nuance. The composition is also disjointed and chaotic, with no clear focus or sense of narrative. The dragon, while visually striking, lacks the elegance and grace I would expect from a creature of fantasy.

Overall, these works fail to impress me. They lack the complexity and nuance of truly great art, instead resorting to surface-level appeal and spectacle. They seem to prioritize entertainment and distraction over any deeper exploration or expression of human experience. I would recommend that both artists take the time to develop a deeper understanding of composition, color, and narrative before attempting to create more meaningful work in the future.
```

Ouch. See `examples/image-compare/` for the images, which are indeed somewhat tasteless spawns of stable diffusion. Though most people do kind of like the bard one.

### Tools

To use tools a.k.a. function calling, simply write a python file with your desired functions, call it `tools.py` and place it in the character folder of the character that is supposed to use the tools. For example, with a character folder called `scribe`:

`scribe/tools.py`

```python
# this is /chars/scribe/tools.py
import os, datetime, sys

file = os.path.expanduser("~/scribe_notes.org")

def directly_answer():
    """Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history"""
    return []
    
def take_note(label : str, text : str) -> dict:
    """Take down a note which will be written to a file on the hard disk." 
    :param label: A short label or heading for the note.
    :param text: The note to save"""
    global file
    try:
        if os.path.isfile(file):
            f = open(file, "a")
        else:
            f = open(file, "w")
        f.write("* " + label + "\ndate: " + datetime.datetime.now().isoformat() + "\n" + text + "\n")
        f.close()
    except:
        return { "status": "Couldn't save note.",
                 "error_message" : traceback.format_exc()}
    return {"status" : "Successfully saved note.",
            "note label" : label,
            "note text" : text}

def read_notes() -> dict:
    """Read the users notes."""
    global file
    if not(os.path.isfile(file)):
        return {"status" : "Failed to read notes.",
                 "error_msg" : "File not found."}
    ws = open(file, "r").read().split("\n*")
    d = {"status" : "Successfully read notes."}
    for i in range(len(ws)):
        if ws[i].strip() == "":
            continue
        vs = ws[i].split("\n")
        try:
            note_data = {"label" : vs[0],
                         "date" : vs[1].replace("date: ", "") if vs[1].startswith("date: ") else "",
                         "text" : vs[2:] if vs[1].startswith("date: ") else vs[1:]}
        except:
            print("warning: Syntax error in Scribe notes, offending note: " + ws[i], file=sys.stderr)
            continue
        d["note " + str(i)] = note_data
    return d
```

The above file defines three tools for the AI: `read_notes`, `take_note`, and `directly_answer`.

The note taking tools allow the AI to interact with the filesystem, using the global FILE defined at the top of tools.py. When itneracting with a user, the scribe AI will freely choose which of the tools to apply.

The `directly_answer` tool is a small trick born out of the idiosyncrasies of tool calling: Which tool should the AI call, when it doesn't really need to call a tool? Imagine a user simply says "Hello Scribe, what's up?". That's not really worthy of taking a note, and it's not appropriate to just start reading out notes either. So in such cases, the AI can call the `directly_answer` tool, which will do nothing, and then return control to the AI.

Here is `scribe/system_msg`

```
You are {{chat_ai}}. You help a user to take notes and write down ideas. You have the capability to do so, but you may choose to not write anything down if it's not appropriate or necessary.
```

And `scribe/config.json`

```
{
	"chat_ai" : "Scribe",
	"cli_prompt" : "\n ðŸª¶",
	"cli_prompt_color" : "blue",
	"temperature" : 0.1
}
```

If `tools.py` is found in the character folder, the `use_tools` option is automatically set to True, and ghostbox parses the file, building tool descriptions for the AI from the top level python functions. The tool descriptions offered to the AI will include information from type hints and docstrings, and this can have a big impact on the AI's ability to make good use of the tools, so it's really worth it to pick up your socks when writing tools.py. Start ghostbox with `--verbose` to see the tools that are built.

Tool calling is a very exciting and active area of development and you can expect to see more functionality here from ghostbox in the future. For a more in-depth example of a tool calling AI assistant, try out `chars/butterscotch`. Just beware. Butterscotch has a kind heart but also full shell access. You have been warned.

### Mini Adventure Game

This is `mini-adventure.py`. 

```python
import ghostbox, time, random

# the generic adapter will work with anything that supports the OAI API
box = ghostbox.from_generic(
    character_folder="game_master",  # see below
    stderr=False,  # since this is a CLI program, we don't want clutter
    tts=True,  # start the tts engine
    quiet=False,  # this means generations will be spoken automatically
    tts_model="kokoro",  # kokoro is nice because it's small and good
    tts_voice="bm_daniel",  # daniel is real GM material
)

if name := input("What is your cool adventurer name?\nName: "):
    print(f"Welcome, {name}! A game master will be with you shortly...")
else:
    name = "Drizzt Do'Urden"
    print("Better sharpen your scimitars...")

# this will make {{chat_user}} expand to the adventurer name
box.set_vars({"chat_user": name})

print(
    box.text(
        "Come up with an adventure scenario and give an introduction to the player."
    )
)

# we start conservative, but the adventure will get wilder as we go on
box.temperature = 0.3
escalation_factor = 0.05
while True:
    user_msg = input("Your response (q to quit): ")
    box.tts_stop()  # users usually like it when the tts shuts up after they hit enter

    if user_msg == "q":
        print(
            box.text(
                "{{chat_user}} will quit the game now. Please conclude the adventure and write a proper goodbye."
            )
        )
        break

    with box.options(
        max_length=100
        + 10
        * random.randint(
            -3, 3
        ),  # keep it from talking for too long, but give some variety
    ):
        print(box.text(user_msg))

    box.temperature = min(box.temperature + escalation_factor, 1.3)

# prevent halting of program until the epilogue narration has ended
box.tts_wait()
```

And this would be in `game_master/system_msg`:

```
You are a game master in a role playing game.
You tell a story collaboratively with a user, who is playing as {{chat_user}}.
```

Try it yourself for the output. With most modern models, you'll get a semi-decent, stereotypical adventure story.

Note that this only works because ghostbox is keeping track of the chat history. Ghostbox will also due context shifting when you exceed the `max_context_length`, which is something that can easily happen in RP scenarios.

### Streaming

All of the above examples used blocking calls for simplicity. Ghostbox has asynchronous and streaming variants of (almost) all of its payload methods. Here's a super simple example:

```python
import ghostbox, threading

box = ghostbox.from_generic(
    character_folder="ghost",
    tts=True,  # we want speech output
    tts_model="kokoro",
    tts_voice="am_santa",
    stderr=False,  # it's a CLI program, so we don't want clutter
)

# we will use this flag to signal whem streaming is done
done = threading.Event()


def done_generating(w: str) -> None:
    """Callback for when we are done streaming."""
    global done
    # we could do something with w here
    # it contains the entire generation
    # but it was already printed/spoken, so we're done
    done.set()


# start the actual streaming
box.text_stream(
    "Can you explain the basics of TCP/IP to me? Give both an explain-like-I'm-five version and one where you think I have technical expertise.",
    chunk_callback=lambda w: None,  # do nothing here. Ghostbox will print/speak the chunks
    generation_callback=done_generating,
)  # this will be called when streaming is done
# Now, text is being streamed to stdout and the tts backend simultaneously.
# You can prevent this by setting quiet = True (see also examples/streaming_quiet.py)
# You may notice that text appears sentence-by-sentence, and not token-by-token.
# streaming individual tokens to the tts engine is a bad idea, so ghostbox does some chunking in the background.
# this behaviour is determined by the stream_flush option. It is set to 'sentence' automatically when the tts is active.
# It also determines what will be passed to the chunk_callback. Setting stream_flush to 'token' will give you the most power.

# now we need to keep the program from exiting right away, while streaming is happening
done.wait()

# speaking is usually slower than generation, so we wait for the tts as well
box.tts_wait()
```

### Requirements
#### Backend 
Ghostbox requires an LLM backend. Currently supported backends are
 - [Llama.cpp](https://github.com/ggerganov/llama.cpp)  
 - [KoboldCPP](https://github.com/LostRuins/koboldcpp)
 - [Llama-box](https://github.com/gpustack/llama-box) (use this for multimodality atm)
 - OpenAI (untested)
 - If you have another backend that supports the OpenAI API, feel free to test ghostbox and give me feedback.
 
If you want to run LLM locally, clone your chosen repository, build the project and run the backend server. Make sure you start ghostbox with the correct `--endpoint` parameter for your chosen backend, at least if you run on a non-default endpoint.

If you use a cloud provider, be sure to set your API key with the `--api_key` command line option or do this if you use ghostbox as a library

```python
import ghostbox
box = ghostbox.from_openai(api_key="hF8sk;08xi'mnottellingyoumykeyareyoucrazy")
bot_payload = box.text("Can you create a typical Sam Altman ðŸ”¥ tweet?")
```

#### Feedwater

Ghostbox requires the [feedwater](https://github.com/mglambda/feedwater) python package, which is used to spawn the TTS process. I wrote this myself, and until I get it on PyPI, you will have to do:

```bash
python -m venv env
./env/bin/activate
git clone https://github.com/mglambda/feedwater
cd feedwater
pip install .
```

### Ghostbox Python Package
The repository can be installed as a python package.

```bash
git clone https://github.com/mglambda/ghostbox
cd ghostbox
python -m venv env # skip this if you already did it for feedwater above
. env/bin/activate
pip install .
```

I try to keep the pyproject.toml up-to-date, but the installation might fail due to one or two missing python packages. If you simply `pip install <package>` for every missing package while in the environment created above, ghostbox will eventually install.


This should make both the `ghostbox` and `ghostbox-tts` commands available. Alternatively, they can be found in the `scripts/` directory.

After a successful installation, while a backend is running, do

```bash
ghostbox -cghost
```

to begin text chat with the helpful ðŸ‘» assistant, or try

```bash
ghostbox -cjoshu --tts --audio --hide
```

for an immersive chat with a zen master.


## Additional Resources
### Orpheus TTS

[Orpheus](https://huggingface.co/canopylabs/orpheus-3b-0.1-pretrained) is a new state-of-the-art TTS model with a natural, conversational style, recently released by [Canopy Labs](https://canopylabs.ai/). It talks like a human, can laugh and sniffle and is permissively licensed (apache as of April 2025, though it should be llama3 licensed IMO). It's honestly really cool. To use it with ghostbox, set `tts_model` to `orpheus`. By default ghostbox will then acquire a [4bit quantization](https://huggingface.co/isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF) of the model. This will knock about 2.5 gigs off your vram, with an additional 300 or so MB taken up by the snac decoder. It's worth it though.

Since orpheus is technically an LLM and based on llama3, it needs to be served by an inference engine. `ghostbox-tts` can work with any OpenAI compatible server, but has currently only been tested with llama.cpp. By default, it will try to start the `llama-server` program, so make sure that it is in your path. `ghostbox-tts` will also download other quants, and can be pointed to any gguf file for an orpheus model. See `ghostbox-tts --help` for more.

#### Orpheus Quickstart

1. Make sure llama-server is in your path. You can e.g. do the following, assuming you compiled llama.cpp in `~/llama.cpp/build`

```bash
cd /usr/bin
sudo ln -s ~/llama.cpp/build/bin/llama-server
```

2. Start ghostbox with orpheus

```bash
ghostbox -cghost --tts --tts_model orpheus
```

It will then begin to download the quantized model from huggingface. This only has to be done once. After it's done, you can do `/ttsdebug` to see the output of `ghostbox-tts`. It should look something like this

```bash
 # Initializing orpheus TTS backend.
 # Using device: cuda
 # Spawning LLM server...
 # Considering orpheus model: 

Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 3 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1886.78it/s]
 # Executing `llama-server --port 8181 -c 1024 -ngl 999 -fa -ctk q4_0 -ctv q4_0 --mlock`.
 # Using 'raw' as prompt format template.
 # Loaded config /home/marius/.config/ghostbox.conf
 # Dumping TTS config options. Set them with '/<OPTION> <VALUE>'. /ls to list again.
 #     temperature	0.6
 #     top_p	0.9
 #     repeat_penalty	1.1
 #     max_length	1024
 #     samplers	['penalties', 'min_p', 'temperature']
 #     available_voices	['tara', 'leah', 'jess', 'leo', 'dan', 'mia', 'zac', 'zoe']
 #     special_tags	['<laugh>', '<chuckle>', '<sigh>', '<cough>', '<sniffle>', '<groan>', '<yawn>', '<gasp>']
 #     voice	jess
 #     volume	1.0
 #     start_token_id	128259
 #     end_token_ids	[128009, 128260, 128261, 128257]
 #     custom_token_prefix	<custom_token_
 #     snac_model	hubertsiuzdak/snac_24khz
 #     sample_rate	24000
 #     voices	False
 #     quiet	False
 #     language	en
 #     pause_duration	1
 #     clone	
 #     clone_dir	/home/marius/prog/ai/ghostbox/voices
 #     seed	420
 #     sound_dir	sounds
 #     model	orpheus
 #     output_method	default
 #     websock_host	localhost
 #     websock_port	5052
 #     zonos_model	hybrid
 #     orpheus_quantization	Q4_K_M
 #     orpheus_model	
 #     llm_server	
 #     llm_server_executable	llama-server
 #     filepath	
 # Good to go. Reading messages from standard input. Hint: Type stuff and it will be spoken.
```

And everything should work from there.

#### Cloning with Orpheus

Coming soon!

#### Additional notes on Orpheus

Orpheus is super new and things are a bit volatile. Here's some things to consider:
 - `tara` is the best voice. I found her a bit quiet though, so `ghostbox-tts` boosts her by 25% by default. She unfortunately also has a light reverb, and I hereby pledge to donate 100% of my egg cartons to Canopy Labs, with which they can plaster the walls of their recording studio for a cheap, DIY soundbooth.
 - Realtime streaming with the 4bit quant works. Here's how:
    - According to the orpheus devs, you need~ 80 token/second on the orpheus LLM to do streaming. I have found I needed more like ~100t/s to avoid buffer underruns.
     - The factory settings for orpheus give me around 60t/s with the 4bit quant (that's on an RTX 3090).
     - The `ghostbox-tts` default settings replace the top_p sampler with min_p, which roughly doubles the t/s for me.
     - You can also disable the `penalties` sampler as well, getting a 3x speed boost and dropping the repeat_penalty. However, this will give glitchy results.
     - This way, I get about 160 to 180 t/s with good quality. Being able to set the samplers is one of the main reasons to use llama.cpp
     - I don't know why this works. I haven't observed such strong effects of samplers on generation speeds before. This may be due to the relatively large token count, or I made a mistake somewhere. Idk, DM if you can explain.
 - Orpheus has special conversational tags like `<laugh>` or `<cough>` trained into the model. By default, ghostbox will append these to the system prompt along with instructions if orpheus is being used. You can disable this with the `--no-tts_modify_system_msg` command line option.
 - The ~2.5G vram proclaimed above are only achievable with a tiny context of 1024 tokens and a quantized KV cache (thanks, llama.cpp). This is enough, though. `ghostbox` and `ghostbox-tts` work together to do chunking on the streamed text to feed on average two complete sentences at a time to the orpheus LLM. This tends to stay within the token limit, while also giving the TTS enough semantic context for nuanced generation. Incidentally, this is also roughly the length of inputs the model was trained on, so it's all coming up milhouse.
 
### Kokoro

[Kokoro](https://github.com/hexgrad/kokoro) is a very lightweight (<1GB vram) but high-quality TTS model. It is installed by default alongside the ghostbox package, and can be used with ghostbox-tts.

By default, the kokoro GPU package is installed. If you want to use the CPU only package, then after installing ghostbox, while in the virtual environment, do

```bash
pip install kokoro_onnx[cpu]
```

If you want to use ghostbox-tts alongside GPU acceleration, To ensure kokoro makes use of the GPU and cuda, do

```bash
export ONNX_PROVIDER=CUDAExecutionProvider
```

This is only needed for the standalone TTS, ghostbox itself sets the environment variable automatically.

To see a list of supported voices, consult the kokoro documentation, or do

```bash
ghostbox --tts --tts_model kokoro
/lsvoices
```

The voices listed will depend on the value of `tts_language`. You can use any of this as the ghostbox tts voice, e.g.

```bash
ghostbox -cghost --tts --tts_voice af_sky
```

#### Coming soon
 - Voice mixing with kokoro
 

### Zonos

[Zonos](https://github.com/Zyphra/Zonos) is a large sota TTS model by Zyphra. It's really good, but will knock 2 or 3 gigs off of your vram, so just be aware of that.

Zonos doesn't come with ghostbox by default, because, as of this writing (March 2025), the official packaging seems broken. To install it yourself, just (again, in the ghostbox virtual environment) do

```bash
git clone http://github.com/Zyphra/Zonos
pip install .[compile]
```

The Zonos model architecture comes in two variants: A pure transformer implementation and a transformer-mamba hybrid. The hybrid is generally better (according to my own testing), but requires flash attention. If, for whatever reason, you don't want it, just leave out the `[compile]` optional dependency above.

To use zonos with ghostbox, simply do

```bash
ghostbox -cghost --tts --tts_model zonos 
```

and if you want to use the pure transformer version, change it to

```bash
ghostbox -cghost --tts --tts_model zonos --tts_zonos_model transformer
```

#### Voice Cloning with Zonos

The Zonos TTS model is able to create voices from small audio samples. These only need to be 5 or 10 seconds long. If you use longer samples, the quality may improve, but the embeddings will become prohibitively large. You may have to experiment a bit. To create a sample from a given file called `example.ext`, do

```bash
ffmpeg -i example.ext -t 10 -ac 1 -ar 44100 sample.wav
```

This will create a sample.wav at 44.1kh sampling rate, which seems to be what Zonos wants natively. Ghostbox looks for voices in the current directory and in `tts_voice_dir`, so make sure the sample.wav is in either of those. You can then do

```bash
ghostbox -cghost --tts --tts_model zonos --tts_zonos_model hybrid --tts_voice sample.wav
```

to tie it all together, and have a helpful assistant with a cloned voice of your choice.

### Amazon Polly
*Note: currently defunct as I'm reworking the TTS backends.*

The ghostbox-tts program allows the use of amazon web services (aws) polly voices. To use them, you must create an aws account, which will give you API keys. You can then do (example for arch-linux)

```bash
pacman -S aws
aws configure
```

and you will be asked for your keys. You can then do

```bash
ghostbox -cghost --tts --tts_model polly
```
and ghostbox should talk using AWS. Doing `/lsvoices` will show you the available voices. The polly voices aren't very expressive, but have the advantage of being cloud hosted and so won't hog up your gpu.

### Local User Data

Run
```bash
./scripts/ghostbox-install
```

To create data folders with some example characters in `~/.local/share/ghostbox` and a default config file `~.config/ghostbox.conf`. This isn't necessary, just convenient.



## Character Folders

Ghostbox relates to AIs as characters. That is, the unifying principle of an LLMs operation is conceived of in terms of personality, task, and intent. This is regardless of wether you want to use your LLM to monitor heavy-duty network traffic, or to be a friendly customer support clerk, and it is irrespective of anyone's opinions on consciousness or AGI or whatever. It is a conceptual crutch, and it works well as such.

In this sense, any generation you make with ghostbox will be in the context of an AI character. You can define an AI character through a character folder, which is an actual directory on your hard drive.

A character folder may contain arbitrary files, but the following ones will be treated as special:
 - `system_msg`: The system prompt sent to the backend with every generation. This defines the personality and intent of the character.
 - `config.json`: A file containing one json dictionary, with key/value pairs being ghostbox options and their respective values. This can be used to set options particular to the character, such as tts_voice, text color, temperature and so on.
 - `tools.py`: A python file containing arbitrary python code, with all top-level functions that do not start with an underscore being taken as tools for this character. If this file is found, `use_tools` is automatically enabled for the character.
 - `initial_msg`: Deprecated. An initial message that is sent to the user on startup and prepended to the chat history with `assistant` role. This used to be a great way to give the LLM some initial color and style, but I'm deprecating it because many newer models break without the first message being from the user.

Any other file that is found in the character folder will be loaded as a file variable, with the name of the variable being the filename, and its content being the file content.

### Using character folders
Ghostbox expects character folders in the following places
 - With the `-c` or `--character_folder` command line argument
 - As the `character_folder` argument to any of the various factory functions, like `ghostbox.from_generic` or `ghostbox.from_llamacpp`
 - As an argument to the `/start` command in the terminal program
 - As an argument to the `start_session` api method, which can let you switch a character while keeping the history constant (see also `/switch`). 
 - various other places
 
Ghostbox will the nlook for character folders in the following places:
 - The current working directory
 - Whatever valid directory paths are set in the `include` option, in order. By default, these are
     - A platform specific ghostbox Application directory location, e.g. `~/.local/share/ghostbox/chars/`
     - A char dir in the ghostbox python package. This contains a handful of built-in characters (like `ghost`). Its location depends on your package manager, but will be something like `env/lib/python/site-packages/ghostbox/data/chars/`.
     - A directory `chars/` in the current working directory.

You can append to the include paths by using the `--include` command line argument.

### File Variables
As mentioned above, all files in a character folder become file variables.

File variables are expanded in text that is sent to the LLM backend. For example, if you have a `system_msg` like this

```
You are Sherlock. You help a user to solve difficult crime mysteries.
The crime you and the user want to solve today is the following:
{{crime.txt}}
```

and you have a file `crime.txt` in the character folder that looks like this

```
Things that are missing:
  - my coffee
  - where is it
```

What the LLM will get as system prompt is this
```
You are Sherlock. You help a user to solve difficult crime mysteries.
The crime you and the user want to solve today is the following:
Things that are missing:
  - my coffee
  - where is it
```

You can list variables like this at the CLI with `/lsvars`, or set them in the API with `set_vars`. Variable expansion is recursive, but stops at a depth of three.

### Dynamic File Variables

Normal file variables are loaded once upon character folder initialization and then expanded/substituted with the same content throughout the execution of the program, unless they are manually reset.

A dynamic file variable is loaded ad-hoc and its contents reloaded from its file everytime it is expanded. Dynamic file variables use square brackets within curly braces, like this
```
{[some_file.py]}
```

You can use this for great convenience at the CLI, e.g.

```bash
 420 ðŸ‘»> Below is a readme file for a small LLM toolkit and library called 'ghostbox'. Can you give me som feedback on it?\
 \
 {[README.md]}
```

In this case, the `{[README.md]}` expression would be expanded into the content of this very file (woah). Incidentally, the backslashes above are used to enter a newline at the CLI.

Note that although they are useful, for security reasons, dynamic file variables are disabled for every other input method except the terminal program. Think putting `{[/etc/passwd]}` deep in some python code and other such skullduggery.

## Tool Calling Guide

Coming soon!

## Options

Ghostbox uses options extensively. See the full list of options and their documentation [here](https://github.com/mglambda/ghostbox/blob/master/OPTIONS.md). An option is something like "temperature" or "backend".

Options can be used in the following places: 
 - command line arguments to the `ghostbox` program, e.g. `--temperature 1.3` or `--tts_voice af_sky`
 - `config.json` files in character folders, which contain one json dictionary that has options as key/value pairs
 - User config files, which also contain json
 - As a property on a ghostbox instance, e.g. `box.temperature = 1.0`
 - Parameters to API functions, including
     - The `from_*` factory functions, e.g. `box = from_llamacpp(temperature=1.3, tts_voice="af_sky")`.
     - The `options` context manager, e.g. `with box.options(temperature=1.3, tts_voice="af_sky"):`
     - The `Ghostbox` constructor
     - as `**options` parameter in many method signatures
 - The `/set` command in the CLI, e.g. `/set temperature 1.3` or `/set tts_voice "af_sky"`. List these with `/lsoptions`

Across these different uses for options, the naming is always consistent.

### Most useful options

Coming soon!

### Services

Setting some options to `True` has side effects, and may start services that run on seperate threads. Those options and their respective services are:

| Option | Effect when set to True |
| --- | ---- |
| tts | Starts a seperate tts process. Which program to start depends on the value of `tts_program`. The default is `ghostbox-tts`. If this is running, it will automatically speak generations, except when `quiet` is true. |
| audio | Begins automatic audio transcription. In the CLI, you can pause the transcription with CTRL+C. This is highly configurable with the various `audio_*` options, including silence threshold and activation phrase. |
| image_watch | Starts a service that watches a particular directory for new image files. When a new file appears, it gets send to the LLM for description. By default, this watches your platforms screenshot directory. |
| http | Starts a simple HTTP web server, serving the web UI at `http_host` with port `http_port`. That's `http://localhost:5050` by default. |
| websock | Starts a websock server that sends out LLM generations and listens for input. After the initial HTTP handshake, this behaves almost exactly like stdout/stdin. This is used by the web UI. |

## Further Documentation

 -  [Full list of options](https://github.com/mglambda/ghostbox/blob/master/OPTIONS.md).
 - [Full list of CLI commands](https://github.com/mglambda/ghostbox/blob/master/COMMANDS.md)
 See the `doc/` directory for extensive API documentation.
 - The `examples/` folder has many in-depth examples, as does the `tests/`. folder.
 -  If anything is still unclear, just ask the `ghostbox-helper` AI about it. I'm only half joking.

## Credits and Contributing

Thanks to the excellent [r/localllama](https://www.reddit.com/r/locallama/), and to all those who have contributed there, either through code or expertise. Thanks to the people at [llama.cpp](https://github.com/ggml-org/llama.cpp) for making local LLMs a reality, and thanks to [KoboldCPP](https://github.com/LostRuins/koboldcpp) for also making them accessible. Speaking of kobolds, you should also check out their [Kobold AI horde](https://horde.koboldai.net/).

Thanks to

https://github.com/isaiahbjork/orpheus-tts-local

for figuring out decoding of orpheus tts tokens.


The number one way to contribut to ghostbox is to test it out and give me feedback. Either by opening github issues or just telling me what kind of features you actually use or want to see in the future. I wrote this in order to build cool stuff with LLMs, and then published in partly in hopes of inspiring others to do the same, so seeing people build stuff with it is always great.
```

## LICENSE

```
                   GNU LESSER GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.


  This version of the GNU Lesser General Public License incorporates
the terms and conditions of version 3 of the GNU General Public
License, supplemented by the additional permissions listed below.

  0. Additional Definitions.

  As used herein, "this License" refers to version 3 of the GNU Lesser
General Public License, and the "GNU GPL" refers to version 3 of the GNU
General Public License.

  "The Library" refers to a covered work governed by this License,
other than an Application or a Combined Work as defined below.

  An "Application" is any work that makes use of an interface provided
by the Library, but which is not otherwise based on the Library.
Defining a subclass of a class defined by the Library is deemed a mode
of using an interface provided by the Library.

  A "Combined Work" is a work produced by combining or linking an
Application with the Library.  The particular version of the Library
with which the Combined Work was made is also called the "Linked
Version".

  The "Minimal Corresponding Source" for a Combined Work means the
Corresponding Source for the Combined Work, excluding any source code
for portions of the Combined Work that, considered in isolation, are
based on the Application, and not on the Linked Version.

  The "Corresponding Application Code" for a Combined Work means the
object code and/or source code for the Application, including any data
and utility programs needed for reproducing the Combined Work from the
Application, but excluding the System Libraries of the Combined Work.

  1. Exception to Section 3 of the GNU GPL.

  You may convey a covered work under sections 3 and 4 of this License
without being bound by section 3 of the GNU GPL.

  2. Conveying Modified Versions.

  If you modify a copy of the Library, and, in your modifications, a
facility refers to a function or data to be supplied by an Application
that uses the facility (other than as an argument passed when the
facility is invoked), then you may convey a copy of the modified
version:

   a) under this License, provided that you make a good faith effort to
   ensure that, in the event an Application does not supply the
   function or data, the facility still operates, and performs
   whatever part of its purpose remains meaningful, or

   b) under the GNU GPL, with none of the additional permissions of
   this License applicable to that copy.

  3. Object Code Incorporating Material from Library Header Files.

  The object code form of an Application may incorporate material from
a header file that is part of the Library.  You may convey such object
code under terms of your choice, provided that, if the incorporated
material is not limited to numerical parameters, data structure
layouts and accessors, or small macros, inline functions and templates
(ten or fewer lines in length), you do both of the following:

   a) Give prominent notice with each copy of the object code that the
   Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the object code with a copy of the GNU GPL and this license
   document.

  4. Combined Works.

  You may convey a Combined Work under terms of your choice that,
taken together, effectively do not restrict modification of the
portions of the Library contained in the Combined Work and reverse
engineering for debugging such modifications, if you also do each of
the following:

   a) Give prominent notice with each copy of the Combined Work that
   the Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the Combined Work with a copy of the GNU GPL and this license
   document.

   c) For a Combined Work that displays copyright notices during
   execution, include the copyright notice for the Library among
   these notices, as well as a reference directing the user to the
   copies of the GNU GPL and this license document.

   d) Do one of the following:

       0) Convey the Minimal Corresponding Source under the terms of this
       License, and the Corresponding Application Code in a form
       suitable for, and under terms that permit, the user to
       recombine or relink the Application with a modified version of
       the Linked Version to produce a modified Combined Work, in the
       manner specified by section 6 of the GNU GPL for conveying
       Corresponding Source.

       1) Use a suitable shared library mechanism for linking with the
       Library.  A suitable mechanism is one that (a) uses at run time
       a copy of the Library already present on the user's computer
       system, and (b) will operate properly with a modified version
       of the Library that is interface-compatible with the Linked
       Version.

   e) Provide Installation Information, but only if you would otherwise
   be required to provide such information under section 6 of the
   GNU GPL, and only to the extent that such information is
   necessary to install and execute a modified version of the
   Combined Work produced by recombining or relinking the
   Application with a modified version of the Linked Version. (If
   you use option 4d0, the Installation Information must accompany
   the Minimal Corresponding Source and Corresponding Application
   Code. If you use option 4d1, you must provide the Installation
   Information in the manner specified by section 6 of the GNU GPL
   for conveying Corresponding Source.)

  5. Combined Libraries.

  You may place library facilities that are a work based on the
Library side by side in a single library together with other library
facilities that are not Applications and are not covered by this
License, and convey such a combined library under terms of your
choice, if you do both of the following:

   a) Accompany the combined library with a copy of the same work based
   on the Library, uncombined with any other library facilities,
   conveyed under the terms of this License.

   b) Give prominent notice with the combined library that part of it
   is a work based on the Library, and explaining where to find the
   accompanying uncombined form of the same work.

  6. Revised Versions of the GNU Lesser General Public License.

  The Free Software Foundation may publish revised and/or new versions
of the GNU Lesser General Public License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns.

  Each version is given a distinguishing version number. If the
Library as you received it specifies that a certain numbered version
of the GNU Lesser General Public License "or any later version"
applies to it, you have the option of following the terms and
conditions either of that published version or of any later version
published by the Free Software Foundation. If the Library as you
received it does not specify a version number of the GNU Lesser
General Public License, you may choose any version of the GNU Lesser
General Public License ever published by the Free Software Foundation.

  If the Library as you received it specifies that a proxy can decide
whether future versions of the GNU Lesser General Public License shall
apply, that proxy's public statement of acceptance of any version is
permanent authorization for you to choose that version for the
Library.

```

## pyproject.toml

```toml
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "ghostbox"
version = '0.22.0'
description = "Ghostbox is a command line interface to local LLM (large language model) server applications, such as koboldcpp or llama.cpp. It's primary purpose is to give a simple, stripped-down way to engage with AI chat bots."
authors = [
    { name = "Marius Gerdes", email = "integr@gmail.com" }
]
readme = "README.md"
license = { file = "LICENSE" }
dependencies = [
    "requests",
    "boto3",
    "appdirs",
    "lazy-object-proxy",
    "openai-whisper",
    "pyaudio",
    "pydub",
    "colorama",
    "automodinit",
    "deepspeed",
    "docstring_parser",
    "jsonpickle",
    "shutils",
    "nltk",
    "huggingface-hub",
    "wget",
    "kokoro_onnx[gpu]",
    "websockets",
    "duckduckgo-search",
    "markdownify",
    "h2",
    "snac",
    "pyperclip"
]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: GNU General Public License v2 (GPLv2)",
    "Operating System :: OS Independent",
]
requires-python = ">=3.7"

[tool.setuptools]
packages = ["ghostbox"]
package-data = { "ghostbox" = ["data/**"] }
script-files = ["scripts/ghostbox", "scripts/ghostbox-tts"]

[project.urls]
"Homepage" = "https://github.com/mglambda/ghostbox"

```

## agency.py

```python
# allows for use of tools with tools.py in char directory.

import os, importlib, inspect, docstring_parser, json, re, traceback
from pydantic import BaseModel
from ghostbox.util import *
from typing import *
from ghostbox.definitions import *

# # example of tool dict
# "tools": [
#     {
#     "type":"function",
#     "function":{
#         "name":"python",
#         "description":"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.",
#         "parameters":{
#         "type":"object",
#         "properties":{
#             "code":{
#             "type":"string",
#             "description":"The code to run in the ipython interpreter."
#             }
#         },
#         "required":["code"]
#         }
#     }
#     }
# ],

def type_to_json_schema(type_):
    type_map = {
        int: {"type": "integer"},
        float: {"type": "number"},
        str: {"type": "string"},
        bool: {"type": "boolean"},
        list: {"type": "array", "items": {}},  # You might need to specify the item type
        dict: {"type": "object"},
        type(None): {"type": "null"}
    }
    if type_ in type_map:
        return type_map[type_]
    elif hasattr(type_, "__origin__"):
        if type_.__origin__ is list:
            return {"type": "array", "items": type_to_json_schema(type_.__args__[0])}
        elif type_.__origin__ is dict:
            return {"type": "object", "additionalProperties": type_to_json_schema(type_.__args__[1])}
        elif type_.__origin__ is Optional:
            return {"oneOf": [type_to_json_schema(type_.__args__[0]), {"type": "null"}]}
    return {}


def makeTools(filepath, display_name="tmp_python_module", tools_forbidden=[]) -> Tuple[List[Tool], Any]:
    """Reads a python file and returns a pair with all the top level functions parsed as tools, and the corresponding module for the file."""
    if not(os.path.isfile(filepath)):
        printerr("warning: Failed to generate tool dictionary for '" + filepath + "' file not found.")
        return ({}, None)


    tools = []
    spec = importlib.util.spec_from_file_location(display_name, filepath)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    for name, value in vars(module).items():
        if name.startswith("_") or not callable(value) or name in tools_forbidden:
            continue
        doc = inspect.getdoc(value)
        if doc is None:
            printerr("error: Missing docstring in function '" + name + "' in file '" + filepath + "'. Aborting tool generation.")
            return ({}, None)
        fulldoc = docstring_parser.parse(doc)
        if fulldoc.description is None:
            printerr("warning: Missing description in function '" + name + "' in file '" + filepath + "'. Please make sure you adhere to standard python documentation syntax.")
            description = doc
        else:
            description = fulldoc.description


        sig = inspect.signature(value)
        paramdocs = {p.arg_name : {"type" : p.type_name, "description" : p.description, "optional" : p.is_optional} for p in fulldoc.params}
        properties = {}
        required_params = []
        for (param_name, param) in sig.parameters.items():
            if param.annotation == inspect._empty:
                printerr("warning: Missing type annotations for function '" + name + "' and parameter '" + param_name + "' in '" + filepath + "'. This will significantly degrade AI tool use performance.")
                # default to str
                param_type = "str"
            else:
                param_type = param.annotation.__name__

            # defaults
            param_description = ""
            param_required = True
            if param_name not in paramdocs:
                printerr("warning: Missing documentation for parameter '" + param_name + "' in function '" + name + "' in '" + filepath + "'. This will significantly degrade AI tool use performance.")
            else:
                p = paramdocs[param_name]
                if p["description"] is None:
                    printerr("warning: Missing description for parameter '" + param_name + "' in function '" + name + "' in '" + filepath + "'. This will significantly degrade AI tool use performance.")
                else:
                    param_description = p["description"]

                #if p["type"] != param_type:
                    #printerr("warning: Erroneous type documentation for parameter '" + param_name + "' in function '" + name + "' in '" + filepath + "'. Stated type does not match function annotation. This will significantly degrade AI tool use performance.")

                if p["optional"] is not None:
                    param_required = not(p["optional"])

            # finally set the payload
            if param_required:
                required_params.append(param_name)
            properties[param_name] = Property(type=type_to_json_schema(param_type).get("type", "string"),
                                              description=param_description)


        parameters = Parameters(required=required_params,
                                properties=properties)
        new_function = Function(name=name,
                                    description=description,
                                    parameters=parameters)
        if new_function.name not in tools_forbidden:
            tools.append(Tool(function=new_function))



    return (tools, module)

def tryParseToolUse(w, start_string = "```json", end_string = "```", magic_word="Action:"):
    """Process AI output to see if tool use is requested. Returns a dictionary which is {} if parse failed, and the input string with json removed on a successful parse.
    :param w: The input string, e.g. AI generated response.
    :param predicate: Optional boolean filter function which takes tool names as input.
    :return: A pair of (list(dict), str), with the parsed json and the input string with json removed if parse succeeded."""
    m = re.match(magic_word + ".*" + start_string + "(.*)" + end_string + ".*", w, flags=re.DOTALL)
    if not(m):
        return {}, w

    try:
        capture = m.groups(1)[-1]
        tools_requested = json.loads(capture)
    except:
        printerr("warning: Exception while trying to parse AI tool use.\n```" + w + "```")
        printerr(traceback.format_exc())
        return {}, w


    if type(tools_requested) != list:
        printerr("warning: Wrong type of tool request. Parse succeeded but no tool application possible.")
        printerr("Dump: \n" + json.dumps(tools_requested, indent=4))
        
    # parse succeeded, clean the input
    w_clean = w.replace(start_string + capture + end_string, "").replace(magic_word, "")
    return (tools_requested, w_clean)


def tryParseAllowedToolUse(w : str,
                           tools_allowed : dict):
    return tryParseToolUse(w, predicate=lambda tool_name: tool_name in allowed_tools.keys())


def getPositionalArguments(func):
    return [param.name for (k, param) in inspect.signature(func).parameters.items() if param.default == inspect._empty]

def getOptionalArguments(func):
    return [param.name for (k, param) in inspect.signature(func).parameters.items() if param.default != inspect._empty]

def makeToolResult(tool_name, result, tool_call_id) -> ChatMessage:
    """Packages a tool call result as a ChatMessage."""
    return ChatMessage(
        role="tool",
        tool_name= tool_name,
        tool_call_id= tool_call_id,
        content=str(result))
            


def makeToolSystemMsg(tools):
    """Deprecated"""
    w = ""
    w += "    ## Available Tools\nHere is a list of tools that you have available to you:\n\n"
    w += json.dumps(tools, indent= 4)
    w += "\n\n"
    return w


def makeToolInstructionMsg():
    """Deprecated"""
    #FIXME: this is currently designed only for command-r, other llms will use different special tokens, for which we have to extend the templating, probably iwth tool_begin and tool_end 
    w = """<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>
Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:  

Action:```json
[
    {
        "tool_name": title of the tool in the specification,
        "parameters": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters
    }
]
```

<|END_OF_TURN_TOKEN|>"""
    return w


def showToolResult(tool_result, indent=0):
    """Takes a tool result of any type and returns a string that can be passed to an AI. Contains no special tokens. Expects whatever is in the 'output' field of the tool use dictionary. If tool_result is a list or dictionary, this function will be recursively appplied."""
    x = tool_result
    pad = " " * indent
    if type(x) == type(None):
        return pad + "output: None\n"
    elif type(x) == str:
        return pad + x + "\n"
    elif type(x) == list:
        if x == []:
            # FIXME: not sure what's best here
            return "[]\n"
        return (pad + "\n").join([showToolResult(y, indent=indent) for y in x])
    elif type(x) == dict:
        return (pad + "\n").join([k + ": " + showToolResult(v, indent=indent+4) for (k, v) in x.items()])
    # default to json. if you pass something that isn't json serializable to the AI, we crash and it's your own fault
    # FIXME: also this won't respect indent. maybe that's ok
    try:
        w = json.dumps(x, indent=4)
    except:
        printerr("warning: Couldn't show the result of a tool call.\nHere's the result dump:\n" + str(x) + "\n and here's the traceback:\n")
        printerr(traceback.format_exc())
        return ""
    return w
    

```

## api_internal.py

```python
import os, datetime, glob, sys, requests, traceback, random, json
from typing import *
from ghostbox.session import Session
from ghostbox.util import ultraglob, printerr

def start_session(plumbing, filepath: str, keep=False) -> str:
    
    allpaths = [filepath] + [p + "/" + filepath for p in plumbing.getOption("include")]    
    for path in allpaths:
        path = os.path.normpath(path)
        failure = False
        try:
            s = Session(dir=path, chat_user=plumbing.getOption("chat_user"), chat_ai=plumbing.getOption("chat_ai"), additional_keys=plumbing.getOption("var_file"), tools_forbidden=plumbing.getOption("tools_forbidden"))
            break
        except FileNotFoundError as e:
            # session will throw if path is not valid, we keep going through the includes
            failure = e


    if failure:
        return "error: " + str(failure)

    # constructing new session worked
    if not(keep):
        plumbing.session = s
    else:
        # something like /switch happened, we want to keep some stuff
        plumbing.session.merge(s)

    w = ""
    # try to load config.json if present
    configpath = path + "/config.json"
    if os.path.isfile(configpath):
        w += load_config(plumbing, configpath, override=False) + "\n"
    plumbing.options["character_folder"] = path

    # this might be very useful for people to debug their chars, so we are a bit verbose here by default
    w += "Found vars " + ", ".join([k for k in plumbing.session.getVars().keys() if k not in Session.special_files]) + "\n"


    # by convention, the initial message is stored in initial_msg
    if plumbing.session.hasVar("initial_msg") and not(keep):
        plumbing.initial_print_flag = True

    # enable tools if any are found
    if plumbing.session.tools:
        plumbing.setOption("use_tools", True)
        w += "Tool dictionary generated from tools.py, setting use_tools to True. Beware, the AI will now call functions.\n"
        if (callback := plumbing.getOption("tools_inject_dependency_function")):
            if plumbing.session.tools_module             is not None:
                callback(plumbing.session.tools_module            )

        if plumbing.getOption("tools_inject_ghostbox"):
            if plumbing.session.tools_module             is not None:
                plumbing.session.tools_module._ghostbox_plumbing = plumbing

            
        if plumbing.getOption("verbose"):
            w += "Dumping tool dictionary. Run with --no-verbose to disable this."
            w += json.dumps([tool.model_dump() for tool in plumbing.session.tools], indent=4) + "\n"
        else:
            w += "AI tools: " + ", ".join([t.function.name for t in plumbing.session.tools]) + "\n"


    # hide if option is set
    if plumbing.getOption("hide"):
        hide_some_output(plumbing)

    w += "Ok. Loaded " + path
    return w


def load_config(plumbing, filepath, override=True, protected_keys: List[str]=[]) -> str:
    try:
        w = open(filepath, "r").read()
    except Exception as e:
        return str(e)
    err = plumbing.loadConfig(w, override=override, protected_keys=protected_keys)
    if err:
        return err
    return "Loaded config " + filepath
    


def hide_some_output(plumbing):
    plumbing.options["cli_prompt"] = "\n"
    plumbing.options["audio_show_transcript"] = False
    plumbing.options["tts_subtitles"] = False
    #plumbing.options["stream"] = False
    plumbing.options["chat_show_ai_prompt"] = False
    plumbing.options["color"] = False
    

def toggle_tts(plumbing) -> str:
    prog = plumbing
    prog.options["tts"] = not(prog.options["tts"])
    w = ""
    if prog.options["tts"]:
        err = prog.initializeTTS()
        if err:
            return err
        w += "Try /hide for a more immersive experience.\n"
        prog.setOption("stream_flush", "sentence")
    else:
        # disabled tts
        prog.tts.close()
        prog.tts = None
    return w + "TTS " + {True : "on.", False : "off."}[prog.options["tts"]]
    


def all_chars(prog) -> List[str]:
    allchars = []
    for dir in prog.getOption("include"):
        if not(os.path.isdir(dir)):
            if prog.getOption("verbose"):
                printerr("warning: Include path '" + dir + "' is not a directory.")
            continue
        for charpath in glob.glob(dir + "/*"):
            if os.path.isfile(charpath):
                continue
            allchars.append(os.path.split(charpath)[1])

    return allchars

```

## api.py

```python
from __future__ import annotations
import traceback
from pydantic import BaseModel, ValidationError
from typing import *
from dataclasses import dataclass
from contextlib import contextmanager
from typing import Callable, Dict
from typing_extensions import Self
import json, time
from ghostbox.main import Plumbing, setup_plumbing
from ghostbox.StoryFolder import StoryFolder
from ghostbox._argparse import makeDefaultOptions
from ghostbox.util import printerr
from ghostbox import commands
from ghostbox.definitions import *
from ghostbox import definitions
from ghostbox.api_internal import *
from ghostbox.agency import Tool, Function, Property, Parameters


def from_generic(endpoint="http://localhost:8080", **kwargs):
    """Returns a Ghostbox instance that connects to an OpenAI API compatible endpoint.
    This generic backend adapter works with many backends, including llama.cpp, llama-box, ollama, as well as online providers, like OpenAI, Anthropic, etc. However, to use features specific to a given backend, that are not part of the OpenAI API, you may need to use a more specific backend.
    Note: Expects ENDPOINT to serve /v1/chat/completions and similar, so e.g. http://localhost:8080/v1/chat/completions should be reachable.
    """
    return Ghostbox(backend=LLMBackend.generic, endpoint=endpoint, **kwargs)


def from_openai_legacy(endpoint="http://localhost:8080", **kwargs):
    """Returns a Ghostbox instance that connects to an OpenAI API compatible endpoint using the legacy /v1/completions interface.
    This generic backend adapter works with many backends, including llama.cpp, llama-box, ollama, as well as online providers, like OpenAI, etc. However, to use features specific to a given backend, that are not part of the OpenAI API, you may need to use a more specific backend.
    Note: There is usually no reason to use this over the generic variant."""
    return Ghostbox(backend=LLMBackend.legacy, endpoint=endpoint, **kwargs)


def from_llamacpp(endpoint="http://localhost:8080", **kwargs):
    """Returns a Ghostbox instance bound to the formidable LLama.cpp. See https://github.com/ggml-org/llama.cpp .
    This uses endpoints described in the llama-server documentation, and will make use of Llama.cpp specific features.
    """
    return Ghostbox(backend=LLMBackend.llamacpp, endpoint=endpoint, **kwargs)


# FIXME: temporarily disabled due to being untested
# ndef from_koboldcpp(endpoint="http://localhost:5001", **kwargs):
#    return Ghostbox(backend="llama.cpp", endpoint=endpoint, **kwargs)


def from_openai_official():
    """Returns a Ghostbox instance that connects to the illustrious OpenAI API at their official servers.
    The endpoint is hardcoded for this one. Use the 'generic' backend to connect to arbitrary URLs using the OpenAI API.
    """
    return Ghostbox(backend=LLMBackend.openai, **kwargs)


class Ghostbox:
    def __init__(self, endpoint: str, backend: LLMBackend, **kwargs):
        self._ct = None  # continuous transcriber
        kwargs["endpoint"] = endpoint
        kwargs["backend"] = backend.name

        default_options, tags = makeDefaultOptions()
        options = (
            {
                k: v
                for k, v in default_options.__dict__.items()
                if not (k.startswith("_"))
            }
            | definitions.api_default_options
            | kwargs
        )
        self.__dict__ |= options
        self.__dict__["_plumbing"] = Plumbing(
            options=options,
            tags=tags,
        )

        if self.config_file:
            load_config(self._plumbing, self.config_file, protected_keys=kwargs.keys())
        setup_plumbing(self._plumbing, protected_keys=kwargs.keys())

        # for arcane reasons we must startthe tts after everything else
        if self._plumbing.tts_flag:
            self._plumbing.tts_flag = False
            self._plumbing.options["tts"] = False
            printerr(toggle_tts(self._plumbing))

    @contextmanager
    def options(self, **kwargs):
        # copy old values
        tmp = {k: v for (k, v) in self._plumbing.options.items() if k in kwargs}
        # this has to be done one by one as setoptions has sideffects
        for new_k, new_v in kwargs.items():
            self._plumbing.setOption(new_k, new_v)
        yield self
        # now unwind, also one by one
        for old_k, old_v in tmp.items():
            self._plumbing.setOption(old_k, old_v)

    @contextmanager
    def option(self, name, value):
        with self.options({name: value}):
            yield self

    def set(option_name: str, value) -> None:
        if option_name in self.__dict__:
            self.__dict__[option_name] = value
        self._plumbing.setOption(option_name, value)

    def get(self, option_name: str) -> object:
        return self._plumbing.getOption(option_name)

    def set_vars(self, injections: Dict[str, str]) -> Self:
        for k, v in injections.items():
            self._plumbing.session.setVar(k, v)
        return self

    def __getattr__(self, k):
        # we intentionally avoid getOption because we want the api to crash if k not found
        return self.__dict__["_plumbing"].options[k]

    def __setattr__(self, k, v):
        if k == "_plumbing":
            return

        if k in self.__dict__:
            self.__dict__[k] = v

        if "_plumbing" in self.__dict__:
            self.__dict__["_plumbing"].setOption(k, v)

    # diagnostics
    def is_busy(self) -> bool:
        """Returns true if an interaction with a backend is currently in progress.
        While busy, all changes to the state of options (e.g. via set or the options context manager) will be buffered and applied when the ghostbox is no longer busy.
        """
        return self._plumbing._frozen

    def timings(self) -> Optional[Timings]:
        """Provides timing and usage statistics for the last request to the backend.
        Returns none if no timings are available, which may happen if either no request has been sent yet, or the backend doesn't support timing.
        :return: Either None or an object with timing statistics.
        """
        return self._plumbing.getBackend().timings()

    # these are the payload functions
    def text(
        self,
        prompt_text: str,
        timeout: Optional[float] = None,
        options: Dict[str, Any] = {},
    ) -> str:
        """Generate text based on a prompt.
        This function blocks until either the generation finishes, or a provided timeout is reached.
        :param prompt_text: The prompt given to the LLM backend.
        :param timeout: Number of seconds to wait before the generation is canceled.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: A string that was generated by the backend, based on the provided prompt.
        """
        with self.options(stream=False, **options):
            return self._plumbing.interactBlocking(prompt_text, timeout=timeout)

    def text_async(
        self,
        prompt_text: str,
        callback: Callable[[str], None],
        options: Dict[str, Any] = {},
    ) -> None:
        """Generate text based on a prompt asynchronously.
        This function does not block. It returns immediately and calls the provided callback when the generation finishes.
        :param prompt_text: The prompt given to the LLM backend.
        :param callback: A function that accepts a string. This will be called when the generation finishes, with the generated string as the only parameter.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: The ghostbox instance.
        """
        with self.options(stream=False, **options):
            # FIXME: this is tricky as we immediately return and set stream = True again ??? what to do
            self._plumbing.interact(prompt_text, user_generation_callback=callback)
        return

    def text_stream(
        self,
        prompt_text: str,
        chunk_callback: Callable[[str], None],
        generation_callback: Callable[[str], None] = lambda x: None,
        options: Dict[str, Any] = {},
    ) -> None:
        """Generate text based on a prompt and stream the response.
        This function does not block. It returns immediately and invokes the provided callbacks when their respective events procure.
        :param prompt_text: The prompt given to the LLM backend.
        :param chunk_callback: This is called for each token that the backend generates, with the token as sole parameter.
        :param generation_callback: This is called exactly once, upon completion of the response, with the entire generation as sole parameter. If you are thinking of concatenating all the tokens in chunk_callback, use this instead.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: The ghostbox instance.
        """
        with self.options(stream=True, **options):
            self._plumbing.interact(
                prompt_text,
                user_generation_callback=generation_callback,
                stream_callback=chunk_callback,
            )
        return

    @staticmethod
    def _make_json_schema(schema: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        if schema is None:
            return {"type": "json_object"}
        return {"type": "json_object", "schema": schema}

    def json(
        self,
        prompt_text: str,
        schema: Optional[Dict] = None,
        timeout: Optional[float] = None,
        options: Dict[str, Any] = {},
    ) -> str:
        """Given a prompt, returns structured output as a string that is json deserializable.
        Output is structured but somewhat unpredictable, unless you provide a json schema. If you are thinking about using pydantic objects and using their model_json_schema method, consider using the ghostbox.new method directly.
        :param prompt_text: The prompt text as natural language.
        :param schema: A dict representing a json schema, which will further restrict the generation.
        :param timeout: Number of seconds to wait before generation is canceled.
        :param options: Additional options that will be passed to the backend, e.g. `{"min_p": 0.01}`.
        :return: A string that contains json, hopefully adequately satisfying the prompt.
        Example use:
        noises = json.loads(box.json("Can you list some animal noises? Please give key/value pairs."))
        noises is now e.g. {"dog": "woof", "cat":"meow", ...}
        """
        with self.options(response_format=self._make_json_schema(schema), **options):
            return self.text(prompt_text, timeout=timeout)

    def json_async(
        self,
        prompt_text: str,
        callback: Callable[[dict], None],
        schema: Optional[Dict[str, Any]] = None,
        options: Dict[str, Any] = {},
    ) -> None:
        """This is an asyncrhonous version of the json method. See its documentation for more. This method returns nothing but expects an additional callback parameter, which will be called with the generated json string as argument once the backend is done generating.
        This function does not block, but returns immediately."""
        with self.options(response_format=self._make_json_schema(schema), **options):
            self.text_async(prompt_text, callback=callback)
        return

    def new(
        self,
        pydantic_class,
        text_prompt: str,
        timeout: Optional[float] = None,
        retries: int = 0,
        options: Dict[str, Any] = {},
    ) -> Any:
        """Given a subclass of pydantic.BaseModel, returns a python object of that type, with its fields filled in by the LLM backend with adherence to a given text prompt.
        This function will block until either generation finishes or a provided timeout is reached.
        This function may raise a pydantic error if the object cannot be validated. Although the LLM will be forced to adhere to the pydantic data model, this can still happen occasionally, for example, in the case of refusals. Either use the retries argument, or wrap a call to new in a try block accordingly.
        :param pydantic_class: The type of object that should be created.
        :param text_prompt: Prompt given to the LLM that should aid in object creation. This will influence how the pydantic object's fields will be filled in. Depending on the model used, certain prompts may lead to refusal, even with object creation, so be careful.
        :param timeout: A timeout in seconds after which generation is canceled.
        :param retries: In case of a validation error, how often should the generation be retried? Since sampling, especially with temperature > 0.0, is not deterministic, retries can eventually yield valid results. A value of 0 means no retries will be performed and the function raises an error on invalid data or refusal. A value of -1 means retry forever or until the timeout is reached.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: A valid python object of the provided pydantic type, with its fields filled in.

        Example:

        ## animal.py
        ```python
        from pydantic import BaseModel
        import ghostbox, json

        class Animal(BaseModel):
            name: str
            cute_name: str
            number_of_legs: int
            friendly: bool
            favorite_foods: List[str]

        box = ghostbox.from_generic(...)
        cat = box.new(Animal, "Please generate a cute housecat.")
        print(json.dumps(cat.model_dump(), indent=4))
        ```

        ## Output

        ```bash
        $ python animal.py
        {
            "name" : "Cat",
            "cute_name" : "Dr. Kisses",
            "number_of_legs" : 4,
            "friendly" : true,
            "favorite_foods" : ["Tuna", "Cat Treats", "Water from the toilet"]
        }
        ```
        """
        with self.options(stream=False, **options):
            while True:
                try:
                    return pydantic_class(
                        **json.loads(
                            self.json(
                                text_prompt,
                                schema=pydantic_class.model_json_schema(),
                                timeout=timeout,
                            )
                        )
                    )
                except ValidationError as e:
                    if retries == 0:
                        raise e
                    retries -= 1

    # images
    @contextmanager
    def images(self, image_urls: List[str]):
        """Creates a context in which ghostbox queries can refer to images.
        This context manager will not raise an error if an image cannot be found on the file system.
        :param image_urls: A list of either file system paths or web URLs (or both) that point to images.
        :return: A ghostbox instance.
        """
        for i in range(len(image_urls)):
            url = image_urls[i]
            self._plumbing.loadImage(url, i)
        yield self
        self._plumbing.images = {}

    # audio transcription

    def audio_start_transcription(
        self,
        transcription_callback: Callable[[str], None],
        threshold_activation_callback: Callable[[], None],
    ):
        """Start to continuously record and transcribe audio above a certain threshold.
        This method is for rolling your own transcriber. If you want to simply have the ghostbox react to a user saying things, just use the "audio"=True option. E.g.
        ```
        box = ghostbox.from_generic(character_folder="some_helpful_assistant",
            audio=True)
        while True:
            # the box will be transcribing and answering the transcribed text on a seperate thread
            # as if you had called box.text with the transcription
            # (provided silence threshold and activation phrase are met and given)
            # so we idle here
            time.sleep(0.3)
        ```

        You can configure this transcriber with the options context manager. See the various audio_* options for more. Especially the audio_silence_threshold may be interesting.
        If the transcription seems to do nothing, make sure the threshold is low enough and that no activation phrase is set.
        :param transcription_callback: A function that will be called whenever a successful transcription was made. For a transcription to be successful, three conditions must be met: 1. The microhphone must record sound above the activation threshold. 2. If an activation phrase is set, a fuzzy version of the phrase must be found in the transcription. 3. The recording must eventually register audio below the silence threshold again, ending the transcription. If all things are true, the callback will be invoked with the finished transcription. Note that this can be quiet a long time (tens of seconds) away from the beginning of recording.
        :param activation_callback: This function will be called whenever the audio recording goes above the silence threshold. It is called immediately when transcription begins. This is useful, if e.g. you are trying to build a responsive AI that stops the tts instantly when the user speaks.
        :return: The ghostbox instance.
        """
        self.audio_stop_transcription()
        self._ct = self._plumbing.whisper.transcribeContinuously(
            callback=transcription_callback,
            on_threshold=threshold_activation_callback,
            websock=self._plumbing.getOption("audio_websock"),
            websock_host=self._plumbing.getOption("audio_websock_host"),
            websock_port=self._plumbing.getOption("audio_websock_port"),
            silence_threshold=self._plumbing.getOption("audio_silence_threshold"),
        )
        return self

    def audio_stop_transcription(self):
        """Stops an ongoing continuous transcription."""
        if self._ct is not None:
            self._ct.stop()

    # managing ghostbox operation

    def start_session(self, filepath: str, keep=False) -> Self:
        """Start a completely new session with a given character folder.
        This function wipes all history and context variables. It's a clean slate. If you want to switch characters while retaining context, use set_char instead.
        :param filepath: Path to a character folder.
        :return: The ghostbox instance.
        """
        printerr(start_session(self._plumbing, filepath))
        return self

    def load_config(self, config_file: str) -> Self:
        """Loads a config file and applies the option/value pairs in it to this ghostbox instance.
        A config file contains a valid json string that describes an options dictionary.
        Configs loaded this way are intended to be user profiles or similar. AI character folders have their own config.json files that, by convention, are loaded automatically. If you want to simply configure an AI char, you should use their dedicated config file as it generally does *the right thing* when it comes to annoying stuff of overriding options and option precedence etc.
        Example:

        ## alice_config.json

        ```json
        {
            "chat_user" : "alice",
            "tts": true,
            "tts_model": "kokoro",
            "tts_voice": "af_sky",
            "log_time": false
        }
        ```
        """
        printerr(load_config(self._plumbing, config_file))
        # FIXME: update self.__dict__?
        return self

    def stop(self) -> None:
        """Stops all ongoing interaction, including asynchronous or streaming text generation, and TTS output."""
        self._plumbing.stopAll()

    def tools_inject_dependency(self, symbol_name: str, obj: object) -> Self:
        """Make a python object available in the python tool module of a running ghostbox AI, without having defined it in the tools.py.
        This can be used to inject dependencies from the 'outside'. This is useful in cases where you must make an object available to the AI that can not be acquired during initialization of the tool module, for example, a resource manager, or a network connection.
        Note that the AI will not be aware of an injected dependency, and be unable to reference it. However, you can refer to the symbol_name in the functions you define for the AI, which will be a bound reference as soon as you inject the dependency. The AI may then use the functions that previously didn't refer to the object now bound by symbol_name.
        :param symbol_name: The name of the identifier to be injected. If this is e.g. 'point', point will be bound to obj in tools.py.
        :param obj: An arbitrary python object. A reference to object will be bound to symbol_name and be available in tools.py.
        :return: Ghostbox
        Here's an example use of dependency injection:
        ## human_resources.py
        ```python
        box = ghostbox.from_generic(...)
        # imaginary database connection e.g.
        # we don't want this in the tools.py of employee_assistant
        database_handle = EmployeeDatabase.open_connection(user="bob", password="bruce_schneier")
        # stuff happens, maybe we start a read/evaluate loop
        # ...

        # start our assistant
        box.start_session("employee_assistant")
        #without this, the tool calls would fail
        box.tools_inject_dependency("DBHANDLE", database_handle)
        with options{use_tools=True):
            good_employees = box.text("Can you give me a quick summary of all the employees in the database that have overperformed last motnh?")
        # do something with good_employees
        # ...
        ```

        ## employee_assistant/tools.py
        ```python
        # many tool definitions here
        # ...

        # then the one the AI will probably choose for our request above
        def query_database(sql_query: str) -> Dict:
            # this would fail without dependency injection
            # since DBHANDLE is not defined in the module
            results = DBHANDLE.query(sql)
            # do stuff with results to bring it into a form the AI likes
            final = some_repackaging(results)
            return final
        ```

        At the point above where tools_inject_dependency is called, the DBHANDLE identified in the tools.py module becoems defined. Without the injection, referencing it would raise an exception.
        """
        module = self._plumbing.session.tools_module
        if module is None:
            printerr(
                "warning: Unable to inject dependency '"
                + symbol_name
                + "'. Tool module not initialized."
            )
            return Self

        # FIXME: should we warn users if they overriade an existing identifier? Let's do ti since if they injected once why do they need to do it again?
        if symbol_name in module.__dict__:
            printerr(
                "warning: While trying to inject a dependency: '"
                + symbol_name
                + "' already exists in tool module."
            )

        module.__dict__[symbol_name] = obj

    def tts_say(self, text: str, interrupt: bool = True) -> Self:
        """Speak something with the underlying TTS engine.
        You can modify the tts behaviour by wrapping this call in a `with options(...)` context. For possible options, see the ghostbox documentation of the various tts_* options.
        :param text: The text to be spoken.
        :param interrupt: If true, this call will interrupt speech output that is currently in progress (the default). Otherwise, text will be queue and spoken in sequence.
        :return: The ghostbox instance.
        """
        self._plumbing.communicateTTS(text, interrupt=interrupt)
        return self

    def tts_is_speaking(self) -> bool:
        """Returns True if the TTS engine is currently speaking."""
        return self._plumbing.ttsIsSpeaking()

    def tts_wait(self, timeout: Optional[float] = None, minimum: float = 2.0) -> bool:
        """Blocks thread of execution until the TTS has finished speaking.
        :param timeout: Number of seconds, if any, after which to resume execution. If you also want to stop the tts you'll have to do that yourself.
        :param minimum: Number of seconds to wait at minimum. This is useful because, although the TTS engines are realtime capable, there may be a small delay between a request for speech output and the actual output starting.
        :return: This function always returns True"""
        begin = time.time()
        time.sleep(minimum)

        # we have to do it this way because we poll the process in plumbing
        while self.tts_is_speaking():
            if timeout is not None:
                if (time.time() - begin) > timeout:
                    break
            time.sleep(0.3)
        return True

    def tts_stop(self) -> Self:
        """Stops speech output that is in progress.
        :return: The ghostbox instance.
        """
        self._plumbing.stopTTS()
        return self

    def set_char(
        self,
        character_folder: str,
        chat_history: Optional[List[ChatMessage | Dict[str, Any]]] = None,
    ) -> Self:
        """Set an active character_folder, which may be the same one, and optionally set the chat history.
        This method differs from start_session in that it doesn't wipe various vars that may be set in the session, and preserves the chat history by default.
        Note: This will wipe the previous history unless chat_history is None.
        :param character_folder: The new character folder to load.
        :param chat_history: A list of ChatHistory items, valid JSON dictionaries that parse as ChatHistoryItems, or a mix of both. If None, chat history will be retained.
        :return: Ghostbox instance."""
        if character_folder != self._plumbing.getOption("character_folder"):
            printerr(start_session(self._plumbing, character_folder))

        if chat_history is None:
            return self

        self._plumbing.session.stories.reset()
        story = self._plumbing.session.stories.get()
        for item in chat_history:
            if type(item) == ChatMessage:
                story.appendMessage(item)
            else:
                # try to parse the item as ChatMessage
                try:
                    story.addRawJSON(item)
                except:
                    printerr(
                        "warning: Couldn't parse chat history. Not a valid ChatMessage. Skipping message. Traceback below."
                    )
                    printerr(traceback.format_exc())
                    continue

        return self

    def clear_history(self) -> None:
        """Resets the chat history."""
        self.set_char(self.character_folder, [])
    
    def history(self) -> List[ChatMessage]:
        """Returns the current chat history for this ghostbox instance.
        :return: The chat history.
        """
        return self._plumbing.session.stories.get().getData()

    # debug

    def get_last_request(self) -> Dict[str, Any]:
        """Returns the last request send to the backend server. The dict may be empty if no request was sent yet."""
        return self._plumbing.getBackend().getLastRequest()        

    def get_last_result(self) -> Dict[str, Any]:
        """Returns the last result that was retrieved from the server. The dict may be empty if no result was received yet."""
        return self._plumbing.getBackend().getLastJSON()

```

## _argparse.py

```python
import argparse, os
from ghostbox.util import *
from ghostbox import backends
from ghostbox.definitions import *
from ghostbox.__init__ import get_ghostbox_data


class TaggedArgumentParser:
    """Creates an argument parser along with a set of tags for each argument.
    Arguments to the constructor are passed on to argparse.ArgumentParser.__init__ .
    You can then use add_arguments just like with argparse, except that there is an additional keyword argument 'tags', which is a dictionary that will be associated with that command line argument.
    """

    def __init__(self, **kwargs):
        self.parser = argparse.ArgumentParser(**kwargs)
        self.tags = {}

    def add_argument(self, *args, **kwargs):
        if "tag" in kwargs:
            # this is a bit tricky, argparse does a lot to find the arg name, but this might do
            # find the longest arg, strip leading hyphens, replace remaining hyphens with _
            arg = (
                sorted(args, key=lambda w: len(w), reverse=True)[0]
                .strip("-")
                .replace("-", "_")
            )
            self.tags[arg] = kwargs["tag"]
            self.tags[arg].name = arg
            # and if there is no help then let it blow up
            self.tags[arg].help = kwargs["help"]
            if "default" in kwargs:
                self.tags[arg].default_value = (
                    kwargs["default"] if kwargs["default"] is not None else "None"
                )
            del kwargs["tag"]

        self.parser.add_argument(*args, **kwargs)

    def get_parser(self):
        return self.parser

    def get_tags(self):
        return self.tags


def makeTaggedParser(default_params) -> TaggedArgumentParser:
    parser = TaggedArgumentParser(
        description="LLM Command Line Interface",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    # we'll be typing these a lot so buckle up
    mktag = ArgumentTag
    AT = ArgumentType
    AG = ArgumentGroup
    parser.add_argument(
        "-I",
        "--include",
        action="append",
        default=[userCharDir(), get_ghostbox_data("chars/"), "chars/"],
        help="Include paths that will be searched for character folders named with the /start command or the --character_folder command line argument.",
        tag=mktag(type=AT.Porcelain, group=AG.Characters, motd=True),
    )
    parser.add_argument(
        "--template_include",
        action="append",
        default=[userTemplateDir(), get_ghostbox_data("templates/"), "templates/"],
        help="Include paths that will be searched for prompt templates. You can specify a template to use with the -T option.",
        tag=mktag(type=AT.Plumbing, group=AG.Templates),
    )
    parser.add_argument(
        "-T",
        "--prompt_format",
        type=str,
        default="auto",
        help="Prompt format template to use. The default is 'auto', which means ghostbox will .let the backend handle templating, which is usually the right choice. You can still use other settings, like 'raw', to experiment. This is ignored if you use the generic or openai backend. Note: Prompt format templates used to be more important in the early days of LLMs, as confusion was rampant and mistakes were not uncommon even in official releases. Nowadays, it is quite safe to use the official templates. You may still want to use this option for experimentation, however.",
        tag=mktag(type=AT.Plumbing, group=AG.Templates),
    )
    parser.add_argument(
        "-s",
        "--stop",
        action="append",
        default=[],
        help="Forbidden strings that will stop the LLM backend generation.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation, motd=True),
    )
    parser.add_argument(
        "-c",
        "--character_folder",
        type=str,
        default="",
        help="character folder to load at startup. The folder may contain a `system_msg` file, a `config.json`, and a `tools.py`, as well as various other files used as file variables. See the examples and documentation for more.",
        tag=mktag(
            type=AT.Porcelain, group=AG.Characters, very_important=True, motd=True
        ),
    )
    parser.add_argument(
        "-p",
        "--prompt",
        type=str,
        default=None,
        help="If provided, process the prompt and exit.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation),
    )
    parser.add_argument(
        "--endpoint",
        type=str,
        default="http://localhost:8080",
        help="Address of backend http endpoint. This is a URL that is dependent on the backend you use, though the default of localhost:8080 works for most, including Llama.cpp and Kobold.cpp. If you want to connect to an online provider that is not part of the explicitly supported backends, this is where you would supply their API address.",
        tag=mktag(type=AT.Porcelain, group=AG.Backend),
    )
    parser.add_argument(
        "--backend",
        type=str,
        default=LLMBackend.generic.name,
        help="Backend to use. The default is `generic`, which conforms to the OpenAI REST API, and is supported by most LLM providers. Choosing a more specific backend may provide additional functionality. Other possible values are "
        + ", ".join([e.name for e in LLMBackend])
        + ".",
        tag=mktag(type=AT.Porcelain, group=AG.Backend, very_important=True),
    )
    parser.add_argument(
        "--api_key",
        type=str,
        default="",
        help="API key for OpenAI. Without the `--backend openai` option, this has no effect.",
        tag=mktag(type=AT.Plumbing, group=AG.OpenAI),
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=300,
        help="Number of tokens to request from backend for generation. Generation is stopped when this number is exceeded. Negative values mean generation is unlimited and will terminate when the backend generates a stop token.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation, very_important=True),
    )
    parser.add_argument(
        "--max_context_length",
        type=int,
        default=32768,
        help="Maximum number of tokens to keep in context.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation, very_important=True),
    )
    parser.add_argument(
        "-u",
        "--chat_user",
        type=str,
        default="user",
        help="Username you wish to be called when chatting in 'chat' mode. It will also replace occurrences of {chat_user} anywhere in the character files. If you don't provide one here, your username will be determined by your OS session login.",
        tag=mktag(type=AT.Porcelain, group=AG.General, very_important=True),
    )
    parser.add_argument(
        "-M",
        "--mode",
        type=str,
        default="default",
        help="Mode of operation. Changes various things behind-the-scenes. Values are currently 'default', or 'chat'.",
        tag=mktag(type=AT.Plumbing, group=AG.Templates),
    )
    parser.add_argument(
        "--force_params",
        action=argparse.BooleanOptionalAction,
        type=bool,
        default=False,
        help="Force sending of sample parameters, even when they are seemingly not supported by the backend (use to debug or with generic",
        tag=mktag(type=AT.Plumbing, group=AG.Backend),
    )
    parser.add_argument(
        "-m",
        "--model",
        type=str,
        help="LLM to use for requests. This only works if the backend supports choosing models.",
        tag=mktag(type=AT.Porcelain, group=AG.Backend, very_important=True),
    )
    parser.add_argument(
        "-g",
        "--grammar_file",
        type=str,
        default="",
        help="Grammar file used to restrict generation output. Grammar format is GBNF.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=True),
    )
    parser.add_argument(
        "--chat_ai",
        type=str,
        default="",
        help="Name  the AI will have when chatting. Has various effects on the prompt when chat mode is enabled. This is usually set automatically in the config.json file of a character folder.",
        tag=mktag(type=AT.Plumbing, group=AG.Characters),
    )
    parser.add_argument(
        "--stream",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Enable streaming mode. This will print generations by the LLM piecemeal, instead of waiting for a full generation to complete. Results may be printed per-token, per-sentence, or otherwise, according to --stream_flush.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation, very_important=True),
    )
    parser.add_argument(
        "--http",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable a small webserver with minimal UI. By default, you'll find it at localhost:5050.",
        tag=mktag(type=AT.Porcelain, group=AG.Interface, service=True),
    )
    parser.add_argument(
        "--websock",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable sending and receiving commands on a websock server running on --websock_host and --websock_port. This is enabled automatically with --http.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface, service=True, motd=True),
    )
    parser.add_argument(
        "--websock_host",
        type=str,
        default="localhost",
        help="The hostname that the websocket server binds to.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--websock_port",
        type=int,
        default=5150,
        help="The port that the websock server will listen on. By default, this is the http port +100.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--http_host",
        type=str,
        default="localhost",
        help="Hostname to bind to if --http is enabled.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--http_port",
        type=int,
        default=5050,
        help="Port for the web server to listen on if --http is provided. By default, the --audio_websock_port will be --http_port+1, and --tts_websock_port will be --http_port+2, e.g. 5051 and 5052.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--http_override",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="If enabled, the values of --audio_websock, --tts_websock, --audio_websock_host, --audio_websock_port, --tts_websock_host, --tts_websock_port will be overriden if --http is provided. Use --no-http_override to disable this, so you can set your own host/port values for the websock services or disable them entirely.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--multiline",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Makes multiline mode the dfault, meaning that newlines no longer trigger a message being sent to the backend. instead, you must enter the value of --multiline_delimiter to trigger a send.",
        tag=mktag(type=AT.Porcelain, group=AG.Interface),
    )
    parser.add_argument(
        "--multiline_delimiter",
        type=str,
        default="\\",
        help="String that signifies the end of user input. This is only relevant for when --multiline is enabled. By default this is a backslash, inverting the normal behaviour of backslashes allowing to enter a newline ad-hoc while in multiline mode. This option is intended to be used by scripts to change the delimiter to something less common.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--color",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Enable colored output.",
        tag=mktag(type=AT.Porcelain, group=AG.Interface, motd=True),
    )
    parser.add_argument(
        "--text_ai_color",
        type=str,
        default="none",
        help="Color for the generated text, as long as --color is enabled. Most ANSI terminal colors are supported.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface, motd=True),
    )
    parser.add_argument(
        "--text_ai_style",
        type=str,
        default="bright",
        help="Style for the generated text, as long as --color is enabled. Most ANSI terminal styles are supported.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--dynamic_file_vars",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Dynamic file vars are strings of the form {[FILE1]}. If FILE1 is found, the entire expression is replaced with the contents of FILE1. This is dynamic in the sense that the contents of FILE1 are loaded each time the replacement is encountered, which is different from the normal file vars with {{FILENAME}}, which are loaded once during character initialization. Replacement happens in user inputs only. In particular, dynamic file vars are ignored in system messages or saved chats. If you want the LLM to get file contents, use tools. disabling this means no replacement happens. This can be a security vulnerability, so it is disabled by default on the API.",
        tag=mktag(type=AT.Plumbing, group=AG.Templates, motd=True),
    )
    parser.add_argument(
        "--warn_trailing_space",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Warn if the prompt that is sent to the backend ends on a space. This can cause e.g. excessive emoticon use by the model.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation),
    )
    parser.add_argument(
        "--warn_unsupported_sampling_parameter",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Warn if you have set an option that is usually considered a sampling parameter, but happens to be not supported by the chose nbackend.",
        tag=mktag(type=AT.Plumbing, group=AG.SamplingParameters),
    )
    parser.add_argument(
        "--warn_audio_activation_phrase",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Warn if audio is being transcribed, but no activation phrase is found. Normally this only will warn once. Set to -1 if you want to be warned every time.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--warn_hint",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Warn if you have a hint set.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation),
    )
    parser.add_argument(
        "--json_grammar",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Force generation output to be in JSON format. This is equivalent to using -g with a json.gbnf grammar file, but this option is provided for convenience.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation, motd=True),
    )
    parser.add_argument(
        "--stream_flush",
        type=str,
        default="token",
        help="When to flush the streaming buffer. When set to 'token', will print each token immediately. When set to 'sentence', it will wait for a complete sentence before printing. This can be useful for TTS software. When set to 'flex', will act like 'sentence', but prebuffer a minimum amount of characters before flushing, according to stream_flush_flex_value. Default is 'token'.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=True),
    )
    parser.add_argument(
        "--stream_flush_flex_value",
        type=int,
        default=50,
        help="How many characters (not tokens) at least to buffer before flushing the queue, when stream_flush is set to 'flex'.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=False),
    )    
    parser.add_argument(
        "--cli_prompt",
        type=str,
        default=" {{current_tokens}} > ",
        help="String to show at the bottom as command prompt. Can be empty.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface, motd=True),
    )
    parser.add_argument(
        "--cli_prompt_color",
        type=str,
        default="none",
        help="Color of the prompt. Uses names of standard ANSI terminal colors. Requires --color to be enabled.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--hint",
        type=str,
        default="",
        help="Hint for the AI. This string will be appended to the prompt behind the scenes. It's the first thing the AI sees. Try setting it to 'Of course,' to get a more compliant AI. Also refered to as 'prefill'.",
        tag=mktag(type=AT.Porcelain, group=AG.Generation),
    )
    parser.add_argument(
        "--hint_sticky",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="If disabled, hint will be shown to the AI as part of prompt, but will be omitted from the story.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation),
    )
    parser.add_argument(
        "--tts",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable text to speech on generated text.",
        tag=mktag(type=AT.Porcelain, group=AG.TTS, very_important=True, service=True),
    )
    parser.add_argument(
        "--tts_model",
        type=str,
        default=TTSModel.kokoro.name,
        help="The TTS model to use. This is ignored unless you use ghostbox-tts as your tts_program. Options are:  "
        + ", ".join([m.name for m in TTSModel]),
        tag=mktag(type=AT.Porcelain, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_zonos_model",
        type=str,
        default=ZonosTTSModel.hybrid.name,
        help="The Zonos TTS model offers two architecural variants: A pure transformer implementation or a transformer-mamba hybrid variant. Hybrid usually gives the best results, but requires flash attention. This option has no effect on non-zonos TTS engines. Options are: "
        + ", ".join([m.name for m in ZonosTTSModel]),
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_orpheus_model",
        type=str,
        default="",
        help="Filepath to gguf file or huggingface repo name of the orpheus model you wish to use with the tts. Leave empty for some reasonable defaults. If you don't use the default, and set this to a repo, the underlying ghostbox-tts might silently download the snapshot, so check with /ttsdebug. This option is ignored unless you actually set tts_model to orpheus.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_llm_server",
        type=str,
        default="",
        help="Address of another LLM server that serves a TTS model on an OpenAI compatible backend. This is only relevant if you use a tts model that needs an LLM as backend. This is currently the case with orpheus. You can also let ghostbox-tts spawn a server on its own if you have llama-server in your path, by setting tts_orpheus_model to an orpheus gguf file or huggingface repository.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_language",
        type=str,
        default="en",
        help="Set the TTS voice language. Right now, this is only relevant for kokoro.",
        tag=mktag(type=AT.Porcelain, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_output_method",
        type=str,
        choices=[om.name for om in TTSOutputMethod],
        default=TTSOutputMethod.default.name,
        help="How to play the generated speech. Using the --http argument automatically sets this to websock.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_websock",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable websock as the output method for TTS. This is equivalent to `--tts_output_method websock`.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_websock_host",
        type=str,
        default="localhost",
        help="The address to bind to for the underlying TTS program when using websock as output method. ghostbox-tts only. This option is normally overriden by --http.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_websock_port",
        type=int,
        default=5052,
        help="The port to listen on for the underlying TTS program when using websock as output method. ghostbox-tts only. This option is normally overriden by --http.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_interrupt",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Stop an ongoing TTS whenever a new generation is spoken. When set to false, will queue messages instead.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_program",
        type=str,
        default="ghostbox-tts",
        help="Path to a TTS (Text-to-speech) program to verbalize generated text. The TTS program should read lines from standard input. Many examples are provided in scripts/ghostbox-tts-* . The ghostbox-tts script offers a native solution using various supported models.",
        tag=mktag(type=AT.Porcelain, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_clone_dir",
        type=str,
        default="",
        help="Directory to check first for wave files used in voice cloning. Note that voice cloning isn't supported by all tts models.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_tortoise_quality",
        type=str,
        default="fast",
        help="Quality preset. tortoise-tts only. Can be 'ultra_fast', 'fast' (default), 'standard', or 'high_quality'",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_volume",
        type=float,
        default=1.0,
        help="Volume for TTS voice program. Is passed to tts_program as environment variable.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_modify_system_msg",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="If enabled, instructions specific to the underlying TTS model will be appended to the system prompt. This may e.g. point out usage of tags like <laugh> and <cough> to the LLM. Support varies depending on model.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )    
    parser.add_argument(
        "--tts_rate",
        type=int,
        default=50,
        help="Speaking rate for TTS voice program. Is passed to tts_program as environment variable. Note that speaking rate is not supported by all TTS engines.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_additional_arguments",
        type=str,
        default="",
        help="Additional command line arguments that will be passed to the tts_program.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--image_watch",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable watching of a directory for new images. If a new image appears in the folder, the image will be loaded with id 0 and sent to the backend. works with multimodal models only (like llava).",
        tag=mktag(type=AT.Porcelain, group=AG.Images, service=True, motd=True),
    )
    parser.add_argument(
        "--image_watch_dir",
        type=str,
        default=os.path.expanduser("~/Pictures/Screenshots/"),
        help="Directory that will be watched for new image files when --image_watch is enabled.",
        tag=mktag(type=AT.Plumbing, group=AG.Images),
    )
    parser.add_argument(
        "--image_watch_msg",
        type=str,
        default="Can you describe this image?",
        help="If image_watch is enabled, this message will be automatically send to the backend whenever a new image is detected. Set this to '' to disable automatic messages, while still keeping the automatic update the image with id 0.",
        tag=mktag(type=AT.Plumbing, group=AG.Images, motd=True),
    )
    parser.add_argument(
        "--image_watch_hint",
        type=str,
        default="",
        help="If image_watch is enabled, this string will be sent to the backend as start of the AI response whenever a new image is detected and automatically described. This allows you to guide or solicit the AI by setting it to e.g. 'Of course, this image show' or similar. Default is ''",
        tag=mktag(type=AT.Plumbing, group=AG.Images, motd=True),
    )
    parser.add_argument(
        "--whisper_model",
        type=str,
        default="base.en",
        help="Name of the model to use for transcriptions using the openai whisper model. Default is 'base.en'. For a list of model names, see https://huggingface.co/openai/whisper-large",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "-y",
        "--tts_voice",
        type=str,
        default="random",
        help="Voice to use for TTS. Default is 'random', which is a special value that picks a random available voice for your chosen tts_program. The value of tts_voice will be changed at startup if random is chosen, so when you find a voice you like you can find out its name with /lsoptions and checking tts_voice. To get a list of voices, start ghostbox with your desired tts model and do /lsvoices.",
        tag=mktag(type=AT.Porcelain, group=AG.TTS),
    )
    parser.add_argument(
        "--tts_subtitles",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Enable printing of generated text while TTS is enabled.",
        tag=mktag(type=AT.Plumbing, group=AG.TTS),
    )
    parser.add_argument(
        "--config_file",
        type=str,
        default="",
        help="Path to a config fail in JSON format, containing a dictionary with OPTION : VALUE pairs to be loaded on startup. Same as /loadconfig or /loadoptions. To produce an example config-file, try /saveconfig example.json.",
        tag=mktag(type=AT.Porcelain, group=AG.General),
    )
    parser.add_argument(
        "--chat_show_ai_prompt",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Controls wether to show AI prompt in chat mode. Specifically, assuming chat_ai = 'Bob', setting chat_show_ai_prompt to True will show 'Bob: ' in front of the AI's responses. Note that this is always sent to the back-end (in chat mode), this parameter merely controls wether it is shown.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--smart_context",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Enables ghostbox version of smart context, which means dropping text at user message boundaries when the backend's context is exceeded. If you disable this, it usually means the backend will truncate the raw message. Enabling smart context means better responses and longer processing time due to cache invalidation, disabling it means worse responses with faster processing time. Note from marius: Beware I haven't looked at this in a while, since newer models all have very large contexts.",
        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=True),
    )
    parser.add_argument(
        "--hide",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Hides some unnecessary output, providing a more immersive experience. Same as typing /hide.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface, motd=True),
    )
    parser.add_argument(
        "--audio",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable automatic transcription of audio input using openai whisper model. Obviously, you need a mic for this.",
        tag=mktag(type=AT.Porcelain, group=AG.Audio, service=True, very_important=True),
    )
    parser.add_argument(
        "--audio_silence_threshold",
        type=int,
        default=2000,
        help="An integer value denoting the threshold for when automatic audio transcription starts recording. (default 2000)",
        tag=mktag(type=AT.Plumbing, group=AG.Audio, motd=True),
    )
    parser.add_argument(
        "--audio_activation_phrase",
        type=str,
        default="",
        help="When set, the phrase must be detected in the beginning of recorded audio, or the recording will be ignored. Phrase matching is fuzzy with punctuation removed.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio, motd=True),
    )
    parser.add_argument(
        "--audio_activation_period_ms",
        type=int,
        default=0,
        help="Period in milliseconds where no further activation phrase is necessary to trigger a response. The period starts after any interaction with the AI, spoken or otherwise.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--audio_interrupt",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Stops generation and TTS  when you start speaking. Does not require activation phrase.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--audio_activation_phrase_keep",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="If false and an activation phrase is set, the triggering phrase will be removed in messages that are sent to the backend.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--audio_show_transcript",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Show transcript of recorded user speech when kaudio transcribing is enabled. When disabled, you can still see the full transcript with /log or /print.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--audio_websock",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable to listen for audio on an HTTP websocket at the given `--websock_url`, instead of recording audio from a microphone. This can be used to stream audio through a website. This is enabled by default with the --http option unless you also supply --no-http_overrid.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--audio_websock_host",
        type=str,
        default="localhost",
        help="The address to bind to when `--audio_websock` is enabled. You can stream audio to this endpoint using the websocket protocol for audio transcription. Normally overriden by --http.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--audio_websock_port",
        type=int,
        default=5051,
        help="The port to listen on when `--audio_websock` is enabled. You can stream audio to this endpoint using the websocket protocol for audio transcription. Normally overriden by --http.",
        tag=mktag(type=AT.Plumbing, group=AG.Audio),
    )
    parser.add_argument(
        "--verbose",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Show additional output for various things.",
        tag=mktag(type=AT.Plumbing, group=AG.General),
    )
    parser.add_argument(
        "--log_time",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Print timing and performance statistics to stderr with every generation. Auto enabled for the API.",
        tag=mktag(type=AT.Plumbing, group=AG.General),
    )
    parser.add_argument(
        "-q",
        "--quiet",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Prevents printing and TTS vocalization of generations. Often used with the API when you want to handle generation results yourself and don't want printing to console.",
        tag=mktag(type=AT.Porcelain, group=AG.Interface),
    )
    parser.add_argument(
        "--stderr",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Wether printing to stderr is enabled. You may want to disable this when building terminal applications using the API.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--stdout",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Wether printing to stdout is enabled. You may want to disable this when building terminal applications using the API.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--expand_user_input",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Expand variables in user input. E.g. {$var} will be replaced with content of var. Variables are initialized from character folders (i.e. file 'memory' will be {$memory}), or can be set manually with the /varfile command or --varfile option. See also --dynamic_file_vars.",
        tag=mktag(type=AT.Plumbing, group=AG.Interface),
    )
    parser.add_argument(
        "--tools_unprotected_shell_access",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Allow an AI to run shell commands, even if not logged in to their own account. The safe way of doing this is to create an account on your system with the same name as the AI, and then run this program under their account. If you don't want to do that, and you are ok with an AI deleting your files through accident or malice, set this flag to true.",
        tag=mktag(type=AT.Plumbing, group=AG.Tools),
    )
    # parser.add_argument("--tools_reflection", action=argparse.BooleanOptionalAction, default=True, help="Continue generation after tools have been applied. This allows the AI to reflect on the results, e.g. summarize a file it retrieved. Only applies when use_tools is true.",
    # tag=mktag(type=AT.Plumbing, group=AG.Tools))
    parser.add_argument(
        "-d",
        "--tools_forbidden",
        action="append",
        default=["Any", "List", "Dict", "launch_nukes"],
        help="Blacklist certain tools. Specify multiple times to forbid several tools. The default blacklist contains some common module imports that can pollute a tools.py namespace. You can override this in a character folders config.json if necessary.",
        tag=mktag(type=AT.Plumbing, group=AG.Tools, very_important=True, motd=True),
    )
    parser.add_argument(
        "--tools_hint",
        type=str,
        default="",
        help="Text that will be appended to the system prompt when use_tools is true.",
        tag=mktag(type=AT.Plumbing, group=AG.Tools),
    )
    parser.add_argument(
        "--tools_inject_dependency_function",
        type=str,
        default="",
        help="API only. Set a callback function to be called whenever an tool-using Ai is initialized. The callback will receive one argument: The tools.py module. You can use this to inject dependency or modify the module after it is loaded.",
        tag=mktag(type=AT.Plumbing, group=AG.Tools, motd=True),
    )
    parser.add_argument(
        "--tools_inject_ghostbox",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Inject a reference to ghostbox itself into an AI's tool module. This will make the '_ghostbox_plumbing' identifier available in the tools module and point it to the running ghostbox Plumbing instance. Disabling this will break many of the standard AI tools that ship with ghostbox.",
        tag=mktag(type=AT.Plumbing, group=AG.Tools),
    )
    parser.add_argument(
        "--use_tools",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Enable use of tools, i.e. model may call python functions. This will do nothing if tools.py isn't present in the char directory. If tools.py is found, this will be automatically enabled.",
        tag=mktag(type=AT.Porcelain, group=AG.Tools, very_important=True, motd=True),
    )
    parser.add_argument(
        "-x",
        "--var_file",
        action="append",
        default=[],
        help="Files that will be added to the list of variables that can be expanded. E.g. -Vmemory means {$memory} will be expanded to the contents of file memory, provided expand_user_input is set. Can be used to override values set in character folders. Instead of using this, you can also just type {[FILENAME]} to have it be automatically expanded with the contents of FILENAME, provided --dynamic_file_vars is enabled.",
        tag=mktag(type=AT.Porcelain, group=AG.Interface, motd=True),
    )

    # don't show all possible parameters on command line, but do show some
    params = backends.supported_parameters | backends.sometimes_parameters
    for name, param in params.items():
        # we need to avoid some redefinitions
        if name in "stop max_length".split(" "):
            continue
        # others require special treatment
        if name == "samplers":
            parser.add_argument(
                "-S",
                "--" + param.name,
                nargs="*",
                default=param.default_value,
                help=param.description,
                tag=mktag(
                    type=AT.Plumbing,
                    group=AG.SamplingParameters,
                    very_important=True,
                    motd=True,
                    is_option=True,
                ),
            )
            continue

        parser.add_argument(
            "--" + param.name,
            type=type(param.default_value),
            default=param.default_value,
            help=param.description,
            tag=mktag(type=AT.Plumbing, group=AG.SamplingParameters, is_option=True),
        )
    return parser


def makeDefaultOptions():
    """Returns a pair of default options and tags."""
    tp = makeTaggedParser(backends.default_params)
    parser = tp.get_parser()
    return parser.parse_args(args=""), tp.get_tags()

```

## autoimage.py

```python
import os, appdirs, threading, time, sys
from ghostbox.util import *
import operator
from stat import ST_MTIME

def mostRecentFile(path):
    """Returns pair of most recent file in directory PATH, as well as its modification date. Returns ("", 0) if path is not a directory."""
    if not(os.path.isdir(path)):
        return ("", 0)
    
    
    all_files = os.listdir(path);
    file_mtimes = dict();
    for file in all_files:
        e = path + "/" + file
        file_mtimes[e] = time.time() - os.stat(e).st_mtime;
    if not(file_mtimes):
        return ("", 0)
    winner =  sorted(file_mtimes.items(), key=operator.itemgetter(1))[0][0]
    return (winner, os.stat(winner).st_mtime)




class AutoImageProvider(object):
    """When instanciated, takes a directory and scans it periodically for a new image file. Once a new file is detected, the AutoImageProvider executes a callback with the image data as argument.
This allows people to take screenshots and have them automatically be described, without having to further input anything."""
    def __init__(self, watch_dir, on_new_image_func, update_period=0.25, image_id=0):
        """watch_dir - Directory that will be periodically checked for new images.
on_new_image_func - Function that takes one argument, a dictionary with keys 'data' and 'id'. This is in the format expected by Llamacpp.
        update_period - How often to check the directory.
image_id - The id of the image. This is relevant as it will inform the tokens used to invoke the image, e.g. with id=0 it would be '[img-0]'."""
        self.watch_dir = watch_dir
        self.update_period = update_period
        self.image_id = image_id
        self.callback = on_new_image_func
        self.latestFile = mostRecentFile(self.watch_dir) # this is pair (filename, modtime)
        self.running = False
        self._initWatchLoop()

    def _initWatchLoop(self):
        self.running = True
        t = threading.Thread(target=self._watchLoop, args=[])
        t.start()

    def stop(self):
        self.running = False
            
    def _watchLoop(self):
        while self.running:
            if not(os.path.isdir(self.watch_dir)):
                printerr("warning: AutoImageProvider cannot watch directory '" + self.watch_dir + "': Not a directory. Halting watch loop.")
                self.stop()
            else:
                (file, modtime) = mostRecentFile(self.watch_dir)
                if modtime > self.latestFile[1] and isImageFile(file):
                    self.latestFile = (file, modtime)
                    self.invokeCallback()
            time.sleep(self.update_period)
            
    def invokeCallback(self):
        (file, time) = self.latestFile
        self.callback(file, self.image_id)
                

```

## backends.py

```python
import time, requests, threading, json
from abc import ABC, abstractmethod
from functools import *
from pydantic import BaseModel
from ghostbox.util import *
from ghostbox.definitions import *
from ghostbox.streaming import *

# this list is based on the llamacpp server. IMO most other backends are subsets of this.
# the sampler values have been adjusted to more sane default options (no more top_p)
sampling_parameters = {
    "temperature": SamplingParameterSpec(
        name="temperature",
        description="Adjust the randomness of the generated text.",
        default_value=0.8,
    ),
    "dynatemp_range": SamplingParameterSpec(
        name="dynatemp_range",
        description="Dynamic temperature range. The final temperature will be in the range of `[temperature - dynatemp_range; temperature + dynatemp_range]`",
        default_value=0.0,
    ),
    "dynatemp_exponent": SamplingParameterSpec(
        name="dynatemp_exponent",
        description="Dynamic temperature exponent.",
        default_value=1.0,
    ),
    "top_k": SamplingParameterSpec(
        name="top_k",
        description="Limit the next token selection to the K most probable tokens.",
        default_value=40,
    ),
    "top_p": SamplingParameterSpec(
        name="top_p",
        description="Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P.",
        default_value=0.95,
    ),
    "min_p": SamplingParameterSpec(
        name="min_p",
        description="The minimum probability for a token to be considered, relative to the probability of the most likely token.",
        default_value=0.05,
    ),
    # this one is renamed from n_predict
    "max_length": SamplingParameterSpec(
        name="max_length",
        description="Set the maximum number of tokens to predict when generating text. **Note:** May exceed the set limit slightly if the last token is a partial multibyte character. When 0, no tokens will be generated but the prompt is evaluated into the cache.",
        default_value=-1,
    ),
    "n_indent": SamplingParameterSpec(
        name="n_indent",
        description="Specify the minimum line indentation for the generated text in number of whitespace characters. Useful for code completion tasks.",
        default_value=0,
    ),
    "n_keep": SamplingParameterSpec(
        name="n_keep",
        description="Specify the number of tokens from the prompt to retain when the context size is exceeded and tokens need to be discarded. The number excludes the BOS token. By default, this value is set to `0`, meaning no tokens are kept. Use `-1` to retain all tokens from the prompt.",
        default_value=0,
    ),
    #    "stream": SamplingParameterSpec(
    #        name="stream",
    #        description="Allows receiving each predicted token in real-time instead of waiting for the completion to finish (uses a different response format). To enable this, set to `true`.",
    #        default_value=False
    #    ),
    "stop": SamplingParameterSpec(
        name="stop",
        description="Specify a JSON array of stopping strings. These words will not be included in the completion, so make sure to add them to the prompt for the next iteration.",
        default_value=[],
    ),
    "typical_p": SamplingParameterSpec(
        name="typical_p",
        description="Enable locally typical sampling with parameter p.",
        default_value=1.0,
    ),
    "repeat_penalty": SamplingParameterSpec(
        name="repeat_penalty",
        description="Control the repetition of token sequences in the generated text.",
        default_value=1.1,
    ),
    "repeat_last_n": SamplingParameterSpec(
        name="repeat_last_n",
        description="Last n tokens to consider for penalizing repetition.",
        default_value=64,
    ),
    "presence_penalty": SamplingParameterSpec(
        name="presence_penalty",
        description="Repeat alpha presence penalty.",
        default_value=0.0,
    ),
    "frequency_penalty": SamplingParameterSpec(
        name="frequency_penalty",
        description="Repeat alpha frequency penalty.",
        default_value=0.0,
    ),
    "dry_multiplier": SamplingParameterSpec(
        name="dry_multiplier",
        description="Set the DRY (Don't Repeat Yourself) repetition penalty multiplier.",
        default_value=0.8,
    ),
    "dry_base": SamplingParameterSpec(
        name="dry_base",
        description="Set the DRY repetition penalty base value.",
        default_value=1.75,
    ),
    "dry_allowed_length": SamplingParameterSpec(
        name="dry_allowed_length",
        description="Tokens that extend repetition beyond this receive exponentially increasing penalty: multiplier * base ^ (length of repeating sequence before token - allowed length).",
        default_value=2,
    ),
    "dry_penalty_last_n": SamplingParameterSpec(
        name="dry_penalty_last_n",
        description="How many tokens to scan for repetitions.",
        default_value=-1,
    ),
    "dry_sequence_breakers": SamplingParameterSpec(
        name="dry_sequence_breakers",
        description="Specify an array of sequence breakers for DRY sampling. Only a JSON array of strings is accepted.",
        default_value=["\n", ":", '"', "*"],
    ),
    "xtc_probability": SamplingParameterSpec(
        name="xtc_probability",
        description="Set the chance for token removal via XTC sampler.\nXTC means 'exclude top choices'. This sampler, when it triggers, removes all but one tokens above a given probability threshold. Recommended for creative tasks, as language tends to become less stereotypical, but can make a model less effective at structured output or intelligence-based tasks.\nSee original xtc PR by its inventor https://github.com/oobabooga/text-generation-webui/pull/6335",
        default_value=0.5,
    ),
    "xtc_threshold": SamplingParameterSpec(
        name="xtc_threshold",
        description="Set a minimum probability threshold for tokens to be removed via XTC sampler.\nXTC means 'exclude top choices'. This sampler, when it triggers, removes all but one tokens above a given probability threshold. Recommended for creative tasks, as language tends to become less stereotypical, but can make a model less effective at structured output or intelligence-based tasks.\nSee original xtc PR by its inventor https://github.com/oobabooga/text-generation-webui/pull/6335",
        default_value=0.1,
    ),
    "mirostat": SamplingParameterSpec(
        name="mirostat",
        description="Enable Mirostat sampling, controlling perplexity during text generation.",
        default_value=0,
    ),
    "mirostat_tau": SamplingParameterSpec(
        name="mirostat_tau",
        description="Set the Mirostat target entropy, parameter tau.",
        default_value=5.0,
    ),
    "mirostat_eta": SamplingParameterSpec(
        name="mirostat_eta",
        description="Set the Mirostat learning rate, parameter eta.",
        default_value=0.1,
    ),
    "grammar": SamplingParameterSpec(
        name="grammar",
        description="Set grammar for grammar-based sampling.",
        default_value=None,
    ),
    "json_schema": SamplingParameterSpec(
        name="json_schema",
        description='Set a JSON schema for grammar-based sampling (e.g. `{"items": {"type": "string"}, "minItems": 10, "maxItems": 100}` of a list of strings, or `{}` for any JSON). See [tests](../../tests/test-json-schema-to-grammar.cpp) for supported features.',
        default_value=None,
    ),
    "seed": SamplingParameterSpec(
        name="seed",
        description="Set the random number generator (RNG) seed.",
        default_value=-1,
    ),
    "ignore_eos": SamplingParameterSpec(
        name="ignore_eos",
        description="Ignore end of stream token and continue generating.",
        default_value=False,
    ),
    "logit_bias": SamplingParameterSpec(
        name="logit_bias",
        description='Modify the likelihood of a token appearing in the generated text completion. For example, use `"logit_bias": [[15043,1.0]]` to increase the likelihood of the token \'Hello\', or `"logit_bias": [[15043,-1.0]]` to decrease its likelihood. Setting the value to false, `"logit_bias": [[15043,false]]` ensures that the token `Hello` is never produced. The tokens can also be represented as strings, e.g. `[["Hello, World!",-0.5]]` will reduce the likelihood of all the individual tokens that represent the string `Hello, World!`, just like the `presence_penalty` does.',
        default_value=[],
    ),
    "n_probs": SamplingParameterSpec(
        name="n_probs",
        description="If greater than 0, the response also contains the probabilities of top N tokens for each generated token given the sampling settings. Note that for temperature < 0 the tokens are sampled greedily but token probabilities are still being calculated via a simple softmax of the logits without considering any other sampler settings.",
        default_value=0,
    ),
    "min_keep": SamplingParameterSpec(
        name="min_keep",
        description="If greater than 0, force samplers to return N possible tokens at minimum.",
        default_value=0,
    ),
    "t_max_predict_ms": SamplingParameterSpec(
        name="t_max_predict_ms",
        description="Set a time limit in milliseconds for the prediction (a.k.a. text-generation) phase. The timeout will trigger if the generation takes more than the specified time (measured since the first token was generated) and if a new-line character has already been generated. Useful for FIM applications.",
        default_value=0,
    ),
    # this doesn't even work in llama
    #    "image_data": SamplingParameterSpec(
    #        name="image_data",
    #        description="An array of objects to hold base64-encoded image `data` and its `id`s to be reference in `prompt`. You can determine the place of the image in the prompt as in the following: `USER:[img-12]Describe the image in detail.\nASSISTANT:`. In this case, `[img-12]` will be replaced by the embeddings of the image with id `12` in the following `image_data` array: `{..., \"image_data\": [{\"data\": \"<BASE64_STRING>\", \"id\": 12}]}`. Use `image_data` only with multimodal models, e.g., LLaVA.",
    #        default_value=[]
    #    ),
    "id_slot": SamplingParameterSpec(
        name="id_slot",
        description="Assign the completion task to an specific slot. If is -1 the task will be assigned to a Idle slot.",
        default_value=-1,
    ),
    "cache_prompt": SamplingParameterSpec(
        name="cache_prompt",
        description="Re-use KV cache from a previous request if possible. This way the common prefix does not have to be re-processed, only the suffix that differs between the requests. Because (depending on the backend) the logits are **not** guaranteed to be bit-for-bit identical for different batch sizes (prompt processing vs. token generation) enabling this option can cause nondeterministic results.",
        default_value=True,
    ),
    "return_tokens": SamplingParameterSpec(
        name="return_tokens",
        description="Return the raw generated token ids in the `tokens` field. Otherwise `tokens` remains empty.",
        default_value=False,
    ),
    "samplers": SamplingParameterSpec(
        name="samplers",
        description="The order the samplers should be applied in. An array of strings representing sampler type names. If a sampler is not set, it will not be used. If a sampler is specified more than once, it will be applied multiple times.",
        default_value=["min_p", "xtc", "dry", "temperature"],
    ),
    "timings_per_token": SamplingParameterSpec(
        name="timings_per_token",
        description="Include prompt processing and text generation speed information in each response.",
        default_value=False,
    ),
    "post_sampling_probs": SamplingParameterSpec(
        name="post_sampling_probs",
        description="Returns the probabilities of top `n_probs` tokens after applying sampling chain.",
        default_value=None,
    ),
    "response_fields": SamplingParameterSpec(
        name="response_fields",
        description='A list of response fields, for example: `"response_fields": ["content", "generation_settings/n_predict"]`. If the specified field is missing, it will simply be omitted from the response without triggering an error. Note that fields with a slash will be unnested; for example, `generation_settings/n_predict` will move the field `n_predict` from the `generation_settings` object to the root of the response and give it a new name.',
        default_value=None,
    ),
    "lora": SamplingParameterSpec(
        name="lora",
        description='A list of LoRA adapters to be applied to this specific request. Each object in the list must contain `id` and `scale` fields. For example: `[{"id": 0, "scale": 0.5}, {"id": 1, "scale": 1.1}]`. If a LoRA adapter is not specified in the list, its scale will default to `0.0`. Please note that requests with different LoRA configurations will not be batched together, which may result in performance degradation.',
        default_value=[],
    ),
    # new - only  got this from the git logs
    "add_generation_prompt": SamplingParameterSpec(
        name="add_generation_prompt",
        description="Include the prompt used to generate in the result.",
        default_value=True,
    ),
}
### end of big list

# this is for fast copy and send to backend
default_params = {hp.name: hp.default_value for hp in sampling_parameters.values()}

# some reference lists for convenience
supported_parameters = {
    p: sampling_parameters[p]
    for p in "temperature frequency_penalty presence_penalty max_length repeat_penalty top_p stop".split(
        " "
    )
}
sometimes_parameters = {
    p: sampling_parameters[p]
    for p in "cache_prompt xtc_probability dry_multiplier min_p mirostat mirostat_tau mirostat_eta samplers".split(
        " "
    )
}
sampling_parameter_tags = {
    p.name: ArgumentTag(
        name=p.name,
        type=ArgumentType.Plumbing,
        group=ArgumentGroup.SamplingParameters,
        is_option=True,
        default_value=p.default_value,
        help=p.description,
    )
    for p in sampling_parameters.values()
}
# some special ones
for p in supported_parameters.keys():
    sampling_parameter_tags[p].very_important = True

sampling_parameter_tags["temperature"].type = ArgumentType.Porcelain
sampling_parameter_tags["top_p"].type = ArgumentType.Porcelain


# These don't fit anywhere else and don't really need documentation
special_parameters = {"response_format": {"type": "text"}}


class AIBackend(ABC):
    """Abstract interface to a backend server, like llama.cpp or openai etc. Use the Program.make*Payload methods to make corresponding payloads. All backends must handle those dictionaries, but not all backends may support all features."""

    @abstractmethod
    def __init__(self, endpoint: str, logger: Optional[Callable[[str], None]] = None):
        self.endpoint = endpoint
        self.stream_done = threading.Event()
        self.last_error = ""
        self._last_request = {}
        self._last_result = {}
        self._config = {}
        self.logger = logger

    def log(self, msg: str) -> None:
        if self.logger is not None:
            self.logger(f"[{self.getName()}] " + msg)

    def configure(self, config: Dict[str, Any] = {}) -> Dict[str, Any]:
        """Configure backend specific options thourhg a key -> value dictionary. Returns the current config of a backend.
        To see a list of possible keys, consult the backend specific configure method's documentation, or call this method without arguments. Passing keys to backends that do not support them will have no effect but is otherwise safe.
        Note that this is *not* the way to set various backend server options, like cache_prompt or temperature etc. Those should go in the payload.
        """
        self._config |= config
        return self._config

    def getLastError(self):
        return self.last_error

    def getLastJSON(self) -> Dict:
        """Returns the last json result sent by the backend."""
        return self._last_result

    def getLastRequest(self) -> Dict:
        """Returns the last payload dictionary that was sent to the server."""
        return self._last_request

    def waitForStream(self):
        self.stream_done.wait()

    @abstractmethod
    def getName(self):
        """Returns the name of the backend. This can be compared to the --backend command line option."""
        pass

    @abstractmethod
    def getMaxContextLength(self):
        """Returns the default setting for maximum context length that is set serverside. Often, this should be the maximum context the model is trained on.
        This should return -1 if the backend is unable to determine the maximum context (some backends don't let you query this at all).
        """
        pass

    @abstractmethod
    def generate(self, payload):
        """Takes a payload dictionary similar to default_params. Returns a result object specific to the backend. Use handleResult to unpack its content."""
        pass

    @abstractmethod
    def handleGenerateResult(self, result) -> Optional[Dict]:
        """Takes a result from the generate method and returns the generated string."""
        pass

    @abstractmethod
    def generateStreaming(self, payload, callback=lambda w: print(w)):
        """Takes a payload dictionary similar to default_params and begins streaming generated tokens to a callback function. Returns True if there was a HTTP error, which you can check with getLastError().
        callback - A function taking one string argument, which will be the generated tokens.
        payload - A dictionary similar to default_params. If this doesn't contain "stream" : True, this function may fail or have no effect.
        returns - True on status code != 200"""
        pass

    @abstractmethod
    def tokenize(self, w):
        """Takes a string w and returns a list of tokens as ints."""
        pass

    @abstractmethod
    def detokenize(self, ts):
        """Takes a list of tokens as ints, and returns a string consisting of the tokens."""
        pass

    @abstractmethod
    def health(self):
        """Returns a string indicating the status of the backend."""
        pass

    @abstractmethod
    def timings(self, result_json=None) -> Optional[Timings]:
        """Returns performance statistics for this backend.
        The method can take a json parameter, which may be a return value of the getLastJSON method. In this case, timings for that result are returned.
        Otherwise, if called without the json parameter, timings for the last request are returned.
        The method may return none if there hasn't been a request yet to determine timings for and no json is provided.
        """
        pass

    @abstractmethod
    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        """Returns a dictionary of sampling_parameters that are supported by the model.
        The dictionary has the parameter names as keys. If a sampling_parameter is present in the dict, it is expected to be supported by the various generation methods.
        """
        pass


class LlamaCPPBackend(AIBackend):
    """Bindings for the formidable Llama.cpp based llama-server program."""

    def __init__(self, endpoint: str ="http://localhost:8080", **kwargs):
        super().__init__(endpoint, **kwargs)
        self._config |= {
            # this means we will use /chat/completions, which applies the jinja templates etc. This is most often what we want.
            # if this option is false, the generate methods will use the /completions endpoint, which requires us to apply our own templates, which is great for experimentation.
            "llamacpp_use_chat_completion_endpoint": True
        }
        self.log(f"Initialized llama.cpp backend with config : {json.dumps(self._config)}")
        
    def getName(self):
        return LLMBackend.llamacpp.name

    def getMaxContextLength(self):
        return -1

    def generate(self, payload):
        # adjust slightly for our renames
        llama_payload = payload | {"n_predict": payload["max_length"]}

        if self._config["llamacpp_use_chat_completion_endpoint"]:
            endpoint_suffix = "/chat/completions"
        else:
            endpoint_suffix = "/completion"
            if "tools" in payload:
                printerr(
                    "warning: Tool use with a custom prompt_format and using llama.cpp backend is currently experimental. Set your prompt_format to 'auto' or use the generic backend for a stable experience."
                )

        if "tools" in llama_payload:
            # FIXME: this is because using tools seems to invalidate the cache in llamacpp. probably because they are putting tool instructions in the system prompt. this is an attempt to fix or at least mitigate that.
            # i.e. we can just cache the inevitable streaming, non-tool generation that follows tool use.
            # however this will still suck for multi-turn tool use
            llama_payload |= {"cache_prompt": False}
        self._last_request = llama_payload
        final_endpoint = self.endpoint + endpoint_suffix
        self.log(f"generate to {final_endpoint}")
        self.log(f"Payload: {json.dumps(llama_payload, indent=4)}")        
        return requests.post(final_endpoint, json=llama_payload)

    def handleGenerateResult(self, result):
        if result.status_code != 200:
            self.last_error = "HTTP request with status code " + str(result.status_code)
            return None
        self._last_result = result.json()

        if self._config["llamacpp_use_chat_completion_endpoint"]:
            # this one wants more oai like results
            return OpenAIBackend.handleGenerateResultOpenAI(result.json())

        # handling the /completion endpoint
        if (payload := result.json()["content"]) is not None:
            return payload
        if (payload := result.json().get("tool_calls", None)) is not None:
            return payload
        return None

    def _makeLlamaCallback(self, callback):
        def f(d):
            if d["stop"]:
                self._last_result = d
            callback(d["content"])

        return f

    def generateStreaming(self, payload, callback=lambda w: print(w)):
        self.stream_done.clear()
        llama_payload = payload | {"n_predict": payload["max_length"], "stream": True}

        def one_line_lambdas_for_python(r):
            # thanks guido
            self._last_result = r

        if self._config["llamacpp_use_chat_completion_endpoint"]:
            endpoint_suffix = "/chat/completions"

            final_callback = OpenAIBackend.makeOpenAICallback(
                callback, last_result_callback=one_line_lambdas_for_python
            )
        else:
            endpoint_suffix = "/completion"
            final_callback = self._makeLlamaCallback(callback)

        self._last_request = llama_payload

        final_endpoint = self.endpoint + endpoint_suffix
        self.log(f"generateStreaming to {final_endpoint}")
        self.log(f"Payload: {json.dumps(llama_payload, indent=4)}")        
        r = streamPrompt(
            final_callback,
            self.stream_done,
            final_endpoint,
            llama_payload,
        )
        if r.status_code != 200:
            self.last_error = "streaming HTTP request with status code " + str(
                r.status_code
            )
            self.stream_done.set()
            return True
        return False

    def tokenize(self, w):
        self.log(f"tokenize {len(w)} characters.")
        r = requests.post(self.endpoint + "/tokenize", json={"content": w})
        if r.status_code == 200:
            return r.json()["tokens"]
        return []

    def detokenize(self, ts):
        self.log("detokenize with {len(ts)} tokens.")
        r = requests.post(self.endpoint + "/detokenize", json={"tokens": ts})
        if r.status_code == 200:
            return r.json()["content"]
        return []

    def health(self):
        r = requests.get(self.endpoint + "/health")
        if r.status_code != 200:
            return "error " + str(r.status_code)
        return r.json()["status"]

    def timings(self, result_json=None) -> Optional[Timings]:
        if result_json is None:
            if (json := self._last_result) is None:
                return None
        else:
            json = result_json

        if "timings" not in json:
            printerr("warning: Got weird server result: " + str(json))
            return

        time = json["timings"]
        # these are llama specific fields which aren't always available on the OAI endpoints
        truncated, cached_n = json.get("truncated", None), json.get(
            "tokens_cached", None
        )
        if (verbose := json.get("__verbose", None)) is not None:
            truncated, cached_n = verbose["truncated"], verbose["tokens_cached"]

            return Timings(
                prompt_n=time["prompt_n"],
                predicted_n=time["predicted_n"],
                prompt_ms=time["prompt_ms"],
                predicted_ms=time["predicted_ms"],
                predicted_per_token_ms=time["predicted_per_token_ms"],
                predicted_per_second=time["predicted_per_second"],
                truncated=truncated,
                cached_n=cached_n,
                original_timings=time,
            )

    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        # llamacpp params are the default
        return sampling_parameters


class OpenAILegacyBackend(AIBackend):
    """Backend for the official OpenAI API. The legacy version routes to /v1/completions, instead of the regular /v1/chat/completion."""

    def __init__(self, api_key: str, endpoint:str="https://api.openai.com", **kwargs):
        super().__init__(endpoint, **kwargs)
        self.api_key = api_key
        self.log(f"Initialized legacy OpenAI backend. This routes to /v1/completion and will not apply the chat template. Using config : {json.dumps(self._config)}")
        
    def getName(self):
        return LLMBackend.legacy.name

    def getMaxContextLength(self):
        return -1

    def generate(self, payload):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = payload | {"max_tokens": payload["max_length"], "stream": False}
        self._last_request = data
        final_endpoint = self.endpoint + "/v1/completions"
        self.log(f"generate to {final_endpoint}")
        self.log(f"Payload: {json.dumps(data, indent=4)}")        
        response = requests.post(
            final_endpoint, headers=headers, json=data
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            return None
        self._lastResult = response.json()
        return response.json()

    def handleGenerateResult(self, result):
        if not result:
            return None

        if (payload := result["choices"][0]["message"]["content"]) is not None:
            return payload
        if (payload := result["choices"][0]["message"]["tool_calls"]) is not None:
            return payload
        return None

    def generateStreaming(self, payload, callback=lambda w: print(w)):
        self.stream_done.clear()
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = payload | {"max_tokens": payload["max_length"], "stream": True, "stream_options": {"include_usage": True}}
        self._last_request = data

        def openaiCallback(d):
            callback(d["choices"][0]["text"])
            self._last_result = d            


        final_endpoint = self.endpoint + "/v1/completions"
        self.log(f"generateStreaming to {final_endpoint}.")
        self.log(f"Payload: {json.dumps(data, indent=4)}")
        response = streamPrompt(
            openaiCallback,
            self.stream_done,
            final_endpoint,
            json=data,
            headers=headers,
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            self.stream_done.set()
            return True
        return False

    def tokenize(self, w):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"prompt": w}
        self.log(f"tokenize with {len(w)} characters.")
        response = requests.post(
            self.endpoint + "/v1/tokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["tokens"]
        return []

    def detokenize(self, ts):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"tokens": ts}
        self.log(f"detokenize with {len(ts)} tokens.")
        response = requests.post(
            self.endpoint + "/v1/detokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["content"]
        return []

    def health(self):
        # OpenAI API does not have a direct health check endpoint
        return "OpenAI API is assumed to be healthy."

    def timings(self, result_json=None) -> Optional[Timings]:
        return OpenAIBackend.timings(self, result_json)


    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        # restricted set
        # i just can't be bothered to test this
        supported = supported_parameters.keys()
        return {
            hp.name: hp for hp in sampling_parameters.values() if hp.name in supported
        }


class OpenAIBackend(AIBackend):
    """Backend for the official OpenAI API. This is used for the company of Altman et al, but also serves as a general purpose API suported by various backends, including llama.cpp, llama-box, and many others."""

    def __init__(self, api_key: str, endpoint:str="https://api.openai.com", **kwargs):
        super().__init__(endpoint, **kwargs)
        self.api_key = api_key
        self._memoized_params = None
        api_str = "" if not(api_key) else " with api key " + api_key[:4] + ("x" * len(api_key[4:]))
        self.log(f"Initialized OpenAI compatible backend {api_str}. Routing to {endpoint}. Config is {self._config}")
                                                                            
    def getName(self):
        return LLMBackend.openai.name

    def getMaxContextLength(self):
        return -1

    def generate(self, payload):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = payload | {"max_tokens": payload["max_length"], "stream": False}

        if "tools" in data:
            # see the llamacpp generate method fixme
            # this has no effect on the official OAI api anyway
            data |= {"cache_prompt": False}

        self._last_request = data
        final_endpoint = self.endpoint + "/v1/chat/completions"
        self.log(f"generate to {final_endpoint}.")
        self.log(f"Payload: {json.dumps(data, indent=4)}")        
        response = requests.post(
            final_endpoint, headers=headers, json=data
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            return None
        self._last_result = response.json()
        return response.json()

    def handleGenerateResult(self, result):
        # this is just so that others can use the openai specific handling, which is kind of an industry standard
        return self.handleGenerateResultOpenAI(result)

    @staticmethod
    def handleGenerateResultOpenAI(result: Dict[str, Any]) -> Any:
        # used to be Optional[Dict[str, Any]]:
        # now it's Optional[str|Dict]
        # it's not completely terrible, since it makes sense - either return the text that the AI generated, or a dict if it was tools, or none on error
        # but still, needs a rework FIXME
        if not result:
            return None

        if (
            payload := result["choices"][0]["message"].get("content", None)
        ) is not None:
            return payload
        if result["choices"][0]["message"].get("tool_calls", None) is not None:
            # consumers of this like applyTools expect a dict here
            # FIXME: this is a bad function since it returns sometimes str sometimes dict. this is a big refactor though, as it would involve rewriting a bunch of internal types in pydantic
            return result
        return None

    @staticmethod
    def makeOpenAICallback(callback, last_result_callback=lambda x: x):
        def openAICallback(d):
            last_result_callback(d)
            choice = d["choices"][0]
            maybeChunk = choice["delta"].get("content", None)
            if maybeChunk is not None:
                callback(maybeChunk)

        return openAICallback

    def generateStreaming(self, payload, callback=lambda w: print(w)):
        self.stream_done.clear()
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = payload | {"max_tokens": payload["max_length"], "stream": True, "stream_options": {"include_usage": True}}
        self._last_request = data

        def one_line_lambdas_for_python(r):
            self._last_result = r


        final_endpoint = self.endpoint + "/v1/chat/completions"
        self.log(f"generateStreaming to {final_endpoint}.")
        self.log(f"Payload: {json.dumps(data, indent=4)}")        
        response = streamPrompt(
            self.makeOpenAICallback(
                callback, last_result_callback=one_line_lambdas_for_python
            ),
            self.stream_done,
            final_endpoint,
            json=data,
            headers=headers,
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            self.stream_done.set()
            return True
        return False

    def tokenize(self, w):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"prompt": w}
        self.log(f"tokenize with {len(w)} characters.")
        response = requests.post(
            self.endpoint + "/v1/tokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["tokens"]
        return []

    def detokenize(self, ts):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"tokens": ts}
        self.log(f"detokenize with {len(ts)} tokens.")
        response = requests.post(
            self.endpoint + "/v1/detokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["content"]
        return []

    def health(self):
        # OpenAI API does not have a direct health check endpoint
        return "OpenAI API is assumed to be healthy."


    def timings(self, result_json=None) -> Optional[Timings]:
        if result_json is None:
            if (json := self._last_result) is None:
                return None
        else:
            json = result_json


        if "__verbose" in json:
            verbose = json["__verbose"]
            time = verbose["timings"]
            truncated = verbose["truncated"]
            cached = verbose["tokens_cached"]
        elif "timings" in json:
            time = json["timings"]
            truncated = False
            cached = None            
        else:
            return None

        return Timings(
            prompt_n=time["prompt_n"],
            predicted_n=time["predicted_n"],
            prompt_ms=time["prompt_ms"],
            predicted_ms=time["predicted_ms"],
            predicted_per_token_ms=time["predicted_per_token_ms"],
            predicted_per_second=time["predicted_per_second"],
            # unfortunately openai don't reveal these, unless we got __verbose
            truncated=truncated,
            cached_n=cached,
            original_timings=time,
        )

    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        # I don't like doing this everytime
        if self._memoized_params is not None:
            return self._memoized_params

        # this is tricky because it really depends on the actual backend.
        # the openai class really is not specific enough for this
        supported = supported_parameters.keys()
        sometimes = sometimes_parameters.keys()
        d = {hp.name: hp for hp in sampling_parameters.values() if hp.name in supported}
        for param in sometimes:
            sp = sampling_parameters[param]
            d[param] = SamplingParameterSpec(
                name=sp.name,
                default_value=sp.default_value,
                description=sp.description
                + "\nNote: May not be supported in this backend. For full support, try out llama.cpp https://github.com/ggml-org/llama.cpp",
            )

            self._memoized_params = d
            return d

```

## commands.py

```python
import os, datetime, glob, sys, requests, traceback, random, json
from ghostbox.session import Session
from ghostbox.util import *
from ghostbox.StoryFolder import *
from ghostbox.definitions import *
from ghostbox.api_internal import *


def newSession(program, argv, keep=False):
    """CHARACTER_FOLDER
    Start a new session with the character or template defined in CHARACTER_FOLDER. You can specify a full path, or just the folder name, in which case the program will look for it in all folders specified in the 'include' paths. See /lsoptions include.
    """
    if argv == []:
        if program.getOption("character_folder"):
            # synonymous with /restart
            argv.append(program.getOption("character_folder"))
        else:
            return "No path provided. Cannot Start."

    filepath = " ".join(argv)
    return start_session(program, filepath, keep=keep)


def printStory(prog, argv, stderr=False, apply_filter=True):
    """[FILENAME]
    Print the current story.
    If FILENAME is provided, save the story to that file."""
    # apply_filter basically means make it pretty
    if apply_filter:
        w = prog.formatStory(with_color=prog.getOption("color"))
    else:
        w = prog.showStory(append_hint=False)

    if stderr:
        printerr(w, prefix="")
        return ""
    elif argv != []:
        filename = " ".join(argv)
        if os.path.isdir(filename):
            return "error: Cannot write to file " + filename + ": is a directory."
        if os.path.isfile(filename):
            printerr("warning: File " + filename + " exists. Appending to file.")
            pre = open(filename, "r").read() + "\n--- new story ---"
        else:
            pre = ""
        f = open(filename, "w")
        f.write(pre + w)
        return ""
    else:
        # if stderr=False, we actually want this to go to stdout
        print(w, end="")
        return ""


def doContinue(prog, argv):
    """
    Continue generating without new input. Use this whenever you want the AI to 'just keep talking'.
    Specifically, this will send the story up to this point verbatim to the LLM backend. You can check where you are with /log.
    If there are templated tokens that are normally inserted as part of the prompt format when a user message is received, they are not inserted. Existing end tokens, like <|im_end|> will be removed before sending the prompt back.
    This command is also executed when you hit enter without any text."""

    prog.setOption("continue", "1")
    # FIXME: often we just get EOS. this is sort of risky though.
    # prog.setOption("ignore_eos", True)
    if prog.session.stories.empty():
        return ""

    return ""


def setOption(prog, argv):
    """OPTION_NAME [OPTION_VALUE]
    Set options during program execution. To see a list of all possible values for OPTION_NAME, do /lsoptions. OPTION_VALUE must be a valid python expression, so if you want to set e.g. chat_user to Bob, do /set chat_user \"Bob\".
    When OPTION_VALUE is omitted, the value is set to \"\". This is equivalent to /unset OPTION_NAME.
    """

    if argv == []:
        return showOptions(prog, [])
    name = argv[0]
    if len(argv) == 1:
        prog.setOption(name, "")
        return ""

    w = " ".join(argv[1:])
    try:
        prog.setOption(name, eval(w))
    except:
        printerr(traceback.format_exc())
        return "Couldn't set " + name + " to " + w + ". Couldn't evaluate."
    return ""


def showOptions(prog, argv):
    """ [OPTION_NAME]
    Displays the list of program options, along with their values. Provide OPTION_NAME to see just its value.
    The options displayed can all be set using /set OPTION_NAME.
    Almost all of them may also be provided as command line arguments with preceding dashes, e.g. include as --include=/some/path.
    Finally, they may be set in various config files, such as in character folders, or in a config file loaded with /load.
    """
   
    if argv == []:
        target = ""
    elif argv[0] == "--emacs":
        # undocumented, outputs all options in a neat form to put as keywords in ghostbox.el
        w = "`("
        for name in prog.options.keys():
            w += '"' + name + '", '
        w = w[:-2] + ")"
        printerr(w)
        return ""
    else:
        target = " ".join(argv)

    w = ""
    for k, v in prog.options.items():
        if target in k:
            w += k + ": " + str(v) + "\n"
    return w.strip()


def showVars(prog, argv):
    """[VARIABLE_NAME]
    Shows all session variables and their respective values. Show a single variable if VARIABLE_NAME is provided.
    Variables are automatically expanded within text that is provided by the user or generated by the LLM backend.
    E.g. if variable favorite_fruit is set to 'Tomatoe', any occurrence of {{favorite_fruit}} in the text will be replaced by 'Tomatoe'.
    There are three ways to set variables:
      1. Loading a character folder (/start). All files in a character folder automatically become variables, with the respective filename becoming the name of the variable, and the file's content becoming the value of the variable.
      2. The -x or --varfile command line option. This argument provides additional files that may serve as variables, similar to (1). The argument may be repeated multiple times.
      3. API only: Using the set_vars method on a Ghostbox object.

    You can also do {[FILENAME]} to ad-hoc splice the contents of FILENAME into a prompt. However, due to security reasons, this only works at the CLI.
    """

    if argv == []:
        return "\n".join(
            [f"{key}\t{value}" for key, value in prog.session.fileVars.items()]
        )

    k = " ".join(argv)
    return f"{prog.session.fileVars.get(k, f"Could not find var '{k}'")}"


def showChars(prog, argv):
    """
    Lists all available character folders. These can be used with the /start command or the -c command line argument, and are valid values for the character_folder parameter in the Ghostbox API.
    To see what places are searched for characters, see the value of the 'include' option.
    """
    return "\n".join(sorted(all_chars(prog)))


def showTemplates(prog, argv):
    """
    Lists all available templates for prompt formats.
    To see the places searched, check the value of the template_include option."""
    allchars = []
    for dir in prog.getOption("template_include"):
        if not (os.path.isdir(dir)):
            printerr("warning: template Include path '" + dir + "' is not a directory.")
            continue
        for charpath in glob.glob(dir + "/*"):
            if os.path.isfile(charpath):
                continue
            allchars.append(os.path.split(charpath)[1])
    return "\n".join(sorted(allchars))


def showVoices(prog, argv):
    """
    List all available voices for the TTS program. These can be used with /set tts_voice.
    To see the places searched for voices, see the value of tts_voice_dir.
    Depending on the value of tts_program, that location may be meaningless, as the voice won't be file based, like with amazon polly voices. In this case /lsvoices tries to give a helpful answer if it can.
    """
    return "\n".join(getVoices(prog))


def toggleMode(prog, argv):
    """[MODE_NAME}
    Put the program into the specified mode, or show the current mode if MODE_NAME is omitted.
    Possible values are 'default', or 'chat'.
    Setting a certain mode has various effects on many aspects of program execution. Currently, most of this is undocumented :)
    """
    if argv == []:
        return "Currently in " + prog.getOption("mode") + " mode."

    mode = " ".join(argv)
    if prog.isValidMode(mode):
        prog.setMode(mode)
        return mode + " mode on"
    return "Not a valid mode. Possible values are 'default' or 'chat'"


def toggleTTS(prog, argv):
    """
    This turns the TTS (text-to-speech) module on or off. When TTS is on, text that is generated by the LLM backend will be spoken out loud.
    The TTS service that will be used depends on the value of tts_program. The tts_program can be any executable or shell script that reads lines from standard input. It may also support additional functionality.
    An example tts program for amazon polly voices is provided with 'ghostbox-tts-polly'. Note that this requires you to have credentials with amazon web services.
    On linux distributions with speech-dispatcher, you can set the value of tts_program to 'spd-say', or 'espeak-ng' if it's installed. This works, but it isn't very nice.
    The voice used depends on the value of tts_voice, which you can either /set or provide with the -V or --tts_voice command line option. It can also be set in character folder's config.json.
    ghostbox will attempt to provide the tts_program with the voice using a -V command line argument.
    To see the list of supported voices, try /lsvoices.
    Enabling TTS will automatically set stream_flush to 'sentence', as this works best with most TTS engines. You can manually reset it to 'token' if you want, though.
    """
    return toggle_tts(prog)


def ttsDebug(prog, argv):
    """
    Get stdout and stderr from the underlying tts_program when TTS is enabled."""
    if not (prog.tts):
        return "TTS is None."

    w = "tts_program: " + prog.getOption("tts_program")
    w += "\ntts exit code: " + str(prog.tts.exit_code())
    w += "\ntts config: "
    if prog.tts_config is None:
        w += "unavailable"
    else:
        w += json.dumps(prog.tts_config, indent=4)
    printerr(w)

    w = "\n### STDOUT ###\n"
    w += "".join(prog.tts.get())
    w += "\n### STDERR ###\n"
    w += "".join(prog.tts.get_error())
    printerr(w, prefix="")
    return ""


def nextStory(prog, argv):
    """
    Go to next branch in story folder."""
    r = prog.session.stories.nextStory()
    if r == 1:
        return "Cannot go to next story branch: No more branches. Create a new branch with /new or /retry."
    return "Now on branch " + str(prog.session.stories.index)


def previousStory(prog, argv):
    """
    Go to previous branch in story folder."""
    r = prog.session.stories.previousStory()
    if r == -1:
        return "Cannot go to previous story branch: No previous branch exists."
    return "Now on branch " + str(prog.session.stories.index)


def retry(prog, argv, predicate=lambda item: item.role == "user"):
    """
        Retry generation of the LLM's response.
    This will drop the last generated response from the current story and generate it again. Use this in most cases where you want to regenerate. If you extended the LLM's repsone (with /cont or hitting enter), the entire repsonse will be regenerated, not just the last part.
    Note that /retry is not destructive. It always creates a new story branch before regenerating a repsonse.
        See also: /rephrase."""
    prog.session.stories.cloneStory()
    prog.session.stories.get().dropUntil(predicate)
    doContinue(prog, [])
    printerr("Now on branch " + str(prog.session.stories.index))
    return ""


def rephrase(prog, argv):
    """
    This will rewind the story to just before your last input, allowing you to rephrase your query.
        Note that /rephrase is not destructive. It will always create a new story branch before rewinding.
        See also: /retry"""
    prog.session.stories.cloneStory()
    story = prog.session.stories.get()
    story.dropUntil(lambda item: item.role == "user")
    story.dropUntil(lambda item: item.role == "assistant")
    printerr("Now on branch " + str(prog.session.stories.index))
    return ""


def dropEntry(prog, argv):
    """
    Drops the last entry from the current story branch, regardless of wether it was user provided or generated by the LLM backend.
    """
    prog.session.stories.cloneStory()
    prog.session.stories.get().drop()
    return (
        "Now on branch " + str(prog.session.stories.index) + " with last entry dropped."
    )


def newStory(prog, argv):
    """
    Create a new, empty branch in the story folder. You can always go back with /prev or /story.
    """
    prog.session.stories.newStory()
    return "Now on branch " + str(prog.session.stories.index) + " with a clean log."


def cloneStory(prog, argv):
    """
    Create a new branch in the story folder, copying the contents entirely from the current story.
    """
    prog.session.stories.cloneStory()
    return (
        "Now on branch "
        + str(prog.session.stories.index)
        + " with a copy of the last branch."
    )


def saveStoryFolder(prog, argv):
    """[STORY_FOLDER_NAME]
    Save the entire story folder in the file STORY_FOLDER_NAME. If no argument is provided, creates a file with the current timestampe as name.
    The file created is your accumulated chat history in its entirety, including the current story and all other ones (accessible with /prev and /next, etc). It is saved in the json format.
    A saved story folder may be loaded with /load."""
    if len(argv) > 0:
        name = " ".join(argv)
    else:
        name = datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S") + ".json"

    filename = saveFile(name, prog.session.stories.toJSON())
    if not (filename):
        return "Could not save story folder. Maybe provide a different filename?"
    return "Saved story folder as " + filename


def changeTemplate(prog, argv):
    """[PROMPT_TEMPLATE]
    Shows the current prompt template. If PROMPT_TEMPLATE is supplied, tries to load that template.
    LLMs usually work best when supplied with input that is formatted similar to their training data. Prompt templates apply some changes to your inputs in the background to get them into a shape that the LLM expects.
        To find out the right template to use, consult the model card of the LLM you are using. When in doubt, 'chat-ml' is a very common prompt format.
    To disable this, set the template to 'raw' and you won't have anything done to your inputs. This can be useful for experimentation.
    Templates are searched for in the directories specified by the template_include option, which can be supplied at the command line.
    To get a full list of available templates, try /lstemplates ."""

    if argv == []:
        return prog.getOption("prompt_format")

    choice = " ".join(argv)
    prog.loadTemplate(choice)
    return ""


def loadStoryFolder(prog, argv):
    """STORY_FOLDER_NAME
    Loads a previously saved story folder from file STORY_FOLDER_NAME. See the /save command on how to save story folders.
    A story folder is a json file containing the entire chat history."""
    if len(argv) == 0:
        return "Please provide a legal json filename in the story-folder format."

    filename = " ".join(argv)
    try:
        w = open(filename, "r").read()
        s = StoryFolder(json_data=w)
    except FileNotFoundError as e:
        return "Could not load " + filename + ": file not found."
    except Exception as e:
        printerr(str(e))
        return "Could not load story folder: Maybe bogus JSON?"
    prog.session.stories = s
    return (
        "Ok. Restored "
        + filename
        + "\nNow on branch "
        + str(prog.session.stories.index)
    )


def gotoStory(prog, argv):
    """[BRANCH_NUMBER]
    Moves to a different branch in the story folder. If BRANCH_NUMBER is omitted, shows the current branch instead.
    """
    if argv == []:
        return "Currently on branch " + str(prog.session.stories.index)

    w = " ".join(argv)
    try:
        n = int(w)
    except:
        return "Cannot go to that branch: Invalid Argument."

    err = prog.session.stories.shiftTo(n)
    if not (err):
        return ""
    return "Could not go to branch " + str(n) + ": " + err


def loadConfig(prog, argv, override=True, protected_keys=[]):
    """CONFIG_FILE
    Loads a json config file at location CONFIG_FILE. A config file contains a dictionary of program options.
    You can create an example config with /saveconfig example.conf.json.
    You can also load a config file at startup with the --config_file command line argument.
    If it exists, the ~/.ghostbox.conf.json will also be loaded at startup.
    The order of config file loading is as follows .ghostconf.conf.json > --config_file > conf.json (from character folder). Config files that are loaded later will override settings from earlier files.
    """

    # loads a config file, which may be a user config, or a config supplied by a character folder. if override is false, the config may not override command line arguments that have been manually supplied (in the long form)
    if argv == []:
        return "Please provide a filename for the config file to load."
    filename = " ".join(argv)
    return load_config(prog, filename, override=override, protected_keys=protected_keys)


def saveConfig(prog, argv):
    """CONFIG_FILE
    Save the current program options and their values to the file at location CONFIG_FILE. This will either create or overwrite the CONFIG_FILE, deleting all its previous contents.
    """

    if argv == []:
        name = "config-" + datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
    else:
        name = " ".join(argv)

    if not (name.endswith(".json")):
        name = name + ".json"

    filename = saveFile(name, json.dumps(prog.options, indent=4))
    if filename:
        return "Saved config to " + filename
    return "error: Could not save config."


def hide(prog, argv):
    """
    Hide various program outputs and change some variables for a less verbose display.
    This does nothing that you cannot achieve by manually setting several options, it just bundles an eclectic mix of them in one command.
    I like to use this with TTS for a more immersive experience."""
    # this is just convenient shorthand for when I want my screen reader to be less spammy
    hide_some_output(prog)
    return ""


def varfile(prog, argv):
    # $FIXME:
    return "Not implemented yet. Use the -V option for now."


def exitProgram(prog, argv):
    """
    Quit the program. Chat history (story folder) is discarded. All options are lost.
    See also /save, /saveoptions, /saveconfig"""
    prog.stopAudioTranscription()
    prog.stopImageWatch()
    prog.running = False
    return ""


def transcribe(prog, argv):
    """Records using the microphone until you hit enter. The recording is then transcribed using openai's whisper, and inserted into the current line at the CLI.
    THe precise model to be used for transcribing is determined using the 'whisper_model' option. Larger models transcribe more accurately, and may handle more languages, but also consume more resources. See https://huggingface.co/openai/whisper-large for a list of model names. The default is 'base.en'.
    The model will be automatically downloaded the first time you transcribe with it. This may take a moment, but will only happen once for each model.
    """
    w = prog.whisper.transcribeWithPrompt()
    sys.stdout.write("\r" + w + "\n")
    prog.continueWith(w)
    return ""


def toggleAudio(prog, argv):
    """Enable/disable audio input. This means the program will automatically record audio and transcribe it using the openai whisper model. A query is send to the backend with transcribed speech whenever a longish pause is detected in the input audio.
    See whisper_model for the model used for transcribing."""
    if prog.isAudioTranscribing():
        prog.stopAudioTranscription()
    else:
        prog.startAudioTranscription()
    return ""


def image(prog, argv):
    """[--id=IMAGE_ID] IMAGE_PATH
    Add images for multimodal models that can handle them. You can refer to images by their id in the form of `[img-ID]`. If --id is omitted, id= 1 is assumed. Examples:
    ```
        /image ~/Pictures.test.png
    Please describe [img-1].
        ```

        Alternatively, with multiple images:
        ```
        /image --id=1 ~/Pictures.paris.png
        /image --id=2 ~/Pictures/berlin.png
    Can you compare [img-1] and [img-2]?
        ```"""

    if argv == []:
        ws = []
        for id, imagedata in sorted(prog.images.items()):
            ws.append(mkImageEmbeddingString(id) + "\t" + imagedata["url"])
        # FIXME: secret, undocumented function: /image without arguments dirties the image cache
        prog._images_dirty = True
        return "\n".join(ws)

    if argv[0].startswith("--id="):
        id = maybeReadInt(argv[0].replace("--id=", ""))
        if id is None:
            return "error: Please specify a valid integer as id."
        url = " ".join(argv[1:])
    else:
        id = 1
        url = " ".join(argv)

    prog.loadImage(url, id)
    return ""


def debuglast(prog, argv):
    """
    Dumps a bunch of information about the last result received. Note that this won't do anything if you haven't sent a request to a working backend server that answered you.
    """
    if not (r := prog.lastResult):
        return "Nothing."

    acc = []
    for k, v in r.items():
        acc.append(k + ": " + str(v))
    return "\n".join(acc)


def lastRequest(prog, argv):
    """
    Dumps a bunch of information about the last request send. Note that this won't do anything if you haven't sent a request to a working backend server that answered you.
    """
    r = prog.getBackend().getLastRequest()
    if not (r):
        return "Nothing."

    return json.dumps(r, indent=4)


def showTime(prog, argv):
    """
    Show some performance stats for the last request."""
    r = prog.getBackend().timings()
    if not (r):
        return "No time statistics. Either no request has been sent yet, or the backend doesn't support timing."

    w = ""
    # timings: {'predicted_ms': 4115.883, 'predicted_n': 300, 'predicted_per_second': 72.88836927580303, 'predicted_per_token_ms': 13.71961, 'prompt_ms': 25.703, 'prompt_n': 0, 'prompt_per_second': 0.0, 'prompt_per_token_ms': None}
    # caching etc
    w += "generated: " + str(r.predicted_n)
    w += ", evaluated: " + str(r.prompt_n)
    w += (
        ", cached: " + (str(r.cached_n) if r.cached_n is not None else "unknown") + "\n"
    )
    w += (
        "context: "
        + str(r.total_n())
        + " / "
        + str(prog.getOption("max_context_length"))
        + ", exceeded: "
        + str(r.truncated)
    )
    if prog._smartShifted:
        w += "(smart shifted)\n"
    else:
        w += "\n"

    factor = 1 / 1000
    unit = "s"
    prep = lambda u: str(round(u * factor, 2))
    w += prep(r.prompt_ms) + unit + " spent evaluating prompt.\n"
    w += prep(r.predicted_ms) + unit + " spent generating.\n"
    w += prep(r.total_ms()) + unit + " total processing time.\n"
    w += (
        str(round(r.predicted_per_second, 2))
        + "T/s, "
        + prep(r.predicted_per_token_ms)
        + unit
        + "/T"
    )
    return w


def showStatus(prog, argv):
    """
    Give an overall report about the program and some subprocesses."""
    if argv == []:
        topics = set("backend mode tts audio image_watch streaming".split(" "))
    else:
        topics = set(argv)

    w = ""
    if "backend" in topics:
        w += (
            "backend: "
            + prog.getOption("backend")
            + " at "
            + prog.getOption("endpoint")
            + "\n"
        )
        w += "backend status: " + str(prog.getBackend().health()) + "\n"
        w += "max_context_length: " + str(prog.getOption("max_context_length"))
        if prog._dirtyContextLlama:
            # context has been set by server
            w += " (set by llama.cpp)\n"
        else:
            w += "\n"

        w += " models\n"
        models = dirtyGetJSON(prog.getOption("endpoint") + "/v1/models").get("data", [])
        for m in models:
            w += " .. " + m["id"] + "\n\n"

    if "mode" in topics:
        w += "mode: " + prog.getOption("mode") + "\n"
        w += "\n"
        # FIXME: more mode stuff here

    if "tts" in topics:
        w += "tts: " + str(prog.getOption("tts")) + "\n"
        w += "tts_program: " + prog.getOption("tts_program") + "\n"
        w += "tts status: "
        if prog.tts is None:
            w += "uninitialized"
        else:
            if prog.tts.is_running():
                w += "running"
            else:
                w += "exit code " + str(prog.tts.exit_code())
        w += "\n\n"

    if "audio" in topics:
        w += "audio transcription: " + str(prog.getOption("audio")) + "\n"
        w += "whisper_model: " + prog.getOption("whisper_model") + "\n"
        w += "continuous record / transcribe status: "
        if prog.ct is None:
            w += "N/A\n"
        else:
            if prog.ct.running:
                w += "running"
                if prog.ct.isPaused():
                    w += " (paused)"
                w += "\n"
            else:
                w += "stopped\n"
        w += "\n"

    if "image_watch" in topics:
        w += "image_watch: " + str(prog.getOption("image_watch")) + "\n"
        w += "image_watch_dir: " + prog.getOption("image_watch_dir") + "\n"
        w += "status: "

        if prog.image_watch is None:
            w += "N/A\n"
        else:
            if prog.image_watch.running:
                w += "running\n"
            else:
                w += "halted"
        w += "\n"

        if "streaming" in topics:
            w += "streaming: " + str(prog.getOption("stream")) + "\n\n"

    return w.strip()


def toggleImageWatch(prog, argv):
    """[DIR]
    Enable / disable automatic watching for images in a specified folder.
        When new images are created / modified in image_watch_dir, a message (image_watch_msg)is automatically sent to the backend.
        If DIR is provided to this command, image_watch_dir will be set to DIR. Otherwise, the preexisting image_watch_dir is used, which defaults to the user's standard screenshot folder.
        This allows you to e.g. take screenshots and have the TTS automatically describe them without having to switch back to this program.
        Check status with /status image_watch."""
    dir = " ".join(argv)
    if dir != "":
        if not (os.path.isdir(dir)):
            return "error: Could not start image_watch: " + dir + " is not a directory."
        prog.setOption("image_watch", dir)

    # toggle
    prog.options["image_watch"] = not (prog.getOption("image_watch"))
    if prog.getOption("image_watch"):
        prog.startImageWatch()
    else:
        prog.stopImageWatch()
        return "image_watch off."
    return ""


def showRaw(prog, argv):
    """
    Displays the raw output for the last prompt that was sent to the backend."""
    printerr(prog._lastPrompt, prefix="")
    return ""


def switch(prog, argv):
    """CHARACTER_FOLDER
    Switch to another character folder, retaining the current story.
    As opposed to /start or /restart, this will hot-switch the AI without wiping your story so far, or adding the initial message. This can e.g. allow the used of specialized AI's in certain situations.
    Specifically, '/switch bob' will do the following:
     - Set your system prompt to bob/system_msg
     - Set all session variables defined in bob/, possibly overriding existing ones
     - Load bob/config.json, if present, not overriding command line arguments.
    See also: /start, /restart, /lschars"""
    if argv == []:
        return "No character folder given. No switch occured."

    prog._lastChar = prog.session.dir
    w = newSession(prog, argv, keep=True)
    # FIXME: when people use this command they probably don't want to be spammed, so we discard w, but maybe we want a verbose mode?
    return w.split("\n")[-1]


def tokenize(prog, argv):
    """[-c] MSG
    Send a tokenize request to the server. Will print raw tokens to standard output, one per line. This is mostly used to debug prompts. With -c, it will print the number of tokens instead.
    """
    if argv != [] and argv[0] == "-c":
        count = True
        argv = argv[1:]
    else:
        count = False

    ts = prog.getBackend().tokenize("".join(argv))
    if count:
        return str(len(ts))

    for t in ts:
        print(t)
    return ""


def detokenize(prog, argv):
    """[-n]
    Turn a list of tokens into strings.
    Tokens can be supplied like this /detokenize 23 20001 1
        If -n is supplied, reads one token per line until an empty line is found."""
    ts = []
    if argv != [] and argv[0] == "-n":
        # one per line
        argv = []
        while True:
            w = input()
            if w == "":
                break
            argv.append(w)

    for arg in argv:
        try:
            t = int(arg)
        except:
            return "Please specify tokens as integers, seperated by spaces, or newlines in case you supplied -n."
        ts.append(t)

    w = prog.backend.detokenize(ts)
    print(w)
    return ""


def testQuestion(prog, argv):
    """
    Send a random test question to the backend.
    The question is pulled from a random set of example questions. These are sometimes funny, but also turn out to be quite useful to get a general sense of a model.
    """

    questions = """I have 8 eggs, 4 water bottles, and 1 laptop. Suggest to me a configuration of these objects with which to balance them on top of another.
How much does 1 kilogram of feathers weigh?
I have 3 apples. I give 2 to timmy. How many apples do I have?
Me and my friends have a rule: "If you borrow a sweater, then you have to return it." Last month I got Jane's sweater. I did not return it. What do I have to do now?
Youâ€™re in a desert walking along in the sand when all of the sudden you look down, and you see a tortoise, itâ€™s crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it canâ€™t, not without your help. But youâ€™re not helping. Why is that?
Describe in single words, only the good things that come into your mind about your mother.
In a magazine you come across a full-page color picture of a nude girl. Your husband likes the picture. The girl is lying facedown on a large and beautiful bearskin rug. Your husband hangs the picture up on the wall of his study. How do you feel?
A young boy shows you his butterfly collection, including the killing jar. How do you feel?
Youâ€™re reading a novel written in the old days before the war. The characters are visiting Fishermanâ€™s Wharf in San Francisco. They become hungry and enter a seafood restaurant. One of them orders lobster, and the chef drops the lobster into the tub of boiling water while the characters watch. What do you think about this?
You are watching an old movie on TV, a movie from before the war. It shows a banquet in progress; the guests are enjoying raw oysters. The entrÃ©e consists of boiled dog, stuffed with rice. Are raw oysters more acceptable to you than a dish of boiled dog?""".split(
        "\n"
    )

    w = random.choice(questions)
    # no prefix
    print(w)
    prog.continueWith(w)
    return ""


cmds_additional_docs = {
    "/log": """
    Prints the raw log of the current story branch.
    This includes prompt-format tokens and other stuff that is normally filtered out. For a prettier display, see /print.
    Also, /log prints to stderr, while /print will output to stdout.""",
    "/resend": retry.__doc__,
    "/rephrase": retry.__doc__,
    "/restart": """
    Restarts the current character folder. Note that this will wipe the current story folder, i.e. your chat history, so you may want to /save.
    /restart is equivalent to /start CURRENT_CHAR_FOLDER.""",
}

```

## definitions.py

```python
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field
from pydantic.types import Json
from typing import *


class Property(BaseModel):
    description: str
    type: str


class Parameters(BaseModel):
    type: str = "object"
    properties: Dict[str, Property]
    required: List[str] = []


class Function(BaseModel):
    name: str
    description: str
    # this wants jsonschema object
    parameters: Parameters


class Tool(BaseModel):
    type: str = "function"
    function: Function


class ImageRef(BaseModel):
    """This is used internally to represent an image context. It is usually discarded after one use.
    To see how images are saved in the history, see ImageContent, ChatContentComplex etc.
    """

    # This is an actual URL, e.g. a filepath
    url: str
    # the base64 encoded binary data.
    data: bytes


class ImageURL(BaseModel):
    # confusingly, this may be an URL, or just base64 image data
    # in this format:
    # f"data:image/{ext};base64,{base64_image}"
    url: str


class ChatContentComplex(BaseModel):
    """Contentfield of a ChatMessage, at least when the content is not a mere string."""

    type: Literal["text", "image_url"]
    content: str
    image_url: Optional[ImageURL] = None


ChatContent = str | List[ChatContentComplex] | Dict


class FunctionCall(BaseModel):
    name: str
    # this is weird but it really is str, no idea why not dict
    arguments: str


class ToolCall(BaseModel):
    type: str = "function"
    function: FunctionCall
    # FIXME: I don't quite understand id field yet
    id: str = ""


class ChatMessage(BaseModel):
    role: Literal["system", "assistant", "user", "tool"]
    content: Optional[ChatContent] = None
    tool_calls: List[ToolCall] = []
    tool_name: str = ""
    tool_call_id: str = ""

    @staticmethod
    def make_image_message(text: str, images: List[ImageRef], **kwargs) -> 'ChatMessage':
        from ghostbox.util import getImageExtension

        complex_content_list = []
        for image_ref in images:
            ext = getImageExtension(image_ref.url, default="png")
            base64_image = image_ref.data.decode("utf-8")
            image_content = ChatContentComplex(
                type="image_url",
                content="",
                image_url=ImageURL(
                    url=(
                        f"data:image/{ext};base64,{base64_image}"
                        if image_ref.data is not None
                        else image_ref.url
                    )
                ),
            )
            complex_content_list.append(image_content)

        # don't forget the prompt
        complex_content_list.append(ChatContentComplex(type="text", content=text))

        return ChatMessage(role="user", content=complex_content_list, **kwargs)


LLMBackend = Enum("LLMBackend", "generic legacy llamacpp koboldcpp openai dummy")

# these are the models supported by ghostbox-tts
TTSModel = Enum("TTSModel", "zonos kokoro xtts polly orpheus")
ZonosTTSModel = Enum("ZonosTTSModel", "hybrid transformer")

# these are ways of playing sound that are supported by ghostbox-tts
TTSOutputMethod = Enum("TTSOutputMethod", "default websock")


# this isn't used yet anywhere, but it's nice to have here already for documentation
PromptFormatTemplateSpecialValue = Enum(
    "PromptFormatTemplateSpecialValue", "auto guess raw"
)


ArgumentType = Enum("ArgumentType", "Porcelain Plumbing")
ArgumentGroup = Enum(
    "ArgumentGroup",
    "General Generation Interface Characters Templates TTS Audio Images Tools Backend SamplingParameters LlamaCPP OpenAI",
)


class ArgumentTag(BaseModel):
    """Metadata associated with a command line argument."""

    name: str = ""
    type: ArgumentType
    group: ArgumentGroup

    # this is for e.g. streaming or temperature.
    very_important: bool = False

    # wether changing the value of this argument may start a service
    service: bool = False

    # for inclusion in the message of the day/tip
    motd: bool = False

    # basically, if its a command or option
    is_option: bool = True
    default_value: Optional[Any] = None

    # same help that is printed in terminal on --help
    help: str = ""

    def show_type(self) -> str:
        if self.is_option:
            return (
                "It is a "
                + self.type.name.lower()
                + " option in the "
                + self.group.name.lower()
                + " group."
            )
        return (
            "It is a "
            + self.type.name.lower()
            + " command in the "
            + self.group.name.lower()
            + " group."
        )

    def show_description(self) -> str:
        w = ""
        w += self.show_type()
        w += (
            "\nYou can set it with `/set "
            + self.name
            + " VALUE` or provide it as a command line parameter with `--"
            + self.name
            + "`"
        )
        return w


class SamplingParameterSpec(BaseModel):
    """Sampling parameters can be provided to backends to influence a model's inference behaviour.
    Most commonly this is temperature, presence penalty etc. However, here we take sampling parameter in the broadest sense, including samplers, CFG and control vectors.
    Note that this class provides only the specification of a sampling_parameter. This is for documentation and to keep track of which backend supports which parameters.
    """

    name: str
    description: str
    default_value: Any


class Timings(BaseModel):
    """Performance statistics for LLM backends.
    Most backends give timing statistics, though the format and particular stats vary. This class unifies the interface and boils it down to only the stats we care about.
    """

    prompt_n: int
    predicted_n: int
    cached_n: Optional[int] = None
    truncated: bool
    prompt_ms: float
    predicted_ms: float
    predicted_per_second: float
    predicted_per_token_ms: float

    original_timings: Optional[Dict[str, Any]] = {}

    def total_n(self) -> int:
        return self.prompt_n + self.predicted_n

    def total_ms(self) -> float:
        return self.prompt_ms + self.predicted_ms


api_default_options = {
    "color": False,
    "verbose": False,
    "stderr": True,
    "log_time": True,
    "cli_prompt": "",
    "dynamic_file_vars": False,
    "max_context_length": 2**15
}

```

## __init__.py

```python
# __all__ = ["I will get rewritten"]
## Don't modify the line above, or this line!
# import automodinit
# automodinit.automodinit(__name__, __file__, globals())
# del automodinit

from ghostbox.api import *
from ghostbox.definitions import *
import os

_ROOT = os.path.abspath(os.path.dirname(__file__))


def get_ghostbox_data(path):
    """Returns PATH preceded by the location of the ghostbox data dir, which is part of the python site-package."""
    return os.path.join(_ROOT, "data", path)


def get_ghostbox_html_path():
    """Returns the path to the index.html and javascript files for the integrated HTTP server."""
    return os.path.join(_ROOT, "html")



__all__ = [
    "from_generic",
    "from_openai_legacy",
    "from_llamacpp",
    "from_openai_official",
    "Ghostbox",
    "ChatMessage",
    "LLMBackend",
    "TTSModel",
    "TTSOutputMethod",
    "PromptFormatTemplateSpecialValue",
    "Timings",
    "api_default_options",
    "get_ghostbox_data",
    "get_ghostbox_html_path"
]

```

## main.py

```python
import requests, json, os, io, re, base64, random, sys, threading, signal, tempfile, string, uuid, textwrap
from queue import Queue, Empty
from typing import *
import feedwater
from functools import *
from colorama import just_fix_windows_console, Fore, Back, Style
from huggingface_hub import snapshot_download, try_to_load_from_cache
from lazy_object_proxy import Proxy  # type: ignore
import argparse
from argparse import Namespace
from ghostbox.commands import *
from ghostbox.autoimage import *
from ghostbox.output_formatter import *
from ghostbox.util import *
from ghostbox import util
from ghostbox import agency
from ghostbox._argparse import *
from ghostbox import streaming
from ghostbox.session import Session
from ghostbox.pftemplate import *
from ghostbox.backends import *
from ghostbox import backends
import ghostbox


def showHelpCommands(prog, argv):
    """
    List commands, their arguments, and a short description."""

    w = ""
    candidate = "/" + "".join(argv).strip()
    failure = True
    for cmd_name, f in sorted(cmds, key=lambda p: p[0]):
        if candidate != "/" and cmd_name != candidate:
            continue
        failure = False
        if f.__doc__ is None:
            docstring = cmds_additional_docs.get(cmd_name, "")
        else:
            docstring = str(f.__doc__)
        w += cmd_name + " " + docstring + "\n"
    printerr(w, prefix="")

    if failure:
        return "Command not found. See /help for more."
    return ""


def show_help_option_tag(prog, tag, markdown: bool = False) -> str:
    w = ""
    if not (markdown):
        cv = str(prog.getOption(tag.name))
        w += (
            tag.name
            + "\n"
            + "\n".join(textwrap.wrap(tag.help))
            + "\n"
            + "\n".join(textwrap.wrap(tag.show_description()))
            + "\nIts current value is "
            + (cv if cv else '""')
            + "."
        )
        if tag.default_value is not None:
            dv = str(tag.default_value)
            w += " Its default value is " + (dv if dv else '""')
        if tag.service:
            w += "\nSetting it to True will start the corresponding service."
    else:
        # we generate markdown for the docs
        w += (
            f"## `{tag.name}`"
            + "\n"
            + tag.help
            + "\n\n```\n"
            + "\n".join(textwrap.wrap(tag.show_description()))
        )
        if tag.default_value is not None:
            dv = str(tag.default_value)
            w += "\nIts default value is " + (dv if dv else '""')
        if tag.service:
            w += "\nSetting it to True will start the corresponding service."  # this is for the docs
        w += "\n```\n"
        # avoid printerr prefix
        print(w)
        return ""
    return w


def showHelp(prog, argv):
    """[TOPIC] [-v|--verbose] [--markdown]
    List help on various topics. Use -v to see even more information."""
    w = ""
    if argv != [] and argv[-1] == "--markdown":
        markdown = True
        del argv[-1]
    else:
        markdown = False

    if argv == []:
        # list topics
        w += "[Commands]\nUse these with a preceding slash like e.g. '/retry'. Do /help COMMANDNAME for more information on an individual command.\n\n"
        command_names = sorted([tuple[0].replace("/", "") for tuple in cmds])
        for i in range(len(command_names)):
            command_name = command_names[i]
            if i % 3 == 0:
                start = "\n  "
            else:
                start = "\t"

            w += start + command_name

        w += "\n\n[Options]\nSet these with '/set OPTIONNAME VALUE', where value is a valid python expression.\n List options and their values with /ls [OPTIONNAME].\n"
        tags = prog.getTags()
        options = sorted(
            tags.items(), key=lambda item: (item[1].group.name, item[1].name)
        )
        last_group = ""
        for i in range(len(options)):
            option, tag = list(options)[i]
            if last_group != tag.group.name:
                w += "\n\n  [" + tag.group.name + "]\n"
                last_group = tag.group.name
            option = (
                wrapColorStyle(option, "", Style.BRIGHT)
                if tag.very_important
                else option
            )
            if i % 3 == 0:
                w += "\n" + option
            else:
                w += "\t" + option

        return w

    topic = argv[0]
    if topic in prog.getTags():
        tag = prog.getTags()[topic]
        if tag.is_option:
            return show_help_option_tag(prog, tag, markdown=markdown)
    elif topic == "commands":
        # list all commands with help
        return showHelpCommands(prog, [])
    elif topic == "options":
        # list full options with description
        return "\n\n".join(
            filter(
                lambda w: w != "",
                [
                    show_help_option_tag(prog, tag, markdown=markdown)
                    for tag in sorted(prog.getTags().values(), key=lambda t: t.name)
                    if tag.is_option
                ],
            )
        )
    else:
        # fix it's a command
        return showHelpCommands(prog, [topic])
    # else:
    # not found
    # return "Topic not found in help. Please choose one of the topics below.\n" + showHelp(prog, [])
    return w


# these can be typed in at the CLI prompt
cmds = [
    ("/help", showHelp),
    ("/helpcommands", showHelpCommands),
    ("/start", newSession),
    ("/switch", switch),
    ("/quit", exitProgram),
    ("/test", testQuestion),
    ("/restart", lambda prog, argv: newSession(prog, [])),
    ("/print", printStory),
    ("/next", nextStory),
    ("/prev", previousStory),
    ("/story", gotoStory),
    ("/retry", retry),
    ("/rephrase", rephrase),
    ("/drop", dropEntry),
    ("/new", newStory),
    ("/clone", cloneStory),
    ("/log", lambda prog, w: printStory(prog, w, stderr=True, apply_filter=False)),
    ("!", transcribe),
    ("/transcribe", transcribe),
    ("/audio", toggleAudio),
    ("/image_watch", toggleImageWatch),
    ("/image", image),
    ("/time", showTime),
    ("/status", showStatus),
    ("/detokenize", detokenize),
    ("/tokenize", tokenize),
    ("/raw", showRaw),
    ("/lastresult", debuglast),
    ("/lastrequest", lastRequest),
    ("/ttsdebug", ttsDebug),
    ("/tts", toggleTTS),
    ("/set", setOption),
    ("/unset", lambda prog, argv: setOption(prog, [argv[0]])),
    ("/template", changeTemplate),
    ("/saveoptions", saveConfig),
    ("/saveconfig", saveConfig),
    ("/loadconfig", loadConfig),
    ("/save", saveStoryFolder),
    ("/load", loadStoryFolder),
    ("/varfile", varfile),
    ("/lstemplates", showTemplates),
    ("/lsoptions", showOptions),
    ("/lschars", showChars),
    ("/lsvoices", showVoices),
    ("/lsvars", showVars),
    ("/mode", toggleMode),
    ("/hide", hide),
    ("/cont", doContinue),
    ("/continue", doContinue),
]

# the mode formatters dictionary is a mapping from mode names to tuples of formatters, wrapped in a lambda that supplies a dictionary with keyword options to the formatters.
# The tuple contains indices :
# 0 - Formats text sent to the console display
# 1 - Formats text sent to the TTS backend
# 2 - Formats text to be saved as user message in the chat history
# 3 - formats text to be saved as AI message in the chat history
mode_formatters = {
    "default": lambda d: (DoNothing, DoNothing, DoNothing, CleanResponse),
    "chat": lambda d: (
        DoNothing,
        NicknameRemover(d["chat_ai"]),
        NicknameFormatter(d["chat_user"]),
        ChatFormatter(d["chat_ai"]),
    ),
}


class Plumbing(object):
    def __init__(self, options={}, initial_cli_prompt="", tags={}):
        # the printerr stuff needs to happen early
        self._printerr_buffer = []
        self._initial_printerr_callback = lambda w: self._printerr_buffer.append(w)
        util.printerr_callback = self._initial_printerr_callback
        if not (options["stderr"]):
            util.printerr_disabled = True
        # this is for the websock clients
        self.stderr_token = "[|STDER|]:"
        self._stdout_ringbuffer = ""
        self._stdout_ringbuffer_size = 1024
        self._frozen = False
        self._freeze_queue = Queue()
        self.options = options
        self.tags = tags
        self.backend = None
        self.template = None
        self.initializeBackend(self.getOption("backend"), self.getOption("endpoint"))
        self.session = Session(chat_user=options.get("chat_user", ""))
        # FIXME: make this a function returning backend.getlAStResult()
        self.lastResult = {}
        self._lastInteraction = 0
        self.tts_flag = False
        self.initial_print_flag = False
        self.initial_cli_prompt = initial_cli_prompt
        self.stream_queue = []
        self.stream_sentence_queue = []
        self._stream_only_once_token_bag = set()
        self.images = {}
        # flag to show wether image data needs to be resent to the backend
        self._images_dirty = False
        self._lastPrompt = ""
        self._dirtyContextLlama = False
        self._stop_generation = threading.Event()
        # indicates generation work being done, really only used to print the goddamn cli prompt
        self._busy = threading.Event()
        self._busy.set()
        self._triggered = False
        self._smartShifted = False
        self._systemTokenCount = None
        self.continue_with = ""
        self.tts = None
        self.tts_config = None
        self.multiline_buffer = ""
        if self.getOption("json_grammar"):
            self.setOption("grammar", getJSONGrammar())
            del self.options["json"]
        elif self.getOption("grammar_file"):
            self.loadGrammar(self.getOption("grammar_file"))
        else:
            self.setOption("grammar", "")
        # template
        self.loadTemplate(self.getOption("prompt_format"), startup=True)

        # whisper stuff. We do this with a special init function because it's lazy
        self.whisper = self._newTranscriber()
        self.ct = None
        self._defaultSIGINTHandler = signal.getsignal(signal.SIGINT)
        self._transcription_suspended = False

        # imagewatching
        self.image_watch = None

        # http server
        self.http_server = None
        self.http_thread = None

        # websock server
        self.websock_thread = None
        self.websock_server = None
        self.websock_clients = []
        self.websock_server_running = threading.Event()
        self.websock_msg_queue = Queue()

        # formatters is to be idnexed with modes
        self.setMode(self.getOption("mode"))
        self.running = True
        self._startCLIPrinter()

    def getTags(self):
        return self.tags | {
            param.name: param
            for param in backends.sampling_parameter_tags.values()
            if param.name in self.getBackend().sampling_parameters().keys()
        }

    def initializeBackend(self, backend, endpoint):
        api_key = self.getOption("api_key")
        kwargs = {"logger": self.verbose}
        if backend == LLMBackend.llamacpp.name:
            self.backend = LlamaCPPBackend(endpoint, **kwargs)
        elif backend == LLMBackend.openai.name:
            if not api_key:
                printerr(
                    "error: OpenAI API key is required for the OpenAI backend. Did you forget to provide --api_key?"
                )
                # this is rough but we are in init phase so it's ok
                sys.exit()
            self.backend = OpenAIBackend(api_key, **kwargs)
            self.setOption("prompt_format", "auto")
        elif backend == LLMBackend.generic.name:
            self.backend = OpenAIBackend(api_key, endpoint=endpoint, **kwargs)
            self.setOption("prompt_format", "auto")
        elif backend == LLMBackend.legacy.name:
            if (
                self.getOption("prompt_format")
                == PromptFormatTemplateSpecialValue.auto.name
            ):
                printerr(
                    "warning: Setting prompt_format to 'raw' as using 'auto' as prompt_format with the legacy backend will yield server errors. This backend exists specifically to *not* apply templates server side. You can manually reset this if you want."
                )
                # doing it without setOption as backend isn't fully initialized yet
                self.options["prompt_format"] = (
                    PromptFormatTemplateSpecialValue.raw.name
                )

            self.backend = OpenAILegacyBackend(api_key, endpoint=endpoint, **kwargs)
        else:
            # Handle other backends...
            pass

        # fill in the default sampler parameters that were not shown in the command line arguments, but are still supported
        for param in self.backend.sampling_parameters().values():
            if param.name not in self.options.keys():
                self.setOption(param.name, param.default_value)

    def getBackend(self):
        return self.backend

    def justUsedTools(self) -> bool:
        # FIXME: not sure if this is the place to do it/right way to do it
        # if the last message in the history has role 'tools', it means
        # we just send results, so we don't use tools for this interaction
        try:
            return self.session.stories.get().getData()[-1].role == "tool"
        except:
            printerr("warning: Tried to check for tools use with empty history.")
        return False

    def _getTTSSpecialMsg(self) -> str:
        """Returns special tags for the TTS or empty string."""
        w = ""
        tags = self.tryGetTTSSpecialTags()
        if tags != []:
            w += (
                "\nUse the following tags in conversation when appropriate (they will be translated into audio by a TTS model): "
                + ", ".join(tags)
            )

        return w

    def makeGeneratePayload(self, text):
        d = {}

        # gather set sampling parameters and warn user for unsupported ones
        all_params = backends.sampling_parameters.keys()
        supported_params = self.getBackend().sampling_parameters().keys()
        for param in all_params:
            if param in self.options:
                if param not in supported_params and not (
                    self.getOption("force_params")
                ):
                    if self.getOption("warn_unsupported_sampling_parameter"):
                        printerr(
                            "warning: Sampling parameter '"
                            + param
                            + "' is not supported by the "
                            + str(self.getBackend().getName())
                            + " backend. Set force_params to true to send it anyway."
                        )
                    else:
                        continue
                else:
                    d[param] = self.getOption(param)

        # some special ones
        for k, v in special_parameters.items():
            d[k] = v if not (self.getOption(k)) else self.getOption(k)

        # don't override grammar if it's null, rather not send it at all
        if "grammar" in d:
            if not (d["grammar"]):
                del d["grammar"]

        # just throw toools in for backends that can use them, unless user disabled
        if self.getOption("use_tools"):
            if not (self.justUsedTools()):
                d["tools"] = [tool.model_dump() for tool in self.session.tools]
                # FIXME: necessary currently with llamacpp
                if "grammar" in d:
                    del d["grammar"]

        if (
            self.getOption("backend") == LLMBackend.generic.name
            or self.getOption("backend") == LLMBackend.openai.name
            or self.getOption("prompt_format")
            == PromptFormatTemplateSpecialValue.auto.name
        ):
            # openai chat/completion needs the 'messages' key
            # we also do this for llama in "auto" template mode
            d["system"] = self.session.getSystem()
            if self.getOption("tts_modify_system_msg"):
                d["system"] += self._getTTSSpecialMsg()

            d["messages"] = [
                {"role": "system", "content": d["system"]}
            ] + copy.deepcopy(self.session.stories.get().to_json())
            # FIXME: I don't even know if this does something right now
            self.images_dirty = False

        else:
            # all others get the text
            # FIXME: there is some deep mishandling here because this uses the text parameter while oai endpoitns use the story. must investigate this
            d["prompt"] = text
        return d

    def isContinue(self):
        return self.getOption("continue")

    def resetContinue(self):
        self.setOption("continue", False)

    def _newTranscriber(self):
        # makes a lazy WhisperTranscriber, because model loading can be slow
        # yes this is hacky but some platforms (renpy) can't handle torch, which the whisper models rely on
        def delayWhisper():
            from ghostbox.transcribe import WhisperTranscriber

            return WhisperTranscriber(
                model_name=self.getOption("whisper_model"),
                silence_threshold=self.getOption("audio_silence_threshold"),
                input_func=lambda: printerr("Started Recording. Hit enter to stop."),
            )

        return Proxy(delayWhisper)

    def continueWith(self, newUserInput):
        # FIXME: the entire 'continue' architecture is a trashfire. This should be refactored along with other modeswitching/input rewriting stuff in the main loop
        self.setOption("continue", "1")
        self.continue_with = newUserInput

    def popContinueString(self):
        self.setOption("continue", False)
        tmp = self.continue_with
        self.continue_with = ""
        return tmp

    def loadGrammar(self, grammar_file):
        if os.path.isfile(grammar_file):
            w = open(grammar_file, "r").read()
            self.setOption("grammar", w)
        else:
            self.setOption("grammar", "")
            printerr(
                "warning: grammar file "
                + grammar_file
                + " could not be loaded: file not found."
            )

    def guessPromptFormat(self):
        """Uses any trick it can do guess the prompt format template. Returns a string like 'chat-ml', 'alpaca', etc."""
        # see if we can find llm_layers
        try:
            data = loadLayersFile()
        except:
            printerr("warning: Couldn't load layers file " + getLayersFile())

        try:
            models = dirtyGetJSON(self.getOption("endpoint") + "/v1/models").get(
                "data", []
            )
            # hope it's just one
            model = os.path.basename(models[0]["id"])
        except:
            printerr(traceback.format_exc())
            printerr("Failed to guess prompt format. Defaulting.")
            return "raw"

        # check if model is in layers file
        for d in data:
            if "name" not in d.keys():
                continue
            if d["name"].lower() == model.lower():
                if d["prompt_format"]:
                    # success
                    return d["prompt_format"]

        # FIXME: at this point it's not in the layers file, but we still have a model name. consider googling it on hugginface and grepping the html
        printerr(
            "Failed to guess prompt format after exhausting all options ðŸ˜¦. Defaulting."
        )
        return "raw"

    def loadTemplate(self, name, startup=False):
        # this is so llamacpp uses the /chat/completions endpoint if we use auto templating
        # has no effect on other backends
        self.getBackend().configure(
            {
                "llamacpp_use_chat_completion_endpoint": (
                    True
                    if name == PromptFormatTemplateSpecialValue.auto.name
                    else False
                )
            }
        )

        # special cases
        if name == PromptFormatTemplateSpecialValue.auto.name:
            if startup and (self.template is not None):
                # don't set this twice
                return
            printerr(
                "Prompt format template set to 'auto': Formatting is handled server side."
            )
            self.template = RawTemplate()
            return
        if name == "guess":
            name = self.guessPromptFormat()

        allpaths = [p + "/" + name for p in self.getOption("template_include")]
        for path in allpaths:
            path = os.path.normpath(path)
            if not (os.path.isdir(path)):
                failure = (
                    "Could not find prompt format template '"
                    + name
                    + "'. Did you supply a --template_include option?"
                )
                continue
            failure = False
            try:
                template = FilePFTemplate(path)
                break
            except FileNotFoundError as e:
                failure = e

        if failure:
            printerr("warning: " + str(failure) + "\nDefaulting to 'raw' template.")
            self.template = RawTemplate()
            self.options["prompt_format"] = "raw"
            return
        # actually load template
        # first unload old stops
        self.options["stop"] = list(
            filter(lambda w: w not in self.template.stops(), self.options["stop"])
        )

        self.template = template
        for w in template.stops():
            if not (w):
                continue
            self.appendOption("stop", w)
        self.options["prompt_format"] = name
        printerr("Using '" + name + "' as prompt format template.")

    def loadConfig(self, json_data, override=True, protected_keys: List[str] = []):
        """Loads a config file provided as json into options. Override=False means that command line options that have been provided will not be overriden by the config file."""
        d = json.loads(json_data)
        if type(d) != type({}):
            return "error loading config: Not a dictionary."
        if not (override):
            # drop keys in the config that can be found in the command line arguments
            # have to do the one letter options manually
            letterMap = {
                "u": "chat_user",
                "c": "character_folder",
                "V": "tts_voice",
                "T": "prompt_format",
            }
            for arg in sys.argv:
                if not (arg.startswith("-")):
                    continue
                key = stripLeadingHyphens(arg)
                # "no-" is for e.g. '--no-tts'
                if key in d or "no-" + key in d:
                    del d[key]
                for letter, full_arg in letterMap.items():
                    if key.startswith(letter):
                        if full_arg in d:
                            del d[full_arg]

        # now actually load the options, with a partial ordering
        items = sorted(
            d.items(), key=cmp_to_key(lambda a, b: -1 if a[0] == "mode" else 1)
        )
        for k, v in items:
            if not (k in protected_keys):
                self.setOption(k, v)
        return ""

    def showCLIPrompt(self):
        if self.isMultilineBuffering():
            return ""

        if self.getOption("cli_prompt") == "":
            return ""

        f = IdentityFormatter()
        if self.getOption("color"):
            f = ColorFormatter(self.getOption("cli_prompt_color")) + f

        return self.session.expandVars(f.format(self.getOption("cli_prompt")))

    def getMode(self):
        w = self.getOption("mode")
        if not (self.isValidMode(w)):
            return "default"
        return w

    def isValidMode(self, mode):
        return mode in mode_formatters

    def setMode(self, mode):
        # FIXME: this isn't affected by freeze, but since most things go through setOption we should be fine, right? ... right?
        if not (self.isValidMode(mode)):
            return

        self.options["mode"] = mode
        if mode.startswith("chat"):
            userPrompt = mkChatPrompt(self.getOption("chat_user"))
            self.setOption("cli_prompt", "\n" + userPrompt)
            self.appendOption("stop", userPrompt, duplicates=False)
            self.appendOption("stop", userPrompt.strip(), duplicates=False)

        else:  # default
            self.options["cli_prompt"] = self.initial_cli_prompt

    def getOption(self, key):
        return self.options.get(key, None)

    def optionDiffers(self, name, newValue):
        if name not in self.options:
            return True
        return self.getOption(name) != newValue

    def getFormatters(self):
        mode = self.getOption("mode")
        if not (self.isValidMode(mode)):
            mode = "default"
            printerr("warning: Unsupported mode '" + mode + "'.. Using 'default'.")
        return mode_formatters[mode](self.options | self.session.getVars())

    def getDisplayFormatter(self):
        return self.getFormatters()[0]

    def getTTSFormatter(self):
        return self.getFormatters()[1]

    def getUserFormatter(self):
        return self.getFormatters()[2]

    def getAIColorFormatter(self):
        if self.getOption("color"):
            return ColorFormatter(
                self.getOption("text_ai_color"), self.getOption("text_ai_style")
            )
        return IdentityFormatter()

    def getAIFormatter(self, with_color=False):
        return self.getAIColorFormatter() + self.getFormatters()[3]

    def addUserText(self, w):
        if w:
            self.session.stories.get().addUserText(
                self.getUserFormatter().format(w), image_context=self.images
            )

    def addAIText(self, w):
        """Adds text toe the AI's history in a cleanly formatted way according to the AI formatter. Returns the formatted addition or empty string if nothing was added.."""
        if w == "":
            return ""

        if self.getOption("continue"):
            continuation = self.getAIFormatter().format(w)
            self.session.stories.get().extendAssistantText(continuation)
            return continuation

        # hint may have been sent to the backend but we have to add it to the story ourselves.
        if self.getOption("hint_sticky"):
            hint = self.getOption("hint")
        else:
            hint = ""

        if (rformat := self.getOption("response_format")) and (
            rformat["type"] != "text"
        ):
            # don't use formatters if user expects structured data
            addition = hint + w
        else:
            addition = self.getAIFormatter().format(hint + w)
        self.session.stories.get().addAssistantText(addition)
        return addition

    def addSystemText(self, w):
        """Add a system message to the chat log, i.e. add a message with role=system. This is mostly used for tool/function call results."""
        if w == "":
            return

        # FIXME: we're currently rawdogging system msgs. is this correct?
        self.session.stories.get().addSystemText(w)

    def applyTools(self, w: str = "", json={}) -> Tuple[List[ChatMessage], ChatMessage]:
        """Takes an AI generated string w and optionally the json result from an OpenAI API compatible tool request. Tries to detect a tool request in both. If detected, will apply the tools and then return their results as structured data, as well as the fully parsed original tool call.
        Ideally the JSON result makes tool use obvious through the field
        json["choices"][0]["finish_reason"] == "tool_calls"
        However, not all models will do this consistently and those who can may fail at higher temperatures.
        So as a fallback we check the raw w for json data that corresponds to tools use.
        :param w: Unstructured AI generation string that may contain a tool request.
        :param json: Structured json that was returned by the AI that may contain a tool request.
        :return: A pair of tool results, and the original tool requesting message, both as json dicts. If no tool request was detected, both are empty dicts.
        """
        import json as j

        nothing = ([], {})

        if not (self.getOption("use_tools")):
            self.verbose("Not applying tools because use_tools=False.")
            return nothing

        try:
            if json["choices"][0]["finish_reason"] != "tool_calls":
                self.verbose(
                    f"Not using tools: finish_reason={json["choices"][0]["finish_reason"]}"
                )
                return nothing
            tool_request = json["choices"][0]["message"]
            tool_calls = tool_request["tool_calls"]
        except:
            tool_calls = []
            printerr(traceback.format_exc())
        # FIXME: add heuristic for scanning w for possible tool request
        # ...
        # here, we know it was a tool request and we have tool_calls
        self.verbose(f"Using tools with calls: {tool_calls}")
        if tool_calls == []:
            # either none requested or something went wrong. anyhow.
            return nothing

        tool_results = []
        try:
            for tool_call in tool_calls:
                if self.getOption("verbose"):
                    printerr(j.dumps(tool_call, indent=4))

                tool = tool_call["function"]
                try:
                    params = j.loads(tool["arguments"])
                except:
                    if self.getOption("verbose"):
                        printerr(
                            "warning: Couldn't pass tool arguments as json: "
                            + tool["arguments"]
                        )
                        continue
                maybeResult = self.session.callTool(tool["name"], params)
                tool_results.append(
                    agency.makeToolResult(tool["name"], maybeResult, tool_call["id"])
                )
        except:
            printerr("warning: Caught the following exception while applying tools.")
            printerr(traceback.format_exc())

        if tool_results == []:
            return nothing

        # NOTE: the responses from the server come back more complex than what is later supposed to be sent in the chat history
        # specifically, a message with role "assistant", conten null, and tool_calls=... should *not* have the tool c<calls be a layered dictionary with type and function key, it's just a plain list of dicts of names and arguments
        # I have no idea why this is inconsistent but that's how it is, and that's the reason for the repackaging below.
        tool_request_msg = ChatMessage(
            role="assistant",
            tool_calls=[
                ToolCall(
                    function=FunctionCall(
                        name=fcall["function"]["name"],
                        arguments=fcall["function"]["arguments"],
                    )
                )
                for fcall in tool_request["tool_calls"]
            ],
        )

        return tool_results, tool_request_msg

    def _formatToolResults(self, results):
        """Based on a list of dictionaries, returns a string that represents the tool outputs to an LLM."""
        # FIXME: currently only intended for command-r
        w = ""
        # w += "<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>"
        for tool in results:
            if len(results) > 1:
                # tell the LLM what tool has which results
                w += "## " + tool["tool_name"] + "\n\n"
            w += agency.showToolResult(tool["output"])
        # w += "<|END_OF_TURN_TOKEN|>"
        return w

    def appendOption(self, name, value, duplicates=True):
        if name not in self.options:
            printerr("warning: unrecognized option '" + name + "'")
            return

        xs = self.getOption(name)
        if type(xs) != type([]):
            printerr(
                "warning: attempt to append to '" + name + "' when it is not a list."
            )
            return

        if not (duplicates):
            if value in xs:
                return
        self.options[name].append(value)

    def freeze(self):
        self._frozen = True

    def unfreeze(self):
        self._frozen = False
        while not (self._freeze_queue.empty()):
            (name, value) = self._freeze_queue.get(block=False)
            self.setOption(name, value)
            if self.getOption("verbose"):
                printerr("unfreezing " + name + " : " + str(value))

    def setOption(self, name, value):
        # we freeze state during some parts of execution, applying options after we unfreeze
        if self._frozen:
            self._freeze_queue.put((name, value))
            return

        # mode gets to call dibs
        if name == "mode":
            self.setMode(value)
            return

        oldValue = self.options.get(name, None)
        differs = self.optionDiffers(name, value)
        self.options[name] = value
        # for some options we do extra stuff
        if (
            name == "tts_voice"
            or name == "tts_volume"
            or name == "tts_tortoise_quality"
        ) and self.getOption("tts"):
            # we don't want to restart tts on /restart
            if differs:
                # printerr("Restarting TTS.")
                self.tts_flag = True  # restart TTS
        elif name == "no-tts":
            self.setOption("tts", not (value))
        elif name == "whisper_model":
            self.whisper = self._newTranscriber()
        # elif name == "cli_prompt":
        # self.initial_cli_prompt = value
        elif name == "tts_websock":
            if value:
                self.setOption("tts_output_method", TTSOutputMethod.websock.name)
            else:
                self.setOption("tts_output_method", TTSOutputMethod.default.name)
            if differs and self.getOption("tts"):
                self.tts_flag = True  # restart TTS
        elif name == "max_context_length":
            self._dirtyContextLlama = False
        elif name == "prompt_format":
            self.loadTemplate(value)
        elif name == "chat_user":
            # userpormpt might be in stopwords, which we have to refresh
            prompt = mkChatPrompt(oldValue)
            badwords = [prompt, prompt.strip()]
            self.options["stop"] = list(
                filter(lambda w: w not in badwords, self.getOption("stop"))
            )

            # and this will add the new username if it's chat mode
            self.setMode(self.getMode())
        elif name == "stop":
            # user may change this in a char config.json. This may be buggy, but let's make sure at least the template is in.
            if self.template:
                self.options[name] = self.template.stops()
            else:
                self.options[name] = []
            self.options[name] += value
        elif name == "chat_ai":
            self.session.setVar(name, value)
        elif name == "http" and differs:
            if value:
                self._initializeHTTP()
            else:
                self._stopHTTP()
        elif name == "websock" and differs:
            if value:
                self.initializeWebsock()
            else:
                self.stopWebsock()
        elif name == "image_watch" and differs:
            if value:
                self.startImageWatch()
            else:
                self.stopImageWatch()
        elif name == "audio" and differs:
            if value:
                self.startAudioTranscription()
            else:
                self.stopAudioTranscription()
        elif name == "stderr":
            util.printerr_disabled = not (value)
        elif name == "use_tools" and value:
            printerr(
                "warning: Streaming is currently not supported with tool use. Setting stream = False."
            )
            self.setOption("stream", False)
        elif name == "stream":
            if value and self.getOption("tool_use"):
                printerr(
                    "warning: Streaming is currently not supported with tool use. Setting stream = False."
                )
                self.setOption("stream", False)
                printerr
        return ""

    def _ctPauseHandler(self, sig, frame):
        printerr("Recording paused. CTRL + c to resume, /text to stop.")
        self.ct.pause()
        signal.signal(signal.SIGINT, self._ctResumeHandler)

    def _ctResumeHandler(self, sig, frame):
        printerr("Recording resumed. CTRL + c to interrupt.")
        self.ct.resume()
        signal.signal(signal.SIGINT, self._ctPauseHandler)

    def _imageWatchCallback(self, image_path, image_id):
        newStory(self, [])
        w = self.getOption("image_watch_msg")
        if w == "":
            return

        self.loadImage(image_path, image_id)
        # FIXME: what if loadImage fails?
        (modified_w, hint) = self.modifyInput(w)
        self.addUserText(modified_w)
        image_watch_hint = self.getOption("image_watch_hint")
        self.addAIText(self.communicate(self.buildPrompt(hint + image_watch_hint)))
        # print(self.showCLIPrompt(), end="")

    def _print_generation_callback(self, result_str):
        """Pass this as callback to self.interact for a nice simple console printout of generations."""
        if result_str == "":
            return

        if not (self.getOption("stream")) and self.getOption("stdout"):
            self.print(
                self.getAIFormatter(with_color=self.getOption("color")).format(
                    result_str
                ),
                end="",
            )
        # self.print(self.showCLIPrompt(), end="", tts=False)
        return

    def suspendTranscription(self):
        """Renders transcription unresponsive until an activation phrase is heard or unsuspendTranscription is called."""
        self._transcription_suspended = True
        self.verbose("Suspending audio interaction.")

    def unsuspendTranscription(self):
        """Resumes interacting by audio after a suspension."""
        self._transcription_suspended = False
        self.verbose("Resuming audio interaction.")

    def modifyTranscription(self, w):
        """Checks wether an incoming user transcription by the whisper model contains activation phrases or is within timing etc. Returns the modified transcription, or the empty string if activation didn't trigger."""

        # want to match fuzzy, so strip of all punctuation etc.
        # FIXME: no need to do this here, again and again
        phrase = (
            self.getOption("audio_activation_phrase")
            .translate(str.maketrans("", "", string.punctuation))
            .strip()
            .lower()
        )
        if not (phrase):
            # user doesn't require phrase. proceed. unless we are suspended
            if not (self._transcription_suspended):
                return w

        # ok we require an activation phrase, but are we within the grace period where none is required?
        if (t := self.getOption("audio_activation_period_ms")) > 0:
            # grace period doesn't matter if we have been suspended.
            if (time_ms() - self._lastInteraction) <= t and not (
                self._transcription_suspended
            ):
                # FIXME: returning here means there might be a phrase in the input even when phrase_keep is false. Maybe it doesn't matter?
                return w

        # now we need an activation phrase or it ain't happenin
        # strip the transcription
        test = w.translate(str.maketrans("", "", string.punctuation)).strip().lower()
        try:
            n = test.index(phrase)
        except ValueError:
            # no biggie, phrase wasn't found. we're done here
            return ""

        # ok it was found
        self.unsuspendTranscription()
        if self.getOption("audio_activation_phrase_keep"):
            return w
        # FIXME: this is 100% not correct, but it may be good enough
        return w.replace(phrase, "").replace(
            self.getOption("audio_activation_phrase"), ""
        )

    def printActivationPhraseWarning(self):
        n = self.getOption("warn_audio_activation_phrase")
        if not (n):
            return
        w = (
            "warning: Your message was received but triggered no response, because the audio activation phrase was not found in it. Hint: The activation phrase is '"
            + self.getOption("audio_activation_phrase")
            + "'."
        )
        if n != -1:
            w += " This warning will only be shown once."
            self.setOption("warn_audio_activation_phrase", False)
        printerr(w)

    def _transcriptionCallback(self, w):
        """Supposed to be called whenever the whisper model has successfully transcribed a phrase."""
        if self.getOption("audio_show_transcript"):
            self.print(w, tts=False)

        w = self.modifyTranscription(w)
        if not (w):
            self.printActivationPhraseWarning()
            return

        self.interact(w, self._print_generation_callback)

    def _transcriptionOnThresholdCallback(self):
        """Gets called whenever the continuous transcriber picks up audio above the threshold."""
        if self.getOption("audio_interrupt"):
            self.stopAll()

    def _streamCallback(self, token, user_callback=None, only_once=None):
        if only_once not in self._stream_only_once_token_bag:
            # this is so that we can print tokens/sentences without interrupting the TTS every time
            # except that we want to interrupt the TTS exactly once -> when we start streaming
            # If you do this elsewhere, i.e. communicate(), we risk a race condition
            self._stream_only_once_token_bag.add(only_once)
            if self.getOption("tts_interrupt"):
                self.stopTTS()

        if user_callback is None:
            user_callback = lambda x: x
        f = lambda w: self.print(
            self.getAIColorFormatter().format(w), end="", flush=True, interrupt=False
        )

        self.stream_queue.append(token)
        method = self.getOption("stream_flush")
        if method == "token":
            f(token)
            user_callback(token)

        elif method == "sentence" or method == "flex":
            self.stream_sentence_queue.append(token)
            if "\n" in token:
                w = "".join(self.stream_sentence_queue)
            else:
                w = IncompleteSentenceCleaner().format(
                    "".join(self.stream_sentence_queue)
                )
                if w.strip() == "":
                    # not a complete sentence yet, let's keep building it
                    return

                if method == "flex" and len(w) < self.getOption(
                    "stream_flush_flex_value"
                ):
                    # sentence isn't long enough yet
                    return

            # w is a complete sentence, or a full line
            self.stream_sentence_queue = []
            f(w)
            user_callback(w)

    def flushStreamQueue(self):
        w = "".join(self.stream_queue)
        self.stream_queue = []
        self.stream_sentence_queue = []
        return w

    def isAudioTranscribing(self):
        return self.ct is not None and self.ct.running

    def startImageWatch(self):
        dir = self.getOption("image_watch_dir")
        printerr("Watching for new images in " + dir + ".")
        self.image_watch = AutoImageProvider(dir, self._imageWatchCallback)

    def startAudioTranscription(self):
        printerr("Beginning automatic transcription. CTRL + c to pause.")
        if self.ct:
            self.ct.stop()
        self.ct = self.whisper.transcribeContinuously(
            callback=self._transcriptionCallback,
            on_threshold=self._transcriptionOnThresholdCallback,
            websock=self.getOption("audio_websock"),
            websock_host=self.getOption("audio_websock_host"),
            websock_port=self.getOption("audio_websock_port"),
        )
        signal.signal(signal.SIGINT, self._ctPauseHandler)

    def stopImageWatch(self):
        if self.image_watch:
            printerr("Stopping watching of images.")
            self.image_watch.stop()

    def stopAudioTranscription(self):
        if self.ct:
            printerr("Stopping automatic audio transcription.")
            self.ct.stop()
        self.ct = None
        signal.signal(signal.SIGINT, self._defaultSIGINTHandler)

    def initializeTTS(self):
        tts_program = self.getOption("tts_program")
        candidate = os.getcwd() + "/" + tts_program
        if os.path.isfile(candidate):
            tts_program = candidate

        if not (tts_program):
            return "Cannot initialize TTS: No TTS program set."

        # download models if need be
        # ghostbox-tts does this as well, but it's stupidly tricky to get it's stderr to print to our stderr only for model download
        # so we do it here to give user some feedback on dl
        if tts_program == "ghostbox-tts":
            if (
                self.getOption("tts_model") == TTSModel.orpheus.name
                and self.getOption("tts_orpheus_model") == ""
            ):
                # the default, which is reasonable for most people, is to use the 4bit quant
                # if users want some more specific stuff they're going to have to work for it a little bit I guess
                downloaded = False
                if (gguf_file := try_to_load_from_cache("isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF", "orpheus-3b-0.1-ft-q4_k_m.gguf")) is not None:
                    if os.path.isfile(gguf_file):
                        downloaded = True

                if not(downloaded):
                    snapshot_download("isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF")

        # pick a voice in case of random
        if self.getOption("tts_voice") == "random":
            # no setOption to avoid recursion
            voices = getVoices(self)
            if voices == []:
                return "error: Cannot initialize TTS: No voices available."
            self.options["tts_voice"] = random.choice(voices)
            printerr("Voice '" + self.getOption("tts_voice") + "' was chosen randomly.")

        if self.tts is not None:
            # restarting
            try:
                if not (self.tts.is_running()):
                    self.tts.close()
            except ProcessLookupError:
                printerr(
                    "warning: TTS process got lost somehow. Probably not a big deal."
                )

        # let's make the path issue absolutely clear. We only track tts_voice_dir, but to the underlying tts program, we expose the tts_voice_abs_dir environment variable, which contains the absolute path to the voice dir
        # FIXME: rewrite the entire path architecture
        tts_voice_abs_dir = self.tryGetAbsVoiceDir()
        self.tts = feedwater.run(
            tts_program,
            env=envFromDict(
                self.options
                | {
                    "tts_voice_abs_dir": tts_voice_abs_dir,
                    "ONNX_PROVIDER": "CUDAExecutionProvider",
                }
            ),
        )
        self.setOption("stream_flush", "flex")
        if self.getOption("verbose"):
            printerr(
                " Automatically set stream_flush to 'flex'. This is recommended with TTS. Manually reset it to 'token' if you really want."
            )

        # new
        # we try to get the tts config
        # it contains special options specific to a model
        # unfortunately it will only be available after the model has finished initializing.
        # so we do it on a seperate thread
        self.spawnUpdateTTSConfig()
        return "TTS initialized."

    def spawnUpdateTTSConfig(self) -> None:
        """Spawns a small worker that tries to update the TTS config by communicating with the TTS backend.
        This may fail or simply go on forever. If the worker succeeds, the thread is terminated.
        Invoking this method is non-blocking, but will set self.tts_config to None."""
        self.tts_config = None

        def update_config():
            retries = 0
            max_retries = 3

            while self.tts_config is None:
                time.sleep(3)
                if self.tts is None:
                    continue

                try:
                    # this can fail e.g. if the tts isn't ready yet
                    # but will also fail with a broken pipe if the underlying process is already closed or the ghostbox deleted
                    # hence we give up after max_retries
                    self.tts.write_line("<dump_config>")
                    lines = self.tts.get()
                except:
                    retries += 1
                    if retries >= max_retries:
                        break
                    continue

                for i in range(len(lines) - 1, -1, -1):
                    line = lines[i]
                    if line.startswith("config: "):
                        try:
                            self.tts_config = json.loads(line[8:])
                        except:
                            printerr("warning: Error while reading tts config.")
                            self.verbose(traceback.format_exc())
                            # give up and don't keep trying
                            self.tts_config = {}

        t = threading.Thread(target=update_config, daemon=True)
        t.start()

    def tryGetTTSSpecialTags(self) -> List[str]:
        """Returns a list of special tags that the underlying tts model may or may not have defined.
        These are things like <laugh> or <cough>, etc.
        Returns empty list if no tags can be ascertained."""
        if self.tts_config is None:
            return []

        return self.tts_config.get("special_tags", [])

    def tryGetAbsVoiceDir(self):
        # this is sort of a heuristic. The problem is that we allow multiple include dirs, but have only one voice dir. So right now we must pick the best from a number of candidates.
        if os.path.isabs(self.getOption("tts_voice_dir")) and os.path.isdir(
            self.getOption("tts_voice_dir")
        ):
            return self.getOption("tts_voice_dir")

        winner = ""
        ok = False
        for path in self.getOption("include"):
            file = path + "/" + self.getOption("tts_voice_dir")
            if os.path.isdir(file):
                winner = file
                ok = True
                break

        abs_dir = os.path.abspath(winner)
        if not (ok):
            printerr(
                "warning: Couldn't cleanly determine tts_voice_dir. Guessing it is '"
                + abs_dir
                + "'."
            )
        return abs_dir

    def stopTTS(self):
        # FIXME: not implemented for all TTS clients
        if self.tts is None:
            return

        self.tts.write_line("<clear>")

    def communicateTTS(self, w, interrupt=False):
        if not (self.getOption("tts")):
            return ""

        if interrupt:
            self.stopTTS()

        # strip color codes - this would be nicer by just disabling color, but the fact of the matter is we sometimes want color printing on console and tts at the same time. At least regex is fast.
        w = stripANSI(w)

        # strip whitespace, we especially don't want to send pure whitespace like ' \n' or '  ' to a tts, this is known to crash some of them. It also shouldn't change the resulting output.
        w = w.strip()

        if self.tts is None or not (self.tts.is_running()):
            self.setOption("tts", False)
            printerr(
                "error: TTS is dead. You may attempt to restart with /tts. Check errors with /ttsdebug ."
            )
            return ""
        self.tts.write_line(w)
        return w

    def verbose(self, w: str):
        """Prints to stderr but only in verbose mode."""
        if self.getOption("verbose"):
            printerr(w)

    def console(self, w):
        """Print something to stderr. This exists to be used in tools.py via dependency injection."""
        printerr(w)

    def console_me(self, w):
        """Prints w to stderr, prepended by the AI name. This exists because it is a very common pattern to be used in tools.py"""
        printerr(self.getOption("chat_ai") + w)

    def _ringbuffer(self, w: str) -> str:
        """Takes a string w and returns it unchanged, but copies it to a local ringbuffer.
        The purpose of this is solely to let the CLI worker check if we have a newline at the end of the output.
        Note: not actually a ringbuffer."""
        self._stdout_ringbuffer = (self._stdout_ringbuffer + w)[
            : self._stdout_ringbuffer_size
        ]
        return w

    def print(
        self,
        w,
        end="\n",
        flush=False,
        color="",
        style="",
        tts=True,
        interrupt=None,
        websock=True,
    ):
        # either prints, speaks, or both, depending on settings
        if w == "":
            return

        if self.getOption("websock"):
            self.websockSend(w)

        if self.getOption("quiet"):
            return

        if tts and self.getOption("tts") and w != self.showCLIPrompt():
            self.communicateTTS(
                self.getTTSFormatter().format(w) + end,
                interrupt=(
                    self.getOption("tts_interrupt") if interrupt is None else interrupt
                ),
            )
            if not (self.getOption("tts_subtitles")):
                return

        if not (self.getOption("stdout")):
            return

        if not (color) and not (style):
            print(
                self._ringbuffer(self.getDisplayFormatter().format(w)),
                end=end,
                flush=flush,
            )
        else:
            print(
                self._ringbuffer(
                    style
                    + color
                    + self.getDisplayFormatter().format(w)
                    + Fore.RESET
                    + Style.RESET_ALL
                ),
                end=end,
                flush=flush,
            )

    def replaceForbidden(self, w):
        for forbidden in self.getOption("forbid_strings"):
            w = w.replace(forbidden, "")
        return w

    def bufferMultilineInput(self, w):
        if self.getOption("multiline"):
            # does not expect backslash at end
            self.multiline_buffer += w + "\n"
        else:
            # expects strings with \ at the end
            self.multiline_buffer += w[:-1] + "\n"

    def isMultilineBuffering(self):
        return self.multiline_buffer != ""

    def flushMultilineBuffer(self):
        w = self.multiline_buffer
        self.multiline_buffer = ""
        return w

    def expandDynamicFileVars(self, w):
        """Expands strings of the form `{[FILEPATH]}` by replacing them with the contents of FILE1 if it is found and readable."""
        if not (self.getOption("dynamic_file_vars")):
            return w

        pattern = r"\{\[(.*?)\]\}"
        matches = re.findall(pattern, w)

        for match in matches:
            var = "{[" + match + "]}"
            # users will probably mess around with this so we wanna be real careful
            common_error_msg = "error: Unable to expand '" + var + "'. "
            try:
                content = open(match, "r").read()
                w = w.replace(var, content)
            except FileNotFoundError:
                printerr(common_error_msg + "File not found.")
            except PermissionError:
                printerr(common_error_msg + "Operation not permitted.")
            except IsADirectoryError:
                printerr(common_error_msg + "'" + match + "' is a directory.")
            except UnicodeDecodeError:
                printerr(common_error_msg + "Unable to decode the file.")
            except IOError:
                printerr(common_error_msg + "Bad news: IO error.")
            except OSError:
                printerr(common_error_msg + "Operating system has signaled an error.")
            except:
                printerr(
                    common_error_msg
                    + "An unknown exception has occurred. Here's the full traceback."
                )
                printerr(traceback.format_exc())
        return w

    def modifyInput(prog, w):
        """Takes user input (w), returns pair of (modified user input, and a hint to give to the ai."""
        if (
            prog.isContinue() and prog.continue_with == ""
        ):  # user entered /cont or equivalent
            if prog.getMode().startswith("chat"):
                # prevent AI from talking for us
                if prog.showStory().endswith("\n"):
                    return ("", prog.adjustForChat("")[1])
            return ("", "")

        if (
            prog.continue_with != ""
        ):  # user input has been replaced with something else, e.g. a transcription
            w = prog.popContinueString()

        # dynamic vars must come before other vars for security reasons
        w = prog.expandDynamicFileVars(w)
        w = prog.session.expandVars(w)
        (w, ai_hint) = prog.adjustForChat(w)

        # tool_hint = agency.makeToolInstructionMsg() if prog.getOption("use_tools") else ""

        # user may also provide a hint. unclear how to best append it, we put it at the end
        user_hint = prog.session.expandVars(prog.getOption("hint"))
        if user_hint and prog.getOption("warn_hint"):
            printerr(
                "warning: Hint is set. Try /raw to see what you're sending. Use /set hint '' to disable the hint, or /set warn_hint False to suppress this message."
            )

        return (w, ai_hint + user_hint)

    def getSystemTokenCount(self):
        """Returns the number of tokens in system msg. The value is cached per session. Note that this adds +1 for the BOS token."""
        if self._systemTokenCount is None:
            # self._systemTokenCount = len(self.getBackend().tokenize(self.session.getSystem())) + 1
            self._systemTokenCount = (
                len(self.getBackend().tokenize(self.showSystem())) + 1
            )
        return self._systemTokenCount

    def getTemplate(self):
        return self.template

    def getRawTemplate(self):
        return RawTemplate()

    def showSystem(self):
        # vars contains system_msg and others that may or may not be replaced in the template
        vars = self.session.getVars().copy()
        if (
            self.getOption("use_tools")
            and (tool_hint := self.getOption("tools_hint")) != ""
        ):
            vars["system_msg"] += "\n" + tools_hint
        return self.getTemplate().header(**vars)

    def showStory(self, story_folder=None, append_hint=True) -> str:
        """Returns the current story as a unformatted string, injecting the current prompt template."""
        # new and possibly FIXME: we need to add another hint from agency.makeToolInstructionMsg when using tools, so we disable the hint here
        if self.getOption("use_tools"):
            append_hint = False

        if story_folder is None:
            sf = self.session.stories
        else:
            sf = story_folder
        if self.isContinue():
            # user hit enter and wants ai to keep talking. this is kind of like using the entire last reply as a hint -> no templating needed
            return self.getRawTemplate().body(
                sf.get(), append_hint, **self.session.getVars()
            )
        return self.getTemplate().body(sf.get(), append_hint, **self.session.getVars())

    def formatStory(self, story_folder=None, with_color=False):
        """Pretty print the current story (or a provided one) in a nicely formatted way. Returns pretty story as a string."""
        if story_folder is None:
            sf = self.session.stories
        else:
            sf = story_folder

        ws = []
        for item in sf.get().getData():
            if type(w := item.content) != str:
                continue
            if item.role == "assistant":
                ws.append(
                    (self.getAIColorFormatter() + self.getDisplayFormatter()).format(w)
                )
                continue
            ws.append(self.getDisplayFormatter().format(w))
        return "\n".join(ws)

    def buildPrompt(self, hint=""):
        """Takes an input string w and returns the full history (including system msg) + w, but adjusted to fit into the context given by max_context_length. This is done in a complicated but very smart way.
        returns - A string ready to be sent to the backend, including the full conversation history, and guaranteed to carry the system msg.
        """
        # FIXME: this is not smart. and it needs to be reworked badly. IIRC a lot of this was useful in the early days, now (march 2025) this kind of stuff is more obsolete because contexts are bigger and templates are handled server side
        # FIXME: it also just doesn't work since we changed how chat msgs are stored
        # another FIXME: also tokenizing needs to be fixed in the backends
        self._smartShifted = False  # debugging
        backend = self.getBackend()

        w = self.showStory() + hint
        gamma = self.getOption("max_context_length")
        k = self.getSystemTokenCount()
        n = self.getOption("max_length")
        # budget is the number of tokens we may spend on story history
        budget = gamma - (k + n)

        if budget < 0:
            # shit the bed
            return self.showSystem() + w

        if not (self.getOption("smart_context")):
            # currently we just dump FIXME: make this version keep the system prompt at least
            return self.showSystem() + w

        # now we need a smart story history that fits into budget
        sf = self.session.stories.copyFolder(only_active=True)
        while len(
            backend.tokenize(self.showStory(story_folder=sf) + hint)
        ) > budget and not (sf.empty()):

            # drop some items from the story, smartly, and without changing original
            self._smartShifted = True
            item = sf.get().pop(0)
            # FIXME: this can be way better, needs moretesting!
        return self.showSystem() + self.showStory(story_folder=sf) + hint

    def adjustForChat(self, w):
        """Takes user input w and returns a pair (w1, v) where w1 is the modified user input and v is a hint for the AI to be put at the end of the prompt."""
        v = ""
        if self.getMode() == "chat":
            w = mkChatPrompt(self.getOption("chat_user")) + w
            v = mkChatPrompt(self.session.getVar("chat_ai"), space=False)
        return (w, v)

    def communicate(self, prompt_text, stream_callback=None):
        """Sends prompt_text to the backend and returns results."""
        backend = self.getBackend()
        payload = self.makeGeneratePayload(prompt_text)
        self._lastPrompt = prompt_text
        if self.getOption("warn_trailing_space"):
            if prompt_text.endswith(" "):
                printerr(
                    "warning: Prompt ends with a trailing space. This messes with tokenization, and can cause the model to start its responses with emoticons. If this is what you want, you can turn off this warning by setting 'warn_trailing_space' to False."
                )

        # FIXME: this is only necessary until llamacpp implements streaming tool calls
        if self.getOption("stream"):
            # FIXME: this is the last hacky bit about formatting
            if self.getOption("chat_show_ai_prompt") and self.getMode().startswith(
                "chat"
            ):
                self.print(
                    self.session.getVar("chat_ai") + ": ",
                    end="",
                    flush=True,
                    interrupt=False,
                )
            if backend.generateStreaming(
                payload,
                lambda token, only_once=uuid.uuid4(): self._streamCallback(
                    token, user_callback=stream_callback, only_once=only_once
                ),
            ):
                printerr("error: " + backend.getLastError())
                return ""
            backend.waitForStream()
            self.setLastJSON(backend.getLastJSON())
            return self.flushStreamQueue()
        else:
            result = backend.handleGenerateResult(backend.generate(payload))
            self.setLastJSON(backend.getLastJSON())

        if not (result):
            printerr(
                "error: Backend yielded no result. Reason: " + backend.getLastError()
            )
            self.verbose(
                "Additional information (last request): \n"
                + json.dumps(backend.getLastRequest(), indent=4)
            )
            return ""
        return result

    def interact(
        self,
        w: str,
        user_generation_callback=None,
        generation_callback=None,
        stream_callback=None,
        blocking=False,
        timeout=None,
    ) -> None:
        """This is as close to a main loop as we'll get. w may be any string or user input, which is sent to the backend. Generation(s) are then received and handled. Certain conditions may cause multiple generations. Strings returned from the backend are passed to generation_callback.
        :param w: Any input string, which will be processed and may be modified, eventually being passed to the backend.
        :param generation_callback: A function that takes a string as input. This will receive generations as they arrive from the backend. Note that a different callback handles streaming responses. If streaming is enabled and this function prints, you will print twice. Hint: check for getOption('stream') in the lambda.
        :return: Nothing. Use the callback to process the results of an interaction, or use interactBlocking.
        """
        # internal state does not change during an interaction
        if generation_callback is None:
            generation_callback = self._print_generation_callback

        self._updateDatetime()
        self.freeze()

        def loop_interact(w):
            self._stop_generation.clear()
            communicating = True
            self._busy.set()
            (modified_w, hint) = self.modifyInput(w)
            self.addUserText(modified_w)
            while communicating:
                # this only runs more than once if there is auto-activation, e.g. with tool use
                if (
                    generated_w := self.communicate(
                        self.buildPrompt(hint), stream_callback=stream_callback
                    )
                ) == "":
                    # empty string usually means something went wrong.
                    # communicate will have already printed to stderr
                    # it is important that we don't loop forever here though, so we bail
                    communicating = False

                # if the generated string has tool calls, we apply them here
                tool_results, tool_call = self.applyTools(
                    generated_w, json=self.lastResult
                )

                output = ""
                if tool_results != []:
                    # need to add both the calls and result to the history
                    self.session.stories.get().addMessages([tool_call] + tool_results)
                    (w, hint) = ("", "")
                    if self.getOption("verbose"):
                        output = json.dumps(tool_results)
                else:
                    if self.getMode() != "chat":
                        output = self.addAIText(generated_w)
                    else:
                        # formatting is a bit broken in chat mode. Actually chat mode is a bit broken
                        output = generated_w
                    communicating = False

            # done communicating
            if user_generation_callback is not None:
                user_generation_callback(output)
            generation_callback(output)
            self.unfreeze()
            self._lastInteraction = time_ms()
            self._busy.clear()

        t = threading.Thread(target=loop_interact, args=[w])
        t.start()
        if blocking:
            t.join(timeout=timeout)
        if self.getOption("log_time"):
            printerr(showTime(self, []))

    def interactBlocking(self, w: str, timeout=None) -> str:
        temp = ""

        def f(v):
            nonlocal temp
            temp = v

        self.interact(w, user_generation_callback=f, timeout=timeout, blocking=True)
        return temp

    def _stopInteraction(self):
        """Stops an ongoing generation, regardless of wether it is streaming, blocking, or async.
        This is a low level function. Consider using stopAll instead.
        Note: Currently only works for streaming generations."""
        streaming.stop_streaming.set()
        self._stop_generation.set()

    def stopAll(self):
        """Stops ongoing generation and interrupts the TTS.
        The side effects of stopping generation depend on the method used, i.e. streaming generation will yield some partially generated results, while a blocking generation may be entirely discarded.
        Note: Currently only works for streaming generations."""
        self._stopInteraction()
        self.stopTTS()

    def hasImages(self):
        return bool(self.images) and self._images_dirty

    def loadImage(self, url, id):
        url = os.path.expanduser(url.strip())
        if not (os.path.isfile(url)):
            printerr("warning: Could not load image '" + url + "'. File not found.")
            return

        self.images[id] = ImageRef(url=url, data=loadImageData(url))
        self._images_dirty = True

    def setLastJSON(self, json_result):
        self.lastResult = json_result
        try:
            current_tokens = str(self.lastResult["usage"]["total_tokens"])
        except:
            # fail silently, it's not that important
            current_tokens = "?"

        self.session.setVar("current_tokens", current_tokens)

    def _updateDatetime(self) -> None:
        """Sets the datetime special var in the current session."""
        self.session.setVar("datetime", getAITime())

    def backup(self):
        """Returns a data structure that can be restored to return to a previous state of the program."""
        # copy strings etc., avoid copying high resource stuff or tricky things, like models and subprocesses
        return (self.session.copy(), copy.deepcopy(self.options))

    def restore(self, backup):
        (session, options) = backup
        self.session = session
        for k, v in options.items():
            self.setOption(k, v)

    def _stopHTTP(self):
        self.http_server.close()

    def _initializeHTTP(self):
        """Spawns a simple web server on its own thread.
        This will only serve the html/ folder included in ghostbox, along with the js files. This includes a minimal UI, and capabilities for streaming TTS audio and transcribing from user microphone input.
        Note: By default, this method will override terminal, audio and tts websock addresses. Use --no-http_override to suppress this behaviour.
        """
        import http.server
        import socketserver

        host, port = self.getOption("http_host"), self.getOption("http_port")

        # take care of terminal, audio and tts
        if self.getOption("http_override"):
            config = {
                "audio_websock_host": host,
                "audio_websock_port": port + 1,
                "audio_websock": True,
                "tts_websock_host": host,
                "tts_websock_port": port + 2,
                "tts_websock": True,
                "websock_host": host,
                "websock_port": port + 100,
                "websock": True,
            }

            for opt, value in config.items():
                self.setOption(opt, value)

        handler = partial(
            http.server.SimpleHTTPRequestHandler,
            directory=ghostbox.get_ghostbox_html_path(),
        )

        def httpLoop():
            with socketserver.TCPServer((host, port), handler) as httpd:
                printerr(
                    f"Starting HTTP server. Visit http://{host}:{port} for the web interface."
                )
                self.http_server = httpd
                httpd.serve_forever()

        self.http_thread = threading.Thread(target=httpLoop)
        self.http_thread.daemon = True
        self.http_thread.start()

    def initializeWebsock(self):
        """Starts a simple websocket server that sends and receives messages, behaving like a terminal client."""
        printerr("Initializing websocket server.")
        self.websock_server_running.set()
        self.websock_thread = threading.Thread(
            target=self._runWebsockServer, daemon=True
        )
        self.websock_thread.start()

        self.websock_regpl_thread = threading.Thread(
            target=regpl, args=[self, self._websockPopMsg], daemon=True
        )
        self.websock_regpl_thread.start()

        # handle printerr
        # FIXME: currently, we are not buffering while we send to a websock server, oh well
        util.printerr_callback = lambda w: self.websockSend(self.stderr_token + w)

    def stopWebsock(self):
        printerr("Stopping websocket server.")
        self.websock_server_running.clear()
        self.websock_clients = []
        self._printerr_buffer = ["Resetting stderr buffer."]
        util.printerr_callback = self._initial_printerr_callback

    def websockSend(self, msg: str) -> None:
        from websockets import ConnectionClosedError

        for i in range(len(self.websock_clients)):
            client = self.websock_clients[i]
            try:
                client.send(msg)
            except ConnectionClosedError:
                printerr(
                    "error: Unable to send message to "
                    + str(client.remote_address)
                    + ": Connection closed."
                )
                del self.websock_clients[i]

    def _websockPopMsg(self) -> str:
        """Pops a message of the internal websocket queue and returns it.
        This function blocks until a message is found on the queue."""
        while self.websock_server_running.is_set():
            try:
                return self.websock_msg_queue.get(timeout=1)
            except Empty:
                # timeout was hit
                # future me: don't remove this: get will super block all OS signals so we have to occasionally loop around or program will be unresponsive
                continue
            except:
                print(
                    "Exception caught while blocking. Shutting down gracefully. Below is the full exception."
                )
                printerr(traceback.format_exc())
                self.stopWebsock()

    def _runWebsockServer(self):
        import websockets
        import websockets.sync.server as WS

        def handler(websocket):
            remote_address = websocket.remote_address
            # printerr("[WEBSOCK] Got connection from " + str(remote_address))
            # update the new client with console log
            for msg in self._printerr_buffer:
                try:
                    websocket.send(self.stderr_token + msg)
                except ConnectionClosedError:
                    printerr(
                        "error: Connection with "
                        + str(remote_address)
                        + " closed during initial history update."
                    )
                    return

            self.websock_clients.append(websocket)
            try:
                while self.websock_server_running.is_set():
                    msg = websocket.recv()
                    if type(msg) == str:
                        self.websock_msg_queue.put(msg)
            except websockets.exceptions.ConnectionClosed:
                pass
            finally:
                self.websock_clients.remove(websocket)

        self.websock_server_running.set()
        self.websock_server = WS.serve(
            handler, self.getOption("websock_host"), self.getOption("websock_port")
        )
        printerr(
            "WebSocket server running on ws://"
            + self.getOption("websock_host")
            + ":"
            + str(self.getOption("websock_port"))
        )
        self.websock_server.serve_forever()

    def _startCLIPrinter(self) -> None:
        """Checks wether we are busy. If we are busy and then aren't, we print the CLI prompt."""
        # we don't want to use the self.print, nor printerr for this as both of them have side effects
        # this really is just for terminal users
        print_cli = lambda prefix="": print(
            prefix + self.showCLIPrompt(), file=sys.stderr, end="", flush=True
        )

        def cli_printer():
            while self.running:
                self._busy.wait()
                while self._busy.is_set():
                    # don't blow up potato cpu
                    # user can wait for their coveted cli for 10ms
                    time.sleep(0.1)
                    continue

                if self.getOption("quiet"):
                    continue

                # bingo
                if self._triggered:
                    # this means someone wrote a command etc so user pressed enter and we don't need a newline
                    print_cli(prefix="")
                    self._triggered = False
                    continue

                # AI generated a bunch of text. there may or may not be a newline. llama actually has this in the result json but we roll our own
                if self._stdout_ringbuffer.endswith("\n"):
                    print_cli()
                else:
                    print_cli(prefix="\n")

        self._cli_printer_thread = threading.Thread(target=cli_printer, daemon=True)
        self._cli_printer_thread.start()

    def triggerCLI(self, check: Optional[str] = None) -> None:
        """Signals that the user has pressed enter, usually executing a command.
        The entire purpose of this is to track wether we need to print a newline before printing the CLI prompt.
        :param check: If none, is checked for a newline at the end, determining wether to print a newline.
        """
        if check is None:
            self._triggered = True
        else:
            self._triggered = True if check.endswith("\n") else False

        # off to the races lulz
        self._busy.set()
        self._busy.clear()

    def ttsIsSpeaking(self) -> bool:
        """Returns true if the tts is currently speaking.
        If querying this is not supported by the tts_program, this method will always return False.
        """
        if self.getOption("tts_program") != "ghostbox-tts":
            return False

        if not (self.getOption("tts")):
            return False

        # to figure out if ghostbox-tts is speaking, we send a '<is_speaking> message
        # the return in stdout should be is_speaking: True'
        self.tts.write_line("<is_speaking>")
        time.sleep(0.1)
        lines = self.tts.get()
        return "is_speaking: True\n" in lines


def main():
    just_fix_windows_console()
    tagged_parser = makeTaggedParser(backends.default_params)
    parser = tagged_parser.get_parser()
    args = parser.parse_args()
    prog = Plumbing(
        options=args.__dict__,
        initial_cli_prompt=args.cli_prompt,
        tags=tagged_parser.get_tags(),
    )
    setup_plumbing(prog, args)

    if (prompt := prog.getOption("prompt")) is not None:

        def input_once():
            prog.running = False
            return prompt

        regpl(prog, input_function=input_once)
    else:
        regpl(prog)


def setup_plumbing(
    prog: Plumbing, args: Namespace = Namespace(), protected_keys: List[str] = []
) -> None:
    # the following is setup, though it is subtly different from Plumbing.init, so beware
    if userConfigFile():
        prog.setOption("user_config", userConfigFile())
        printerr(
            loadConfig(
                prog, [userConfigFile()], override=False, protected_keys=protected_keys
            )
        )

    if prog.getOption("config_file"):
        printerr(loadConfig(prog, [prog.options["config_file"]]))

    try:
        # this can fail if called from api
        if "-u" in sys.argv or "--chat_user" in sys.argv:
            prog.setOption("chat_user", args.chat_user)
    except AttributeError:
        # we don't have arg.xxx -> fine
        pass

    if prog.getOption("character_folder"):
        printerr(newSession(prog, []))

    if prog.getOption("hide"):
        hide(prog, [])

    if prog.getOption("tts"):
        prog.tts_flag = True

    if prog.getOption("image_watch"):
        del prog.options["image_watch"]
        prog.setOption("image_watch", True)

    if prog.getOption("http"):
        del prog.options["http"]
        prog.setOption("http", True)

    if prog.getOption("websock") and not (prog.websock_server_running.is_set()):
        del prog.options["websock"]
        prog.setOption("websock", True)

    if prog.getOption("audio") and prog.ct is None:
        del prog.options["audio"]
        prog.setOption("audio", True)


def regpl(prog: Plumbing, input_function: Callable[[], str] = input) -> None:
    """Read user input, evaluate, generate LLM response, print loop."""
    skip = False
    prog.triggerCLI()
    prog._busy.clear()

    # check if someone started the cli without a char and try to be helpful
    if not (prog.getOption("character_folder")):
        printerr(
            "warning: Running ghostbox without a character folder. Use `/start <charname>` to actually load an AI character.\n  For example `/start ghostbox-helper` will load a helpful AI that can explain ghostbox to you.\n  Other choices are: "
            + ", ".join(all_chars(prog))
        )

    while prog.running:
        last_state = prog.backup()
        try:
            # have to do TTS here for complex reasons; flag means to reinitialize tts, which can happen e.g. due to voice change
            if prog.tts_flag:
                prog.tts_flag = False
                prog.options["tts"] = False
                printerr(toggleTTS(prog, []))

            if prog.initial_print_flag:
                prog.initial_print_flag = False
                print("\n\n" + prog.formatStory(), end="")

            w = input_function()

            # check for multiline
            # this works different wether we have multiline mode enabled, or are doing ad-hoc multilines
            if prog.getOption("multiline"):
                # multiline mode
                if w != prog.getOption("multiline_delimiter"):
                    prog.bufferMultilineInput(w)
                    continue
                else:
                    # :-1 for trailing newline
                    w = prog.flushMultilineBuffer()[:-1]
            else:
                # ad hoc multilines
                if w.endswith("\\") and not (w.endswith("\\\\")):
                    prog.bufferMultilineInput(w)
                    continue
                elif prog.isMultilineBuffering():
                    w = prog.flushMultilineBuffer() + w

            # for convenience when chatting
            if w == "":
                # New: changed from
                # w = "/cont"
                # to the below because /cont wasn't used much and continue doesn't work very well with OAI API which is getting more prevalent
                prog.stopAll()
                prog.triggerCLI()
                continue

            # expand session vars, so we can do e.g. /tokenize {{system_msg}}
            w = prog.session.expandVars(w)

            for cmd, f in cmds:
                # FIXME: the startswith is dicey because it now makes the order of cmds defined above relevant, i.e. longer commands must be specified before shorter ones.
                if w.startswith(cmd):
                    # execute a / command
                    v = f(prog, w.split(" ")[1:])
                    printerr(v)
                    prog.triggerCLI(check=v)
                    if not (prog.isContinue()):
                        # skip means we don't send a prompt this iteration, which we don't want to do when user issues a command, except for the /continue command
                        skip = True
                    break  # need this to not accidentally execute multiple commands like /tts and /ttsdebug

            if skip:
                skip = False
                continue

            # this is the main event

            # for CLI use, we want a new generation to first stop all ongoing generation and TTS
            prog.stopAll()
            prog.interact(w, generation_callback=prog._print_generation_callback)

            prog.resetContinue()
        except KeyboardInterrupt:
            prog.running = False
            sys.exit
        except:
            printerr("error: Caught unhandled exception in main()")
            printerr(traceback.format_exc())
            try:
                prog.restore(last_state)
            except:
                printerr(
                    "error: While trying to recover from an exception, another exception was encountered. This is very bad."
                )
                printerr(traceback.format_exc())
                printerr(saveStoryFolder(prog, []))
                sys.exit()
            printerr("Restored previous state.")


if __name__ == "__main__":
    main()

```

## output_formatter.py

```python
from abc import ABC, abstractmethod
from functools import *
from ghostbox.util import *
from colorama import Fore, Style

class OutputFormatter(ABC):
    """An interface for formatting the chat history. The OutputFormatter interface is used both for formatted text that is presented to the user, as well as formatting text to send to the backend, e.g. to prepend 'Bob:' style chat prompts.
OutputFormatters work on both AI and human text and ensure a certain style, without restricting the LLM's grammar outright. Because the LLM will pick up on your style to varying degrees, depending on various factors, the following law must hold for OutputFormatters:

    ```
f = OutPutFormatter()
f.format(w) == f.format(f.format(w))
```

for all strings w. In other words, f is idempotent under repeated application. We might get a string 'How are you, Sally?' which the formatter turns into 'Bob: How are you, Sally?', but we don't want this to turn into 'Bob: Bob: How are you, Sally?' in case the LLM starts to prepend 'Bob: ' itself."""

    @abstractmethod
    def format(self, w):
        pass

    def compose(xs):
        """Takes a list of objects supporting the OutputFormatter interface and returns a ComposedFormatter that is the result of their composition."""
        return reduce(ComposedFormatter, xs, IdentityFormatter())

    def __add__(self, other):
        return ComposedFormatter(self, other)
    
    def sequence(xs, w):
        """Takes a list of OutputFormatters xs and applies them to w in sequence. Tip: Read the list from left to right and imagine applying each formatter to the string in succession."""
        return reduce(lambda v, f: f.format(v), xs, w)
    
class IdentityFormatter(OutputFormatter):
    """This formatter returns its input unchanged."""
    def __init__(self):
        pass
    
    def format(self, w):
        return w

class ComposedFormatter(OutputFormatter):
    """Allows composition of two given OutputFormatters.
In general, the following laws hold

```
a = OutputFormatter() #i.e. a has superclass OutputFormatter
e = IdentityFormatter()
    c = ComposedFormatter(a, e)

c.format(w) == a.format(w)
    ```

    for any given string w. In other words, IdentityOutputFormatter is the identity element for composition of formatters. Also

    ```
    a = OutputFormatter()
    b = OutputFormatter()
    c1 = ComposedFormatter(a, b)
    c2 = ComposedFormatter(b, a)

    c1.format(w) != c2.format(w)
    ```

    at least not for all a and b. So in general, composition of formatters does not commute."""

    def __init__(self, a, b):
        self.a = a
        self.b = b

    def format(self, w):
        return self.a.format(self.b.format(w))
    
class NicknameFormatter(OutputFormatter):
    """This formatter prepends chat names, according to a given decorator. By default, names are decorated with colon, as in 'Bob: '."""
    
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self.decorator_func=decorator_func
        self.nickname = nickname

    def setNickname(self, nickname):
        self.nickname = nickname
        return self
        
    def format(self, w):
        def f(v, line, nick=self.decorator_func(self.nickname)):
            if line.startswith(nick):
                return v + "\n" + line
            else:
                return v + "\n" + nick + line
        return reduce(f, w.split("\n"), "")[1:]

    def unformat(self, w):
        def f(v, line, nick=self.decorator_func(self.nickname)):
            if line.startswith(nick):
                return v + "\n" + line[len(nick):]
            else:
                return v + "\n" + line
        return reduce(f, w.split("\n"), "")[1:]

class NicknameRemover(OutputFormatter):
    """Removes prepended nicknames according to a supplied decorator. By default, this removes stuff like 'Bob: ', and it matches it a little fuzzy to account for spaces."""
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self._f = NicknameFormatter(nickname, decorator_func)
        
    def format(self, w):
        return self._f.unformat(w)
    
class IncompleteSentenceCleaner(OutputFormatter):
    """Removes incomplete sentences at the end of text."""

    def __init__(self,     stopchars = '! . ? ; `'.split(" ")):
        self.stopchars = stopchars
        
    def format(self, w):
        if w == "":
            return w


        exclusions = []
        # exclude stuff like '1.', '2.' etc
        exclusions += list(re.finditer(r"\d\.", w))

        skip = False
        stopchars = self.stopchars
        for i in range(len(w)-1, -1, -1):
            for match in exclusions:
                if i > match.start() and i < match.end():
                    skip = True

            if skip:
                skip = False
                continue
            if w[i] in stopchars:
                break

        if i == 0:
            return ""
        return w[:i+1]

class WhitespaceFormatter(OutputFormatter):
    """Removes whitespace at the beginning and end of text."""

    def format(self, w):
        return w.strip()
    
class LonelyPromptCleaner(OutputFormatter):
    """Removes lonely occurrences of trailing chat prompts. Like trailing 'Bob: ' etc, since these are hard to spot with incomplete sentence cleaning. You can supply another decorator besides the default '<nick>: ' pattern. Also note that cleaning is a little fuzzy when it comes to spaces."""
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self.decorator_func = decorator_func
        self.nickname = nickname

    def setNickname(self, nickname):
        self.nickname = nickname
        return self
        
    def format(self, w):
        prompt = self.decorator_func(self.nickname)
        ws = w.split("\n")
        return "\n".join(filter(lambda v: not(v in prompt), ws))


class ChatFormatter(OutputFormatter):
    """General purpose chat formatter with some extra functionality. Will prepend nicknames according to a decorator, and cleans up some bogus responses."""
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self.decorator_func = decorator_func
        self.nickname = nickname

    def setNickname(self, nickname):
        self.nickname = nickname

    def format(self, w):
        return OutputFormatter.sequence([IncompleteSentenceCleaner(),
                                         LonelyPromptCleaner(self.nickname, decorator_func=self.decorator_func),
                                         NicknameFormatter(self.nickname, decorator_func=self.decorator_func)],
                                        w)


class ColorFormatter(OutputFormatter):
    """Adds a given color and style to the output. Color and style can be provided as strings, e.g. 'red' and 'bright'm which will be converted to ANSI terminal codes."""
    def __init__(self, color, style="default"):
        super().__init__()
        self.color = stringToColor(color)
        self.style = stringToStyle(style)

    def format(self, w):
        return wrapColorStyle(w, self.color, self.style)
        
DoNothing = IdentityFormatter() # more descriptive name                
DefaultResponseCleaner = OutputFormatter.compose([IncompleteSentenceCleaner()])
CleanResponse = DefaultResponseCleaner
DefaultResponseCleaner.__doc__ =     """Does minimum cleanup. Intended for AI responses."""


    

```

## pftemplate.py

```python
import os, glob
from abc import ABC, abstractmethod
from functools import *
from ghostbox.util import *

class PFTemplate(ABC):
    """Abstract base class for prompt format templates, used to turn Story objects into properly formatted strings."""

    @abstractmethod
    def header(self, system_msg, **kwargs):
        pass

    @abstractmethod
    def body(self, story, append_hint=True, **kwargs):
        pass

    @abstractmethod
    def strip(self, w):
        pass

    @abstractmethod
    def stops(self):
        """Returns a list of strings that may stop generation. This is intended for EOS delimiters, like <|im_end|> etc."""
        pass
    
class FilePFTemplate(PFTemplate):
    """Simple, customizable prompt format templates based on loading dictionaries with certain files.

Files expected:
    system - Will be prepended to every prompt. It should contain '{{system_msg}}', which will be replaced by the actual content of the system prompt when the header method is called.
    begin_user - Contains string that will be prepended to user messages. Be sure to include newlines, if you want them
    end_user - Contains string that will be appended to user message.
    begin_assistant - Contains string that will be prepended to generated AI message. This may be the same as begin_user, or it may differ.
    end_assistant - Contains string that will be appended to the generated AI message.
    stop_lines - Contains strings that will cause generation to stop, seperated by newlines
    hint - Contains a special string that is sometimes appended at the end of a user message. It should contain a string that guides the AI's compleetion, e.g. this may just be '<|im_start|>assistant\n' in the case of chat-ml, which will heavily discourage the LLM from speaking for the user. This is only appended when append_hint is True in the body method.

The following files are optional:
    begin_system - Special tokens to be prepended to a system message within the chat history, e.g. a tool or function call result. This will be prepended to message with role=system in the chat history.
    end_system - Special tokens to be appended to a system message within the chat history, e.g. a tool or function call result. This will be appended to message with role=system in the chat history.
    If optional files are missing but their conditions are met (e.g. a message  with role=system), the template will default to something reasonable (e.g. begin_assistant)

All methods accept additional **kwargs, which contain replacements for double curly braced strings in the story content and system message. Things like '{{char_name}}' etc.
Example:
    ```
from ghostbox.Story import *
s = Story()
s.addUserText("The {{adjective}}, brown fox jumps over the lazy hedgehog!")
t = FilePFTemplate("templates/chat-ml")
print(t.body(s, append_hint=True, adjective="quick"))
```

Output:
    
```
The quick, brown fox jumps over the lazy hedgehog!<|im_end|><|im_start|>assistant

```
"""    

    var_decorator = lambda w: "{{" + w + "}}"
    
    def __init__(self, dir):
        self.dir = dir
        self._loadFiles()

    def _loadFiles(self):
        if not(os.path.isdir(self.dir)):
            raise FileNotFoundError("Could not find path " + self.dir)

        allfiles = glob.glob(self.dir + "/*")
        for filepath in allfiles:
            filename = os.path.split(filepath)[1]
            if os.path.isfile(filepath):
                self.__dict__[filename] = open(filepath, "r").read()
                
    def header(self, system_msg, **kwargs):
        return replaceFromDict(self.system.replace("{{system_msg}}", system_msg), kwargs, key_func=FilePFTemplate.var_decorator)

    def body(self, story, append_hint=True, **kwargs):
        def build(w, item):
            # you could do this more modular but why? this way users see the files and the template scheme is obvious. I bet this covers 99% of actual use cases for LLM
            content = replaceFromDict(item.content, kwargs, key_func=FilePFTemplate.var_decorator)
            if item.role == "user":
                return w + self.begin_user + content + self.end_user
            elif item.role == "assistant":
                return w + self.begin_assistant + content + self.end_assistant
            elif item.role == "system":
                # this is a bit more hairy
                begin = self.__dict__.get("begin_system", self.begin_assistant)
                end = self.__dict__.get("end_system", self.end_assistant)
                return w + begin + content + end
            # throw if people use weird or no roles with this template
            raise ValueError(item.role + " is not a valid role for this template.")
        if append_hint:
            hint = replaceFromDict(self.hint, kwargs, key_func=FilePFTemplate.var_decorator)
        else:
            hint = ""
        return reduce(build, story.getData(), "") + hint
    def stops(self):
        return self.stop_lines.split("\n")
        
    
    def strip(self, w):
        #FXIME: only preliminary for testing like this
        targets = [self.begin_user, self.begin_assistant, self.end_user, self.end_assistant]
        return reduce(lambda v, target: v.replace(target, ""), targets, w)
        


class RawTemplate(PFTemplate):
    """This is a dummy template that doesn't do anything. Perfect if you want to experiment."""
    def header(self, system_msg, **kwargs):
        return system_msg


    def body(self, story, append_hint=True, **kwargs):
        return "".join([item.content for item in story.getData()
                          if type(item.content) == str])
            
    def stops(self):
        return []
    
    def strip(self, w):
        return w

```

## session.py

```python
import os, glob, time
from copy import deepcopy
from ghostbox.util import *
from ghostbox.StoryFolder import *
from ghostbox.agency import *


class Session(object):
    special_files = "chat_ai config.json tools.py".split(" ")

    def __init__(
        self, dir=None, chat_user="", chat_ai="", additional_keys=[], tools_forbidden=[]
    ):
        self.dir = dir
        self.fileVars = {
            "chat_user": chat_user,
            "chat_ai": chat_ai,
            "system_msg": "",
            "current_tokens": "0",
            "datetime": getAITime(),
        }
        self.stories = StoryFolder()
        self.tools = []
        self.tools_file = ""
        self.tools_module = None
        if self.dir is not None:
            self._init(additional_keys, tools_forbidden)

    def copy(self):
        # can't deepcopy a module
        tmp = self.tools_module
        self.tools_module = None
        new = deepcopy(self)
        # module becomes a singleton
        self.tools_module = tmp
        new.tools_module = tmp
        return new

    def merge(self, other):
        """Merges some things from a session object other into itself. This generally means keeping story etc, of self, but possibly adding fileVars from other, including overriding our own."""
        self.fileVars = self.fileVars | other.fileVars
        self.dir = other.dir

    def hasVar(self, var):
        return var in self.fileVars

    def getVar(self, var, default=None):
        if var not in self.fileVars:
            if default is None:
                printerr(
                    "warning: session.getVar: Key not defined '"
                    + var
                    + "'. Did you forget to create "
                    + self.dir
                    + "/"
                    + var
                    + "?"
                )
                return ""
            else:
                return default
        return self.expandVars(self.fileVars[var])

    def setVar(self, name, value):
        self.fileVars[name] = value

    def getVars(self):
        return {k: self.expandVars(v) for (k, v) in self.fileVars.items()}

    def getSystem(self):
        return self.getVar("system_msg")

    def expandVars(self, w, depth=3):
        """Expands all variables of the form {{VAR}} in a given string w, if VAR is a key in fileVars. By default, will recursively expand replacements to a depth of 3."""
        for i in range(0, depth):
            w_new = replaceFromDict(w, self.fileVars, lambda k: "{{" + k + "}}")
            if w == w_new:
                break
            w = w_new
        return w_new

    def _init(self, additional_keys=[], tools_forbidden=[]):
        if not (os.path.isdir(self.dir)):
            raise FileNotFoundError("Could not find path " + self.dir)

        allfiles = glob.glob(self.dir + "/*") + additional_keys
        for filepath in allfiles:
            filename = os.path.split(filepath)[1]
            if os.path.isfile(filepath) and filename not in self.special_files:
                # self.fileVars[filename] = self.expandVars(open(filepath, "r").read())
                self.fileVars[filename] = open(filepath, "r").read()
                # this is useful but too verbose
                # printerr("Found " + filename)
            elif filename == "tools.py":
                self.tool_file = filepath
                (self.tools, self.tools_module) = makeTools(
                    filepath,
                    display_name=os.path.basename(self.dir) + "_tools",
                    tools_forbidden=tools_forbidden,
                )

        init_msg = self.getVar("initial_msg", "")
        if init_msg:
            self.stories.get().addAssistantText(init_msg)

    def callTool(self, name, params):
        if name not in [tool.function.name for tool in self.tools]:
            return
        try:
            f = getattr(self.tools_module, name)
        except:
            printerr(
                "warning: Failed to call tool '"
                + name
                + "': Not found in module '"
                + self.tools_file
                + "'."
            )
            printerr(traceback.format_exc())
            return

        # we have to build a function call somewhat laboriously because the order of arguments is not guaranteed
        try:
            pargs = [params[arg] for arg in getPositionalArguments(f)]
        except KeyError:
            printerr(
                "warning: Couldn't call tool '"
                + name
                + "': Required positional parameter missing."
            )
            printerr(traceback.format_exc())
            return

        kwargs = {k: v for (k, v) in params.items() if k in getOptionalArguments(f)}

        # here goes nothing
        try:
            result = f(*pargs, **kwargs)
        except:
            printerr(
                "warning: Caught exception when calling tool '"
                + name
                + "'. Here's a dump of the arguments:"
            )
            printerr(json.dumps(params, indent=4))
            printerr("\nAnd here is the full exception:")
            printerr(traceback.format_exc())
            return
        return result

```

## StoryFolder.py

```python
import jsonpickle #type:ignore
from pydantic import BaseModel
import copy 
from ghostbox.Story import *

class StoryFolder(object):
    """Thin wrapper around a list of Story objects."""

    def __init__(self, json_data=None):
        self.stories = [Story()]
        self.index = 0 # points to where to append next
        if json_data:
            self.stories = jsonpickle.loads(json_data) # throw if illegal json
            # FIXME: this will crash and burn if json is bogus, but oh well

    def empty(self):
        return self.stories[self.index] == []
            
    def get(self):
        return self.stories[self.index]
    
    def newStory(self):
        self.stories.append(Story())
        self.index = len(self.stories) - 1

    def reset(self) -> None:
        """Reset storyfolder to an empty state."""
        self.stories = [Story()]
        self.index = 0
        
    def cloneStory(self, index=-1):
        if index == -1:
            # -1  means currrent story
            index = self.index

        l = len(self.stories)
        if index >= 0 and index < l:
            self.stories.append(copy.deepcopy(self.stories[index]))
            self.index = l

    def copyFolder(self, only_active=False):
        sf = StoryFolder()
        if only_active:
            sf.stories = copy.deepcopy(self.stories[self.index:self.index+1])
        else:
            sf.stories = copy.deepcopy(self.stories)
            sf.index = self.index
        return sf
        
    def _shiftStory(self, i):
        l = len(self.stories)
        newIndex = self.index + i
        if newIndex >= l:
            return 1

        if newIndex < 0:
            return -1

        self.index = newIndex
        return 0

    def nextStory(self):
        return self._shiftStory(1)

    def previousStory(self):
        return self._shiftStory(-1)
    
    def toJSON(self):
        return jsonpickle.dumps(self.stories)
    
    def shiftTo(self, i):
        l = len(self.stories)
        if i >= l or i < 0:
            return "Index out of range."
        self.index = i
        return ""
    

```

## Story.py

```python
from typing import *
from pydantic import BaseModel
from ghostbox.definitions import *

class Story(BaseModel):
    """A story is a thin wrapper around a list of ChatMessages."""
    
    data: List[ChatMessage] = [] 

    def addUserText(self, w: str, image_context: Dict[int, ImageRef]={}, **kwargs) -> None:
        """Adds a user message to the story.
        :param w: The user's prompt or message as plaintext.
        :param images: A list of 0 or more images to include with the message. The images, confusingly, may be http URLs, filenames, or binary data.
        """
        if image_context == {}:
            new_data = ChatMessage(role = "user", content = w, **kwargs)
        else:
            new_data = ChatMessage.make_image_message(w, image_context.values(), **kwargs)
        self.data.append(new_data)

    def addAssistantText(self, w: str, **kwargs):
        self.data.append(ChatMessage(role="assistant", content= w, **kwargs))

    def addRawJSON(self, json: Dict[str, Any]) -> None:
        """Try to parse a raw json dictionary as a ChatMessage and then append it to the story.
        This will throw if the parsing fails."""
        self.data.append(ChatMessage(**json))
        
    def addRawJSONs(self, json_list: List[Dict[str, Any]]) -> None:
        """Add one or more python dictionaries that will be interpreted as ChatMessages and appended to the story.
                         If any of the passed dictionaries don't conform to the ChatMessage schema, you will get a pydantic ValidationError."""
        self.data.extend([ChatMessage(**item)
                          for item in json_list])

    def addMessage(self, msg: ChatMessage) -> None:
        """Appends a chat message to the story."""
        self.data.append(msg)
        
    def addMessages(self, msgs: List[ChatMessage]) -> None:
        """Appends ChatMessages to the story."""
        self.data.extend(msgs)
        
    def addSystemText(self, w: str, **kwargs) -> None:
        self.data.append(ChatMessage(role="system", content= w, **kwargs))

    def extendAssistantText(self, w: str) -> None:
        """Alters the latest found message in the story that is by the assistant, and extends it with w. If no such message exists, it adds w as an assistant message to the story."""
        for i in range(-1, -1*(len(self.data)+1), -1):
            # go through msgs from back to front
            msg = self.data[i]
            if msg.role == "assistant":
                if msg.content is None:
                    # this case is too weird, we just skip empty content
                    continue
                elif type(msg.content) == str:
                    # easy case
                    msg.content += w
                    return
                else:
                    # the content is complex -> a list of images + text or smth
                    for cmsg in msg.content:
                        if cmsg.type == "text":
                            cmsg.content += w
                            return

        # if we reached this point, no assistant was found
        # we just append a new message
        self.addAssistantText(w)
                

                
                

            
    def extendLast(self, w:str) -> None:
        """Appends w to the last message. Does nothing if there are no messages."""
        if self.data == []:
            return
        
        msg = self.data[-1]
        if msg.content is None:
            # tricky case, I say we do nothing
            return
        elif type(msg.content) == str:
            msg.content += w
        else:
            # the message is complex
            for cmsg in msg.content:
                # this is a truly weird case, I guess we mirror the behaviour of extend assistant and just extend the first text field
                if cmsg.type == "text":
                    cmsg.content += w
                    return
                
            
            
    def getData(self) -> List[ChatMessage]:
        return self.data

    def drop(self, n:int=-1) -> bool:
        """Safely remove the nth story item from the story. Defaults to the last item. If there are no elements, of if n is out of range, this has no effect. Returns True if an item was removed."""
        if self.data == []:
            return False

        if n == -1:
            n = len(self.data) - 1
        
        if not(n in range(0, len(self.data))):
            return False
        self.pop(n)
        return True
    
    
    def pop(self, n:int=-1) -> ChatMessage:
        """Remove and return the last story item. If n is supplied, item at position n is removed and returned."""
        return self.data.pop(n)
            
    def dropUntil(self, predicate: Callable[[ChatMessage], bool]) -> bool:
        """Drops chat messages from the back of the list until predicate is True. Predicate takes a ChatMessage as argument. Returns true if predicate was true for an item."""
        while self.data != []:
            if predicate(self.data[-1]):
                return True
            self.pop()
        return False
        
    def to_json(self) -> List[Dict[str, Any]]:
        """Returns internal data as json models.
        Shorthand for mapping model_dump over a getData() call."""
        return [msg.model_dump() for msg in self.data]

```

## streaming.py

```python
import requests, json
#from requests_html import HTMLSession
from time import sleep
from threading import Thread, Event
from ghostbox.util import printerr

# FIXME:   # poor man's closure; somehow this isn't enough yet to warrant making a class
stop_streaming = Event()

def connect_to_endpoint(url, prompt, headers=""):
    try:
        #session = HTMLSession()
        session = requests.Session()
        r = session.post(url, json=prompt, stream=True, headers=headers)
        return r
    except Exception as e:
        printerr(f"Error connecting to {url}: {e}")
        return None



  
    
def process_sse_streaming_events(callback, done_flag, r):
    global stop_streaming
    for event in r.iter_lines():
        if stop_streaming.is_set():
            # FIXME: this being global will be an issue :/
            break
            
        if event:
            w = event.decode()
            if w == "data: [DONE]":
                # openai do this
                #break
                pass
            elif w.startswith("data: "):
                d = json.loads(w[6:])            
                callback(d)
            else:
                # this works usually if people aren't actually streaming, but maybe we should just crash
                printerr("warning: Malformed data in process_sse_streaming_events. Are you actually streaming?")
                d = json.loads(w)            
                callback(d)                

                
    done_flag.set()


def streamPrompt(callback, done_flag, url, json="", headers=""):
    global stop_streaming
    stop_streaming.clear()
    
    response = connect_to_endpoint(url, json, headers=headers)
    if response:
        thread = Thread(target=process_sse_streaming_events, args=(callback, done_flag, response))
        thread.start()
    return response

```

## transcribe.py

```python
import whisper
import time, os, sys, contextlib, threading
import wave
import tempfile
from ctypes import *
import pyaudio
from pydub import pyaudioop
import audioop
import math
from collections import deque
from queue import Queue
import websockets.sync.server as WS
import websockets
import numpy as np
from ghostbox.util import printerr


def loadModel(name="base.en"):
    return whisper.load_model(name)

def getWhisperTranscription(filename, model):
    result = model.transcribe(filename, fp16=False)
    return result["text"].strip()

# unfortunately pyaudio will give a bunch of error messages, which is very irritating for using it in a shell program, so we supress the msgs
@contextlib.contextmanager
def ignoreStderr():
    devnull = os.open(os.devnull, os.O_WRONLY)
    old_stderr = os.dup(2)
    sys.stderr.flush()
    os.dup2(devnull, 2)
    os.close(devnull)
    try:
        yield
    finally:
        os.dup2(old_stderr, 2)
        os.close(old_stderr)

class WhisperTranscriber(object):
    def __init__(self, model_name="base.en", silence_threshold=2500, input_func=None):
        """model_name is the name of a whisper model, e.g. 'base.en' or 'tiny.en'.
        silence_threshold is an integer value describing a decibel threshold at which recording starts in the case of continuous transcribing.
input_func is a 0-argument function or None. If not None, it is called before transcribing, though only with the one-shot 'transcribe' method, not with transcribeContinuously. You can use this to print to stdout, or play a sound or do anything to signal to the user that recording has started."""
        self.model = loadModel(model_name)
        self.silence_threshold = silence_threshold
        self.input_func = input_func

    def transcribeWithPrompt(self, input_msg="", input_func=None, input_handler=lambda w: w):
        """Records audio directly from the microphone and then transcribes it to text using Whisper, returning that transcription.
input_msg - String that will be shown at the prompt.
input_func - 0-argument callback function that will be called immediately before prompt. This will be called in addition to, and immediately after, WhisperTranscriber.input_func
input_handler - Function that takes the user supplied input string as argument. This is most often unused as the user just presses enter, but sometimes you may use this to check for /quit etc.
        Returns - String signifying the transcript of the recorded audio.
This function will record from the point it is called and until the user hits enter, as per the builtin input() function."""

        # Create a temporary file to store the recorded audio (this will be deleted once we've finished transcription)
        temp_file = tempfile.NamedTemporaryFile(suffix=".wav")

        sample_rate = 16000
        bits_per_sample = 16
        chunk_size = 1024
        audio_format = pyaudio.paInt16
        channels = 1

        def callback(in_data, frame_count, time_info, status):
            wav_file.writeframes(in_data)
            return None, pyaudio.paContinue

        # Open the wave file for writing
        wav_file = wave.open(temp_file.name, 'wb')
        wav_file.setnchannels(channels)
        wav_file.setsampwidth(bits_per_sample // 8)
        wav_file.setframerate(sample_rate)

        # Suppress ALSA warnings (https://stackoverflow.com/a/13453192)
        ERROR_HANDLER_FUNC = CFUNCTYPE(None, c_char_p, c_int, c_char_p, c_int, c_char_p)
        def py_error_handler(filename, line, function, err, fmt):
            return

        c_error_handler = ERROR_HANDLER_FUNC(py_error_handler)
        asound = cdll.LoadLibrary('libasound.so')
        asound.snd_lib_error_set_handler(c_error_handler)

        # Initialize PyAudio
        audio = None
        with ignoreStderr():
            audio = pyaudio.PyAudio()

        # Start recording audio
        stream = audio.open(format=audio_format,
                            channels=channels,
                            rate=sample_rate,
                            input=True,
                            frames_per_buffer=chunk_size,
                            stream_callback=callback)

        if self.input_func:
            self.input_func()
        if input_func:
            input_func()

        input_handler(input(input_msg))
        # Stop and close the audio stream
        stream.stop_stream()
        stream.close()
        audio.terminate()

        # Close the wave file
        wav_file.close()

        # And transcribe the audio to text (suppressing warnings about running on a CPU)
        result = getWhisperTranscription(temp_file.name, self.model)
        temp_file.close()
        return result

    def transcribeContinuously(self, callback=None, on_threshold=None, websock=False, websock_host="localhost", websock_port=5051):
        """Starts recording continuously, transcribing audio when a given volume threshold is reached.
        This function is non-blocking, but returns a ContinuousTranscriber object, which runs asynchronously and can be polled to get the latest transcription (if any).
        Alternatively or in addition to polling, you can allso supply a callback, which gets called whenever a string is transcribed with the string as argument.
        :param callback: Function taking a string as argument. Gets called on a successful transcription.
        :param on_threshold: A zero argument function that gets called whenever the audio threshold is crossed while recording.
        :param websock: Boolean flag to enable WebSocket recording.
        :param websock_host: The hostname to bind the websocket server to.
        :param websock_port: The listening port for the WebSocket connection.
        :return: A ContinuousTranscriber object."""
        return ContinuousTranscriber(self.model, self.silence_threshold, callback=callback, on_threshold=on_threshold, websock=websock, websock_host=websock_host, websock_port=websock_port)


class ContinuousTranscriber(object):
    def __init__(self, model, silence_threshold, callback=None, on_threshold=None, websock=False, websock_host="localhost", websock_port=5051):
        self.model = model
        self.callback = callback
        self.on_threshold = on_threshold
        self.silence_threshold = silence_threshold
        # sampel rate is in self._samplerate. This is a tricky value, as it gets set by the client in websock mode.
        self._set_samplerate(44100) 
        self.buffer = []
        self.running = False
        self.resume_flag = threading.Event()
        self.payload_flag = threading.Event()
        self.websock = websock
        self.websock_host = websock_host
        self.websock_port = websock_port
        self.websock_server = None
        #self.audio_buffer = b""
        self.audio_buffer = Queue()
        self._spawnThread()
        if self.websock:
            self._setup_websocket_server()                    

    def _handle_client(self, websocket):
        while self.running:
            try:
                packet = websocket.recv(1024)
                if type(packet) == str:
                    w = packet
                    if w.startswith("samplerate:"):
                        #printerr("[DEBUG] Setting " + w)
                        self._set_samplerate(int(w.split(":")[1]))
                    elif w == "END":
                        self.running = False
                        self.resume_flag.set()
                else:
                    #self.audio_buffer += packet
                    # FIXME: using queue here means chunks might be < 1024, we could use a bytearray in this loop to buffer until we have a chunk
                    # this only happens on buffer underrun though, e.g. during high network latency. It's ok to fail transcribing in such cases, this should be handled by record_on_detect
                    self.audio_buffer.put(packet)
            except websockets.exceptions.ConnectionClosed:
                self.running = False
                self.resume_flag.set()

                
    def _setup_websocket_server(self):
        def run_server():
            printerr("Starting websock server for audio transcription.")
            self.websock_server = WS.serve(self._handle_client, host=self.websock_host, port=self.websock_port)
            self.websock_server.serve_forever()

        server_thread = threading.Thread(target=run_server)
        server_thread.start()
                
    def _spawnThread(self):
        self.running = True
        self.resume_flag.set()
        self.payload_flag.clear()
        thread = threading.Thread(target=self._recordLoop, args=())
        thread.start()

    def _recordLoop(self):
        while self.running:
            self.resume_flag.wait()
            temp_file = tempfile.NamedTemporaryFile(suffix=".wav")
            if self.record_on_detect(temp_file.name, silence_threshold=self.silence_threshold):
                continue

            import shutil
            shutil.copy(temp_file.name, "/home/marius/etc/diagnostic.wav")
            self.buffer.append(getWhisperTranscription(temp_file.name, self.model))
            if self.callback:
                self.callback(self.buffer[-1])
            self.payload_flag.set()

    def pause(self):
        self.resume_flag.clear()

    def isPaused(self):
        return not(self.resume_flag.is_set())

    def resume(self):
        self.resume_flag.set()

    def stop(self):
        self.running = False
        self.resume()
        if self.websock and self.websock_server is not None:
            self.websock_server.shutdown()            

    def pop(self):
        """Returns a list of strings that were recorded and transcribed since the last time poll or pop was called.
        This function is non-blocking."""
        tmp = self.buffer
        self.buffer = [] #FIXME: race condition?
        return tmp

    def poll(self):
        """Returns a list of strings that were recorded and transcribed since the last time poll or pop was called.
This function will block until new input is recorded."""
        self.payload_flag.wait()
        self.payload_flag.clear()
        return self.pop()


    def _set_samplerate(self, samplerate):
        self._samplerate = samplerate

    def get_samplerate(self):
        return self._samplerate
        

    def record_on_detect(self, file_name, silence_limit=1, silence_threshold=2500, chunk=1024, prev_audio=1):
        """Records audio from the microphone or WebSocket and saves it to a file.
        Returns False on error or if stopped.
        Silence limit in seconds. The max amount of seconds where
        only silence is recorded. When this time passes the
        recording finishes and the file is delivered.

        The silence threshold intensity that defines silence
        and noise signal (an int. lower than THRESHOLD is silence).
        Previous audio (in seconds) to prepend. When noise
        is detected, how much of previously recorded audio is
        prepended. This helps to prevent chopping the beginning
        of the phrase."""

        rate = self.get_samplerate()
        # FIXME: this is necessary until I find out how to send stereo audio from javascript, otherwise we get chipmunk sound
        CHANNELS = 2 if not(self.websock) else 1
        FORMAT = pyaudio.paInt16
        with ignoreStderr():
            p = pyaudio.PyAudio()
        stream = None
        if not self.websock:
            stream = p.open(format=p.get_format_from_width(2),
                            channels=CHANNELS,
                            rate=rate,
                            input=True,
                            output=False,
                            frames_per_buffer=chunk)
        listen = True
        started = False
        rel = rate / chunk
        frames = []
        prev_audio = deque(maxlen=int(prev_audio * rel))
        slid_window = deque(maxlen=int(silence_limit * rel))
        while listen:
            if not(self.running) or self.isPaused():
                return True

            data = None
            if self.websock:
                data = self.audio_buffer.get()
                #if len(self.audio_buffer) >= chunk:
                    #data = np.frombuffer(self.audio_buffer[:chunk], dtype=np.int16)
                    #self.audio_buffer = self.audio_buffer[chunk:]
            else:
                data = stream.read(chunk)

            if data is not None:
                slid_window.append(math.sqrt(abs(audioop.avg(data, 4))))

            if sum([x > silence_threshold for x in slid_window]) > 0:
                if not started:
                    # recording starts here
                    started = True
                    if self.on_threshold is not None:
                        self.on_threshold()
            elif started:
                started = False
                listen = False
                prev_audio = deque(maxlen=int(0.5 * rel))

            if started and data is not None:
                frames.append(data)
            elif data is not None:
                prev_audio.append(data)

        if not self.websock:
            stream.stop_stream()
            stream.close()
            p.terminate()

        wf = wave.open(file_name, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(pyaudio.get_sample_size(FORMAT))
        wf.setframerate(rate)
        wf.writeframes(b''.join(list(prev_audio)))
        wf.writeframes(b''.join(frames))
        wf.close()
        return False


    #debug notes
    # https://community.openai.com/t/playing-audio-in-js-sent-from-realtime-api/970917/8
    

```

## tts_backends_orpheus.py

```python
from typing import *
import traceback
import subprocess
import os
from glob import glob
import threading
import sys
import requests
import json
import time
import wave
import asyncio
import numpy as np
import torch
import queue
from huggingface_hub import snapshot_download
from ghostbox.tts_backends import TTSBackend


class OrpheusBackend(TTSBackend):
    """Backend for the orpheus tts model.
        https://huggingface.co/canopylabs/orpheus-3b-0.1-pretrained
    https://github.com/canopyai/Orpheus-TTS

    Much of the code for this was taken from:
    https://github.com/isaiahbjork/orpheus-tts-local
    much respect.

        This implementation has been tested with llama.cpp as provider for the underlying llm. Any server that offers an OpenAI compatible endpoint should in principle work.
    """

    def __init__(self, **kwargs):
        """Creates a Orpheus TTS backend."""
        # some defaults for sampling parameters (meant for llamacpp through ghostbox)
        # you can experiment, but at least these are known to work
        default_config = {
            "temperature": 0.6,
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "max_length": 1024,
            # this is factory default
            # "samplers": ["penalties", "top_p", "temperature"],
            # this is what we do, which is roughly 3x faster
            "samplers":["penalties", "min_p","temperature"],
            # orpheus specific stuff
            "available_voices": [
                "tara",
                "leah",
                "jess",
                "leo",
                "dan",
                "mia",
                "zac",
                "zoe",
            ],
            "special_tags": [
                "<laugh>",
                "<chuckle>",
                "<sigh>",
                "<cough>",
                "<sniffle>",
                "<groan>",
                "<yawn>",
                "<gasp>",
            ],
            "voice": "",  # we set tara as default below
            "volume": 1.0, # user can set this to change generated volume. we also boost the default volume by a bit
            "start_token_id": 128259,
            "end_token_ids": [128009, 128260, 128261, 128257],
            "custom_token_prefix": "<custom_token_",
            # this is the audio decoder
            "snac_model": "hubertsiuzdak/snac_24khz",
            "sample_rate": 24000,
        }

        if "config" in kwargs:
            kwargs["config"] = default_config | kwargs["config"]

        super().__init__(**kwargs)

        # we want to allow users to use undeifined voices
        # but we provide tara as default if they don't specify at all
        if self.config["voice"] == "":
            self.config["voice"] = "tara"


        # we boost the volume of all orpheus voices
        # as I find them too quiet
        # but tara is especially egregious
        if self.config["voice"] == "tara":
            self.volume_boost = 1.5
        else:
            # FIXME: keep experimenting, the other voices might not need a boost
            self.volume_boost = 1.25
                
            
        from ghostbox.util import printerr
        self._print_func = printerr
        # this is used for time stats when streaming, the default value below will be overriden
        self._start_time = time.time()
        
        self._init()

    def _print(self, w):
        self._print_func(w)

    def _init(self):
        self._print("Initializing orpheus TTS backend.")
        from snac import SNAC
        import ghostbox

        # SNAC model uses 24kHz

        self.model = SNAC.from_pretrained(self.config["snac_model"]).eval()
        # Check if CUDA is available and set device accordingly
        self.snac_device = (
            "cuda"
            if torch.cuda.is_available()
            else "mps" if torch.backends.mps.is_available() else "cpu"
        )
        self._print(f"Using device: {self.snac_device}")
        self.model = self.model.to(self.snac_device)

        # figure out the llm server endpoint
        # if the user set it, it's easy
        if (llm_server := self.config["llm_server"]) != "":
            endpoint = llm_server
        else:
            self._print("Spawning LLM server...")
            self._spawn_llm_server()
            endpoint = "http://localhost:8181"

        self.box = ghostbox.from_openai_legacy(
            character_folder="",
            endpoint=endpoint,
            stdout=False,
            temperature=self.config["temperature"],
            top_p=self.config["top_p"],
            max_legnth=self.config["max_length"],
            samplers=self.config["samplers"],
            # samplers=["min_p","temperature", "xtc"],
            repeat_penalty=self.config["repeat_penalty"],
            prompt_format="raw",
            # for debug
            verbose=False,
        )

    def _spawn_llm_server(self) -> None:
        model_name = self._get_orpheus_model_name()
        executable = self.config["llm_server_executable"]
        max_context_length = self.config["max_length"]
        quant = (self.config["orpheus_quantization"][:2] + "_0").lower()
        port = 8181

        # load all of it or gtfo
        layers = 999

        # obviously this is specific to llamacpp
        command = f"{executable} --port {port} -c {max_context_length} -ngl {layers} -fa -ctk {quant} -ctv {quant} --mlock"
        cmd_list =             command.split(" ")
        # extend so we properly escape the model_name
        cmd_list.extend(["-m", model_name])
        self._print(f"Executing `{command}`.")
        self._server_process = subprocess.Popen(
            cmd_list,
            text=True,

            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            stdin=subprocess.PIPE,
        )

    def _get_orpheus_model_name(self) -> str:
        """Returns filepath to an orpheus model."""
        self._print(f"Considering orpheus model: {self.config["orpheus_model"]}")
        if (model := self.config["orpheus_model"]) != "":
            if os.path.isfile(model):
                return model
        else:
            # see the quant and just use some defaults
            quant = self.config["orpheus_quantization"]
            if quant == "Q4_K_M":
                model = "isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF"
            elif quant == "Q8_0":
                model = "lex-au/Orpheus-3b-FT-Q8_0.gguf"
            else:
                raise RuntimeError(
                    f"fatal error: Sorry, we don't support that quantization level yet ({quant})."
                )

        # assume model is a huggingface repo
        repo_dir = snapshot_download(model)
        try:
            return glob(os.path.join(repo_dir, "*.gguf"))[0]
        except:
            raise RuntimeError(f"fatal error: Could not find gguf file in directory '{repo_dir}' after downloading snapshot for '{model}'.")
            
        
    def get_config(self) -> Dict[str, Any]:
        return self.config

    def get_voices(self) -> List[str]:
        return self.config["available_voices"]

    def configure(self, **kwargs) -> None:
        # FIXME: too lazy to put the argument list here atm
        self.config |= kwargs

    def split_into_sentences(self, text: str) -> List[str]:
        # need to experiment what works best with orpheus
        # return super().split_into_sentences(text)
        return (text.strip()).split("\n")


    

    def can_stream(self) -> bool:
        return True

    def tts_to_generator(self, text:str) -> Iterator[bytes]:
        voice = os.path.basename(self.config["voice"])
        self._start_time = time.time()
        return self.tokens_decoder_sync(
            self.generate_tokens_from_api(
                prompt=text,
                voice=voice
            ),
        )

    
    def tts_to_file(
        self, text: str, file_path: str, language: str = "en", speaker_file: str = ""
    ) -> None:
        # speaker file is just a name in this case, and we default to tara
        if speaker_file == "":
            speaker_file = self.config["voice"]

        # FIXME: see _init
        speaker_file = os.path.basename(speaker_file)
            
        start_time = time.time()
        self.tokens_decoder_sync(
            self.generate_tokens_from_api(
                prompt=text,
                voice=speaker_file,
            ),
            output_file=file_path,
        )

        end_time = time.time()
        self._print(
            f"Speech generation completed in {end_time - start_time:.2f} seconds"
        )

    def format_prompt(self, prompt: str, voice: str) -> str:
        """Format prompt for Orpheus model with voice prefix and special tokens."""
        if voice not in self.config["available_voices"]:
            self._print(
                f"warning: Voice '{voice}' not recognized. Hope you know what you're doing!"
            )

        # Format similar to how engine_class.py does it with special tokens
        formatted_prompt = f"{voice}: {prompt}"

        # Add special token markers for the LM Studio API
        special_start = "<|audio|>"  # Using the additional_special_token from config
        special_end = "<|eot_id|>"  # Using the eos_token from config
        return f"{special_start}{formatted_prompt}{special_end}"

    def generate_tokens_from_api(
        self,
        prompt: str,
        voice: str,
    ):
        """Generate tokens using the API of a llama.cpp llama-server."""
        formatted_prompt = self.format_prompt(prompt, voice)

        token_counter = 0
        q = queue.Queue()
        done = threading.Event()
        done.clear()
        generation = ""

        def process_token(w):
            nonlocal token_counter
            q.put(w.strip())
            token_counter += 1

        def gen(w):
            nonlocal done
            nonlocal generation
            generation = w
            done.set()

        self.box.clear_history()
        with self.box.options(
            **{
                k: v
                for k, v in self.config.items()
                if k
                in "temperature top_p min_p repeat_penalty samplers max_length".split(
                    " "
                )
            }
        ):
            self.box.text_stream(
                formatted_prompt,
                chunk_callback=process_token,
                generation_callback=gen,
            )

        # have to do it this way because the original code used generators
        # and we are lazy


        self._print(f"`{prompt}`")
        while not (done.is_set()):
            try:
                token = q.get(timeout=1)
                yield token
            except queue.Empty:
                # without this threads don't react to signals which is annoying
                continue


        # debug
        # the ghostbox object will print timing information as long as box.timings == True
        #self._print(f"Token generation complete with {token_counter} tokens.")        
        #timings = self.box._plumbing.getBackend().timings()
        #self._print(f"tokens generated: {timings.predicted_n}\nt/s: {timings.predicted_per_second}")
        #self._print(f"timings: {json.dumps(timings.model_dump(), indent=4)}")
        # #self._print(generation)

    ### decoder methods below ###
    async def tokens_decoder(self, token_gen):
        """Asynchronous token decoder that converts token stream to audio stream."""
        buffer = []
        count = 0
        async for token_text in token_gen:
            token = self.turn_token_into_id(token_text, count)
            if token is not None and token > 0:
                buffer.append(token)
                count += 1

                # Convert to audio when we have enough tokens
                if count % 7 == 0 and count > 27:
                    buffer_to_proc = buffer[-28:]
                    audio_samples = self.convert_to_audio(buffer_to_proc, count)
                    if audio_samples is not None:
                        yield audio_samples

    def tokens_decoder_sync(self, syn_token_gen, output_file: Optional[str]=None):
        """Synchronous wrapper for the asynchronous token decoder."""
        audio_queue = queue.Queue()
        audio_segments = []

        # If output_file is provided, prepare WAV file
        wav_file = None
        if output_file:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
            wav_file = wave.open(output_file, "wb")
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(self.config["sample_rate"])

        # Convert the synchronous token generator into an async generator
        async def async_token_gen():
            for token in syn_token_gen:
                yield token

        async def async_producer():
            async for audio_chunk in self.tokens_decoder(async_token_gen()):
                audio_queue.put(audio_chunk)
            audio_queue.put(None)  # Sentinel to indicate completion

        def run_async():
            asyncio.run(async_producer())

        # Start the async producer in a separate thread
        thread = threading.Thread(target=run_async, daemon=True)
        thread.start()

        # Process audio as it becomes available
        while True:
            try:
                audio = audio_queue.get(timeout=1)
                if audio is None:
                    break
            except queue.Empty:
                continue

            audio_segments.append(audio)

            # Write to WAV file if provided
            if wav_file:
                wav_file.writeframes(audio)
            else:
                yield audio

        # Close WAV file if opened
        if wav_file:
            wav_file.close()

        thread.join()

        # Calculate and print duration
        duration = (
            sum([len(segment) // (2 * 1) for segment in audio_segments])
            / self.config["sample_rate"]
        )
        self._print(f"Generated {len(audio_segments)} audio segments")
        self._print(f"Generated {duration:.2f} seconds of audio")
        if not(wav_file):
            # print the stats here
            end_time = time.time()
            self._print(
                f"Speech generation completed in {end_time - self._start_time:.2f} seconds"
            )
        
            


    def convert_to_audio(self, multiframe, count):
        """Convert token frames to audio."""
        frames = []
        if len(multiframe) < 7:
            return

        codes_0 = torch.tensor([], device=self.snac_device, dtype=torch.int32)
        codes_1 = torch.tensor([], device=self.snac_device, dtype=torch.int32)
        codes_2 = torch.tensor([], device=self.snac_device, dtype=torch.int32)

        num_frames = len(multiframe) // 7
        frame = multiframe[: num_frames * 7]

        for j in range(num_frames):
            i = 7 * j
            if codes_0.shape[0] == 0:
                codes_0 = torch.tensor(
                    [frame[i]], device=self.snac_device, dtype=torch.int32
                )
            else:
                codes_0 = torch.cat(
                    [
                        codes_0,
                        torch.tensor(
                            [frame[i]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )

            if codes_1.shape[0] == 0:
                codes_1 = torch.tensor(
                    [frame[i + 1]], device=self.snac_device, dtype=torch.int32
                )
                codes_1 = torch.cat(
                    [
                        codes_1,
                        torch.tensor(
                            [frame[i + 4]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
            else:
                codes_1 = torch.cat(
                    [
                        codes_1,
                        torch.tensor(
                            [frame[i + 1]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
                codes_1 = torch.cat(
                    [
                        codes_1,
                        torch.tensor(
                            [frame[i + 4]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )

            if codes_2.shape[0] == 0:
                codes_2 = torch.tensor(
                    [frame[i + 2]], device=self.snac_device, dtype=torch.int32
                )
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 3]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 5]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 6]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
            else:
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 2]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 3]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 5]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )
                codes_2 = torch.cat(
                    [
                        codes_2,
                        torch.tensor(
                            [frame[i + 6]], device=self.snac_device, dtype=torch.int32
                        ),
                    ]
                )

        codes = [codes_0.unsqueeze(0), codes_1.unsqueeze(0), codes_2.unsqueeze(0)]
        # check that all tokens are between 0 and 4096 otherwise return *
        if (
            torch.any(codes[0] < 0)
            or torch.any(codes[0] > 4096)
            or torch.any(codes[1] < 0)
            or torch.any(codes[1] > 4096)
            or torch.any(codes[2] < 0)
            or torch.any(codes[2] > 4096)
        ):
            return

        with torch.inference_mode():
            audio_hat = self.model.decode(codes)

        audio_slice = audio_hat[:, :, 2048:4096]
        detached_audio = audio_slice.detach().cpu()
        audio_np = detached_audio.numpy()
        # boost the volume as I find the default rather quiet
        # and also apply user volume
        audio_np = audio_np * self.volume_boost * self.config["volume"]
        audio_int16 = (audio_np * 32767).astype(np.int16)
        audio_bytes = audio_int16.tobytes()
        return audio_bytes

    def turn_token_into_id(self, token_string, index):
        """Convert token string to numeric ID for audio processing."""
        # Strip whitespace
        token_string = token_string.strip()

        # Find the last token in the string
        last_token_start = token_string.rfind(self.config["custom_token_prefix"])

        if last_token_start == -1:
            return None

        # Extract the last token
        last_token = token_string[last_token_start:]

        # Process the last token
        if last_token.startswith(
            self.config["custom_token_prefix"]
        ) and last_token.endswith(">"):
            try:
                number_str = last_token[14:-1]
                token_id = int(number_str) - 10 - ((index % 7) * 4096)
                return token_id
            except ValueError:
                return None
        else:
            return None

```

## tts_backends.py

```python
import traceback, appdirs, os, wget
from abc import ABC, abstractmethod
from functools import *
from typing import *
from ghostbox.util import *
from ghostbox.definitions import *


class IgnoreValueError(ValueError):
    def __init__(self, ignored_value, **kwargs):
        super().__init__(**kwargs)
        self.ignored_value = ignored_value


def assert_downloaded(filepath: str, download_url: str) -> None:
    """Makes sure a file FILEPATH exists, downloading it from DOWNLOAD_URL if necessary."""
    if os.path.isfile(filepath):
        return
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    printerr("Downloading " + os.path.basename(filepath) + " from " + download_url)
    wget.download(download_url, out=filepath)
    printerr("\nSuccessfully saved to " + filepath)


import nltk.data

nltk.download("punkt_tab")
tokenizer = nltk.data.load("tokenizers/punkt/english.pickle")


class TTSBackend(ABC):
    """Abstract interface to a TTS model, like xtts2 (now derelict), tortoise, zonos, and many others."""

    @abstractmethod
    def __init__(self, config: Dict[str, Any] = {}) -> None:
        self.config = config

    @abstractmethod
    def tts_to_file(
            self, text: str, file_path: str
    ) -> None:
        """Given a message, writes the message spoken as audio to a wav file."""
        pass

    def can_stream(self) -> bool:
        return False

    def tts_to_generator(self, text:str) -> Iterator[bytes]:
        printerr("error: Streaming not supported by this backend.")
    
    @abstractmethod
    def split_into_sentences(self, text: str) -> List[str]:
        """Returns a list of sentences, where a 'sentence' is any string the TTS backend wants to process as a chunk.
        The default implementation splits on common punctuation marks."""
        return tokenizer.tokenize(text)

    @abstractmethod
    def configure(self, **kwargs) -> None:
        """Set parameters specific to a TTS model."""
        pass

    def clone_path(self) -> str:
        """Returns the full path to a voice to be cloned. If the model is not configued to clone a voice, returns empty string."""
        if (clone := self.config.get("clone", None)) is None:
            return ""

        if (clone_dir := self.config.get("clone_dir", None)) is None:
            # try the current directory
            clone_dir = "."
            
        return os.path.join(clone_dir, clone)
    
    
    @abstractmethod
    def get_voices(self) -> List[str]:
        """Returns a list of all voices supported by the model.
        This may be empty or inexhaustive for some models, e.g. if files need to be provided for cloning.
        """
        pass

    @abstractmethod
    def get_config(self) -> Dict[str, Any]:
        pass


def dump_config(backend: TTSBackend) -> List[str]:
    if "config" not in backend.__dict__:
        return []
    return ["    " + key + "\t" + str(value) for key, value in backend.config.items()]


class XTTSBackend(TTSBackend):
    """Bindings for the xtts2 model, which is currently (2025) in license limbo and should not be used for production purposes.
    This immplementation remains here as a reference implementation."""

    def __init__(self, config: Dict[str, Any] = {}):
        super().__init__()
        # fail importing these early
        import torch
        from TTS.api import TTS

        device = "cuda" if torch.cuda.is_available() else "cpu"
        printerr("Using " + device)
        self.tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)

    def tts_to_file(
            self, text: str, file_path: str
    ) -> None:
        printerr("`" + text + "`")
        self.tts.tts_to_file(
            text=text, speaker_wav=self.clone_path(), language=self.config.get("language", "en"), file_path=file_path
        )

    def split_into_sentences(self, text: str) -> List[str]:
        return self.tts.synthesizer.split_into_sentences(msg)

    def configure(self, **kwargs) -> None:
        super().configure(**kwargs)

    def get_voices(self) -> List[str]:
        return []

    def get_config(self):
        return {}


class ZonosBackend(TTSBackend):
    """Bindings for the zonos v0.1 model. See https://github.com/Zyphra/Zonos"""

    def __init__(self, config: Dict[str, Any] = {}, **kwargs):
        super().__init__(config=config, **kwargs)
        self._speakers = {}
        # default config
        self.config |= {
            # we get this from command args
            #"zonos_model": "Zyphra/Zonos-v0.1-transformer",
            #"seed": 420,            
            "pitch_std": 200.0,
        }
        self.config |= self.get_default_emotions()
        self._model_fallback = self.config["zonos_model"]
        self.configure(**config)
        self._init()

    def _init(self) -> None:
        printerr("Initializing zonos TTS model " + self.config["zonos_model"] + ".")
        # fail importing these early
        import torch
        import torchaudio
        from zonos.model import Zonos
        from zonos.conditioning import make_cond_dict
        from zonos.utils import DEFAULT_DEVICE as device

        printerr("Using " + str(device))
        # self._model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-hybrid", device=device)
        # self._model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device=device)

        try:
            self._model = Zonos.from_pretrained(
                self.config["zonos_model"], device=device
            )
        except:
            if self.config["zonos_model"] != self._model_fallback:
                printerr(
                    "warning: Couldn't load model. Retrying with fallback. Below is the full traceback."
                )
                printerr(traceback.format_exc())
                self.configure(zonos_model=self._model_fallback)
                self._init()
                return
            printerr(
                "error: Couldn't load model. Panicing, as there is no more fallback. Below is the full traceback. Goodbye."
            )
            printerr(traceback.format_exc())

    def configure(self, **kwargs) -> None:
        for key, value in kwargs.items():
            # we only configure options that are in the default config
            if key not in self.config:
                continue

            # some special cases
            if key == "zonos_model":
                if value == "transformer":
                    self.config["zonos_model"] = "Zyphra/Zonos-v0.1-transformer"
                elif value == "hybrid":
                    self.config["zonos_model"] = "Zyphra/Zonos-v0.1-hybrid"
                else:
                    self.config["zonos_model"] = value
            else:
                self.config[key] = value

    def tts_to_file(
        self, text: str, file_path: str
    ) -> None:
        language = self.config["language"]
        if language == "":
            language = "en-us"
            
        clone = self.clone_path()
        if clone == "":
            raise RuntimeError("fatal error: Zonos model currently requires you to specify a voice to clone.")
        
        import torch
        import torchaudio
        from zonos.conditioning import make_cond_dict

        if text == "":
            return

        printerr("`" + text + "`")

        # we want to support the 'en' code because xtts uses it
        if language == "en":
            language = "en-us"

        if clone not in self._speakers:
            self._create_speaker(clone)
        speaker = self._speakers[clone]

        torch.manual_seed(self.config["seed"])
        cond_dict = make_cond_dict(
            text=text,
            speaker=speaker,
            language=language,
            emotion=list(self.get_config_emotions().values()),
            pitch_std=self.config["pitch_std"],
        )
        conditioning = self._model.prepare_conditioning(cond_dict)
        codes = self._model.generate(conditioning)
        wavs = self._model.autoencoder.decode(codes).cpu()
        torchaudio.save(file_path, wavs[0], self._model.autoencoder.sampling_rate)

    def split_into_sentences(self, text: str) -> List[str]:
        ws = super().split_into_sentences(text)
        # debug
        # print(str(ws))
        return ws

    def get_default_emotions(self) -> Dict[str, float]:
        names = "happiness sadness disgust fear surprise anger other neutral".split(" ")
        emotions = [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077]
        return dict(list(zip(names, emotions)))
        return

    def get_config_emotions(self) -> Dict[str, float]:
        names = self.get_default_emotions().keys()
        return {name: self.config[name] for name in names}

    def _create_speaker(self, speaker_file):
        import torchaudio

        wav, sampling_rate = torchaudio.load(speaker_file)
        self._speakers[speaker_file] = self._model.make_speaker_embedding(
            wav, sampling_rate
        )

    def get_voices(self) -> List[str]:
        return []

    def get_config(self):
        return self.config


class KokoroBackend(TTSBackend):
    """Bindings for the indomitable kokoro tts https://github.com/hexgrad/kokoro ."""

    def __init__(self, config: Dict[str, Any] = {}, **kwargs) -> None:
        super().__init__(config=config, **kwargs)
        # set some defaults
        if self.config["voice"] == "":
            self.config["voice"] = "af_sky"
            
        
        self._default_onnx_file = "kokoro-v1.0.onnx"
        self._default_voice_file = "voices-v1.0.bin"
        self._init()

    def _init(self) -> None:
        printerr("Initializing kokoro.")
        # import even if unused just to fail early
        import soundfile as sf
        from kokoro_onnx import Kokoro

        # FIXME: ok there is a problem upstream with kokoro and gpu support. problem is it tends to default to CPU
        # we need to make sure that user did
        # pip install kokoro_onnx[gpu]
        # which is annoying, additionally, we require
        # export ONNX_PROVIDER=CUDAExecutionProvider
        # then it will run with cuda, but apparently the latest cuda libs aren't supported. wip.
        # update: it actually works with
        # pacman -S cudnn
        # still, it's all a bit hairy.

        self._model = Kokoro(self._get_onnx_path(), self._get_voice_path())

    def _data_dir(self) -> str:
        return appdirs.AppDirs("ghostbox-tts").user_data_dir

    def _get_onnx_path(self) -> str:
        onnx_path = self._data_dir() + "/" + self._default_onnx_file
        # FIXME: maybe host these yourself
        assert_downloaded(
            onnx_path,
            "https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx",
        )
        return onnx_path

    def _get_voice_path(self) -> str:
        voice_file = (
            appdirs.AppDirs("ghostbox-tts").user_data_dir
            + "/"
            + self._default_voice_file
        )
        assert_downloaded(
            voice_file,
            "https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin",
        )
        return voice_file

    def tts_to_file(
        self, text: str, file_path: str
    ) -> None:
        """Given a message, writes the message spoken as audio to a wav file."""
        import soundfile as sf
        language = self.config.get("language", "en-us")
        voice = self.config.get("voice", "af_sky")
        
        if text == "":
            return

        printerr("`" + text + "`")
        if language == "en":
            language = "en-us"

        try:
            samples, sample_rate = self._model.create(
                text, voice=voice, speed=1.0, lang=language
            )
            sf.write(file_path, samples, sample_rate)
        except ValueError as e:
            # this happens when kokoro doesn't like a string, e.g. "---".
            # rather than filtering all of these, we just throw and continue in the main loop
            raise IgnoreValueError(text)
        except:
            # this happens e.g. when a wrong voice is picked. we exit to avoid infinite loop with the main thread retries.
            printerr(traceback.format_exc())
            sys.exit()

    def split_into_sentences(self, text: str) -> List[str]:
        """Returns a list of sentences, where a 'sentence' is any string the TTS backend wants to process as a chunk.
        The default implementation splits on common punctuation marks."""
        # kokoro can deal with long sentences without degrading output.
        # but it can end up reserving too much vram.
        # I'm not certain of this but I also think that it does a better job the more material it has.
        # so we compromise
        if len(text) > 300:
            return super().split_into_sentences(text)
        return [text.strip()]

    def configure(self, **kwargs) -> None:
        """Set parameters specific to a TTS model."""
        pass

    def get_config(self):
        return {}

    def get_voices(self) -> List[str]:
        return self._model.get_voices()

```

## tts_output.py

```python
import traceback, threading, wave, time
import numpy
from abc import ABC, abstractmethod
from functools import *
from typing import *
from queue import Queue, Empty
from ghostbox.util import *
from ghostbox.definitions import *


class TTSOutput(ABC):
    """Manages output of TTS sound."""

    @abstractmethod
    def __init__(self, volume: float= 1.0, **kwargs):
        self.volume = volume

    @abstractmethod
    def enqueue(self, payload: str | Iterator[bytes]) -> None:
        """Enqeueu a wave file or generator for playback. Start playback immediately if nothing is playing.
        This function is non-blocking.
        :param payload: Either a string denoting the filename of a wave file, or a generator yielding wave audio data."""
        pass


    def set_volume(factor: float = 1.0) -> None:
        """Set the volume for the output module.
        :param factor: A multiplier that will be applied to the base volume. 1.0 is no change to the base."""
        self.volume = factor
        
    
    def stop(self) -> None:
        """Instantly interrups and stops any ongoing playback. This method is thread safe."""
        pass

    @abstractmethod
    def is_speaking(self) -> bool:
        """Returns true if the output module is currently playing sound."""
        pass

    @abstractmethod
    def shutdown(self) -> None:
        """Gracefully shut down output module, finishing playback of all enqueued files.
        Calling any of the methods after this one is undefined behaviour."""
        pass


class DefaultTTSOutput(TTSOutput):
    """Local TTS sound output using pyaudio."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # fail early if pyaudio isn't available
        import pyaudio

        printerr("Using pyaudio for local playback.")
        self.stop_flag = threading.Event()
        self._queue = Queue()
        self.pyaudio = pyaudio.PyAudio()
        self.running = True
        ### streaming stuff
        # used for streaming only, playing files opens its own pyaudio stream using the wav files parameters

        # we use yet another queue and buffer
        # that will contain the actual raw audio for streaming mode
        self._audio_queue = Queue()
        
        # this is the actual stream object we will write to
        # init_stream will start a worker
        # and set self._stream
        self._init_stream()
        
        def play_worker():
            while self.running or not (self._queue.empty()):
                # don't remove the loop or timeout or else thread will not be terminated through signals
                try:
                    payload = self._queue.get(timeout=1)
                except Empty:
                    continue

                # _play will block until stop is called or playback finishes
                self.stop_flag.clear()                
                self._play(payload)

        self.worker = threading.Thread(target=play_worker)
        self.worker.start()


    def _init_stream(self):
        """Initializes a stream for streaming playback."""
        chunk = 512
        # FIXME: hardcoded some stuff as we are trying out streaming with orpheus first
        p = self.pyaudio
        self._stream = p.open(
            format=p.get_format_from_width(2),
            channels=1,
            rate=24000,
            output=True,
            frames_per_buffer=chunk,
        )
        self._stream.start_stream()
        # so atm we sometimes get buffer underruns
        # this is on a rtx 3090 with let's say 150is t/s
        # it tends to happen at the start of generation, so this is an experimental fix for that particular situation
        # we don't start streaming until we have n number of audio chunks prebuffered
        n_prebuffer = 3
        
        def stream_worker():
            skip_prebuffer = False
            while self.running:
                if len(self._audio_queue.queue) < n_prebuffer and not(skip_prebuffer):
                    time.sleep(0.1)
                    continue
                else:
                    skip_prebuffer = True                    
                    
                try:
                    chunk = self._audio_queue.get(timeout=1)
                except Empty:
                    skip_prebuffer = False
                    continue


                # FIXME: this doesn't work
                #if self.volume != 1.0:
                    #chunk_np = numpy.fromstring(chunk, numpy.int16) * self.volume
                    #chunk = chunk_np.astype(numpy.int16)

                self._stream.write(chunk)

        self._stream_thread = threading.Thread(target=stream_worker, daemon=True)
        self._stream_thread.start()

    def _stream_write(self, audio_chunk) -> None:
        """Small helper to play audio chunks in streaming mode."""
        self._audio_queue.put(audio_chunk)
        
    def enqueue(self, payload) -> None:
        self._queue.put(payload)

    def _play(self, payload: str | Iterator[bytes], **kwargs) -> None:
        if type(payload) == str:
            self._play_file(payload, **kwargs)
        else:
            self._play_stream(payload, **kwargs)
            
    def _play_file(self, filename: str) -> None:
        import pyaudio
        wf = wave.open(filename, "rb")
        chunk = 1024
        p = self.pyaudio
        stream = p.open(
            format=p.get_format_from_width(wf.getsampwidth()),
            channels=wf.getnchannels(),
            rate=wf.getframerate(),
            output=True,
            frames_per_buffer=chunk,
        )

        # Play the audio data
        while data := wf.readframes(chunk):
            if self.stop_flag.isSet():
                break

            stream.write(data)

        stream.stop_stream()
        stream.close()
        self.stop_flag.set()



    def _play_stream(self, audio_stream: Iterator[bytes]) -> None:
        # Play the audio data
        for data in audio_stream:
            if self.stop_flag.isSet():
                break
            self._stream_write(data)

        #self._stream.stop_stream()
        self.stop_flag.set()
        
    def stop(self) -> None:
        """Instantly interrupts and stops all playback."""
        self.stop_flag.set()
        self._queue.queue.clear()
        self._audio_queue.queue.clear()        

    def is_speaking(self) -> bool:
        #print(f"{self._queue.empty()=}")
        #print(f"{self.stop_flag.is_set()=}")
        return not(self._queue.empty()) or not(self._audio_queue.empty()) or not(self.stop_flag.is_set())

    def shutdown(self) -> None:
        """Gracefully shut down output module, finishing playback of all enqueued files."""
        self.running = False
        super().shutdown()


class WebsockTTSOutput(TTSOutput):
    def __init__(self, host="localhost", port=5052, **kwargs):
        # fail early
        import websockets.sync.server

        super().__init__(**kwargs)
        self.clients = []
        self.stop_flag = threading.Event()
        self.go_flag = threading.Event()
        self.go_flag.set()
        self.server_running = threading.Event()
        self._queue = Queue()
        self.host = host
        self.port = port

        # start websock server
        self.server_thread = threading.Thread(target=self._run_server)
        self.server_thread.daemon = True
        self.server_thread.start()
        printerr("WebSocket TTS output initialized.")

        def play_worker():
            while self.server_running.isSet() or not (self._queue.empty()):
                # don't remove the loop or timeout or else thread will not be terminated through signals
                try:
                    filename = self._queue.get(timeout=1)
                except Empty:
                    continue

                # _play will block until stop is called or playback finishes
                self._play(filename)

        # start queue worker
        self.worker_thread = threading.Thread(target=play_worker)
        # deliberately not a daemon so we deal with EOF properly
        self.worker_thread.start()

    def _run_server(self):
        import websockets
        import websockets.sync.server as WS

        def handler(websocket):
            remote_address = websocket.remote_address
            printerr("[WEBSOCK] Got connection from " + str(remote_address))
            self.clients.append(websocket)
            try:
                while self.server_running.isSet():
                    msg = websocket.recv()
                    # print(msg)
                    if msg == "done":
                        # current sound has finished playing
                        self.go_flag.set()

            except websockets.exceptions.ConnectionClosed:
                pass
            finally:
                self.clients.remove(websocket)
                # FIXME: this doesn't work with multiple clients. see play()
                self.go_flag.set()
                printerr("[WEBSOCK] Closed connection with " + str(remote_address))

        self.server_running.set()
        self.server = WS.serve(handler, self.host, self.port)
        printerr("WebSocket server running on ws://" + self.host + ":" + str(self.port))
        self.server.serve_forever()

    def stop_server(self) -> None:
        self.server_running.clear()
        printerr("Halting websocket server.")

    def enqueue(self, filename: str) -> None:
        """Send a wave file   over the network, or enqueue it to be sent if busy.
        This method is non-blocking."""
        self._queue.put(filename)

    def _play(self, filename: str) -> None:
        """Sends a wave file ofer the network to all connected sockets."""
        from websockets import ConnectionClosedError

        printerr("[WEBSOCK] Playing with " + str(len(self.clients)) + " clients.")

        with open(filename, "rb") as wf:
            # Read the entire file content
            # data = wf.readframes(wf.getnframes())
            data = wf.read()

            for client in self.clients:
                printerr(
                    "[WEBSOCK] Playing audio to "
                    + str(client.remote_address)
                    + " with file "
                    + filename
                )
                try:
                    client.send(data, text=False)
                except ConnectionClosedError:
                    print(
                        "[WEBSOCK] error: Unable to send data to "
                        + str(client.remote_address)
                        + ": Connection closed."
                    )
        # we block now, just as if we were playing the sound waiting for it to finish
        self.go_flag.clear()
        while True:
            # FIXME: this doesn't work 100% correctly with multiple clients (it might be ok though), but that is not the intended use case
            if self.go_flag.isSet():
                break
            time.sleep(0.1)

    def stop(self) -> None:
        """Instantly stop playback and clears the queue."""
        self._queue.queue.clear()
        self.stop_flag.set()
        self.go_flag.clear()
        for client in self.clients:
            client.close()
        printerr("[WEBSOCK] TTS output stopped.")

    def is_speaking(self) -> bool:
        return self.go_flag.is_set()
        
    def shutdown(self) -> None:
        """Shuts down the output module gracefully, waiting for playback/sending of all enqueue files to be finished."""
        while not (self._queue.empty()):
            time.sleep(0.1)
        self.stop()
        self._stop_server()
        super().shutdown()

```

## tts.py

```python
#!/usr/bin/env python
import argparse, traceback, sys, tempfile, ast, shutil, json
from ghostbox.definitions import TTSOutputMethod, TTSModel
from ghostbox.tts_util import *
from ghostbox.tts_state import *
from ghostbox.tts_backends import *
from ghostbox.tts_backends_orpheus import OrpheusBackend
from ghostbox.tts_output import *
    
def main():
    program_name = sys.argv[0]
    parser = argparse.ArgumentParser(description= program_name + " - TTS program to consume text from stdin and speak it out/ save it as wav file.")
    #parser.add_argument("-f", '--filepath', type=str, default="", help="Filename to save accumulated spoken lines in. Output is in wav format.")
    parser.add_argument("--voices", action=argparse.BooleanOptionalAction, default=False, help="List all available voices for chosen model, then exit the program.")
    parser.add_argument("-q", "--quiet", action=argparse.BooleanOptionalAction, default=False, help="Do not play any audio.")
    parser.add_argument("-l", "--language", type=str, default="en", help="Language for the TTS output. Not all TTS models support all languages, and many don't need this option.")
    parser.add_argument("-p", "--pause_duration", type=int, default=1, help="Duration of pauses after newlines. A value of 0 means no or minimal-duration pause.")
    parser.add_argument("-y", "--voice", type=str, default="", help="Name of the voice to use. What voices are supported differs with each model. For a list, start the program with --voices.")
    parser.add_argument("--clone", type=str, default="", help="Path to a .wav file of a voice to clone, for models that support cloning. This will usually override the --voice option.")
    parser.add_argument("--clone_dir", type=str, default="", help="Directory in which to search for wave files to clone with --clone.")
    parser.add_argument("-i", "--volume", type=float, default=1.0, help="Volume for the voice playback. Not supported by all models in all modes (streaming vs file playback).")
    parser.add_argument("-s", "--seed", type=int, default=420, help="Random seed for voice models that use it.")
    parser.add_argument("--sound_dir", type=str, default="sounds", help="Directory where sound files are located to be played with #sound <SNDNAME>")
    parser.add_argument("-m", "--model", type=str, choices=[tm.name for tm in TTSModel], default=TTSModel.zonos.name, help="Text-to-speech model to use.")
    parser.add_argument("-o", "--output-method", type=str, choices=[om.name for om in TTSOutputMethod], default=TTSOutputMethod.default.name, help="How to play the generated speech.")
    parser.add_argument("--websock-host", type=str, default="localhost", help="The hostname to bind to when using websock as output method.")
    parser.add_argument("--websock-port", type=int, default=5052, help="The port to listen on for connections when using websock as output method.")
    # zonos specific
    parser.add_argument("--zonos_model", type=str, default=ZonosTTSModel.hybrid.name, help="The pretrained checkpoint to use with the Zonos TTS engine. Hybrid seems to get the best results. This argument is ignored unless you use the Zonos model. Options: " + ", ".join([m.name for m in ZonosTTSModel]))
    parser.add_argument("--orpheus_quantization", type=str, default="Q4_K_M", help="Quantization method to use for the orpheus model. Options currently supported are 'Q4_M_K', 'Q8_0'. Using lower quants may degrade performance, but will lower vram requirements significantly. This option makes ghostbox figure out the exact model to use. If you want a quant that's not supported, set bta custom model or huggingface repo with the --orpheus_model option, which will cause this option to be ignored.")
    parser.add_argument("--orpheus_model", type=str, default="", help="The exact orpheus model to use. By default, ghostbox-tts will figure this out on its own based on the value of orpheus_quantization. Setting this option will override orpheus_quantization. You may set this option to either a filepath pointing to a model (e.g. a gguf file), or to a huggingface repo like 'https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf'.")
    parser.add_argument("--llm_server", type=str, default="", help="Hostname and port of LLM server to query for models that need it. Currently this is used only by Orpheus. Any OpenAI compatible backend can be used. If this is set, ghostbox-tts will not spawn its own server and options like orpheus_model or orpheus_quantization are ignored.")
    parser.add_argument("--llm_server_executable", type=str, default="llama-server", help="Path to an executable of a server capable of loading LLM models. This is only relevant when ghostbox-tts attempts to spawn its own llm server. Note: only tested with llama.cpp.")
    args = parser.parse_args()


    import time, threading, os

    def initTTS(model: str, config: Dict[str, Any] = {}) -> TTSBackend:
        if model == TTSModel.xtts.name:
            return XTTSBackend()
        elif model == TTSModel.zonos.name:
            return ZonosBackend(config=config)
        elif model == TTSModel.kokoro.name:
            return KokoroBackend(config=config)
        elif model == TTSModel.orpheus.name:
            return OrpheusBackend(config=config)        
        raise ValueError("Not a valid TTS model: " + model + ". Valid choices are " + "\n  ".join([tm.name for tm in TTSModel]))

    def initOutputMethod(method: str, args) -> TTSOutput:
        if method == TTSOutputMethod.default.name:
            return DefaultTTSOutput(volume=args.volume)
        elif method == TTSOutputMethod.websock.name:
            return WebsockTTSOutput(host=args.websock_host, port=args.websock_port, volume=args.volume)
        raise ValueError("Not a valid output method: " + method + ". Valid choices are " + "\n  ".join([om.name for om in TTSOutputMethod]))

    # initialization happens here
    prog = TTSState(args)
    output_module = initOutputMethod(prog.args.output_method, prog.args)
    tts = initTTS(prog.args.model, config=vars(prog.args))
    # we have to put something on the message queue that signals EOF but isn't actually EOF
    # the tokens are only for the msg_queue, and can on principle never be a user request, since we strip leading and trailing whitespace
    eof_token = "  <EOF>  "
    silence_token = "  <silence>  "
    # thisis different from e.g. <clear>, which is currently the only special string that might actullay be user input. oh well

    # list voices if requested
    if args.voices:
        for voice_name in tts.get_voices():
            print(voice_name)
        sys.exit()


    config_options = dump_config(tts)
    if config_options != []:
        printerr("Dumping TTS config options. Set them with '/<OPTION> <VALUE>'. /ls to list again.")
        for w in config_options:
            printerr(w)

    from queue import Queue, Empty
    msg_queue = Queue()
    done = threading.Event()

    def input_loop():
        nonlocal done
        nonlocal prog
        nonlocal tts
        nonlocal output_module

        while True:
            try:
                w = input()
                if w == "<clear>":
                    with msg_queue.mutex:
                        output_module.stop()                                    
                        msg_queue.queue.clear()
                    prog.clearRetries()
                    continue
                elif w.strip() == "":
                    msg_queue.put(silence_token)
                elif w == "<is_speaking>":
                    print("is_speaking: " + str(output_module.is_speaking()), flush=True)
                    continue
                elif w == "<dump_config>":
                    print("config: " + json.dumps(tts.config))
                    continue
                elif w.startswith("/"):
                    vs = w[1:].split(" ")
                    option = vs[0]
                    if option == "ls":
                        for u in dump_config(tts):
                            printerr(u)
                        continue
                    elif option in tts.get_config().keys():
                        try:
                            value = ast.literal_eval(" ".join(vs[1:]))
                        except:
                            printerr("Couldn't set config option '" + vs[0] + "'. Error in value literal?")
                            continue
                        tts.configure(**{option:value})
                        continue

                # main event -> speak input msg w
                ws = tts.split_into_sentences(w)
                for chunk in ws:
                    msg_queue.put(chunk)
            except EOFError as e:
                printerr("EOF")
                msg_queue.put(eof_token)
                break
            except:
                printerr("Exception caught while blocking. Shutting down gracefully. Below is the full exception.")
                printerr(traceback.format_exc())                    
                time.sleep(3)
                done.set()
                break


    t = threading.Thread(target=input_loop)
    t.daemon = True
    t.start()


    # this is so muggels know to type stuff when they accidentally run ghostbox-tts standalone
    printerr("Good to go. Reading messages from standard input. Hint: Type stuff and it will be spoken.")
    while True:
        # here we handle text chunks that were placed on the msg_queue
        if done.is_set():
            break

        try:
            if prog.isRetrying():
                rawmsg = prog.popRetryMSG()
            else:
                # so fun fact
                # Queue.get blocks. you knew that, ofc
                # but did you know that it super blocks? that's right - it refuses to handle any signals send to the application, including sigint and sigkill
                # so we have to sporadically use a timeout and loop around. btw all of this is undocumented.
                # Thanks, Guido!
                rawmsg = msg_queue.get(timeout=1)
                if rawmsg == eof_token:
                    done.set()
                    continue
                elif rawmsg == silence_token:
                    if not(prog.args.quiet):
                        output_module.enqueue(prog.silence_filename())
                    continue
        except Empty:
            # timeout was hit
            continue
        except: #EOFError as e:
            printerr("Exception caught while blocking. Shutting down gracefully. Below is the full exception.")
            printerr(traceback.format_exc())        
            time.sleep(3)
            os._exit(1)

        (msg, cont, err) = prog.processMsg(rawmsg)
        if err:
            printerr(err)
        if cont:
            continue


        output_file = prog.temp_wav_file()
        try:
            if tts.can_stream():
                payload = tts.tts_to_generator(msg)
            else:
                tts.tts_to_file(text=msg, file_path=output_file.name)
                payload = output_file
        except IgnoreValueError as e:
            # this happens on some bad values that are hard to filter but harmless.
            # e.g. "---" for text in kokoro
            printerr("Ignored `" + e.ignored_value + "`.")
            continue
        except ZeroDivisionError:
            printerr("Caught zero division error. Ignoring.")
            # this happens when the tts is asked to process whitespace and produces a wav file in 0 seconds :) nothing to worry about
            continue
        except AssertionError as e:
            printerr(str(e) + "\nwarning: Caught assertion error on msg: " + msg)
            prog.retry(msg)
            continue # we retry the msg that was too long

        if prog.args.quiet:
            continue


        if not(tts.can_stream()):
            # this filecopy is horrible but it is necessary because
            # even with ful program synchronization, the filesystem might not play ball
            # in any case without this, the wavefile created on this thread wouldn't show up on the other one
            newfilename = tempfile.mkstemp(suffix=".wav")[1]
            shutil.copy(output_file.name, newfilename)
            payload = newfilename

        # queue and play
        output_module.enqueue(payload)


    # this will let all enqueued files finish playing
    output_module.shutdown()
    prog.cleanup()

if __name__ == "__main__":
    main()
    

```

## tts_state.py

```python
import os, subprocess, tempfile
import ghostbox
#from moviepy.editor import *
from ghostbox.tts_util import *
from queue import Queue

class TTSState(object):
    def __init__(self, args):
        self.args = args
        if args.model == "xtts":
            self._default_samplerate = "24000"
        else:
            self._default_samplerate = "44100"
            # FIXME: accumulation temporarily disabled
            args.filepath = ""
        if args.filepath != "":
            # user wants to keep acc file
            self._keep_acc = True
        else:
            self._keep_acc = False
        self.accfile = getAccumulatorFile(args.filepath)
        self.accfile.close()
        self._empty_filename = ghostbox.get_ghostbox_data("empty." + self._default_samplerate + ".wav")
        self._silence_filename = ghostbox.get_ghostbox_data("silence.1." + self._default_samplerate + ".wav")
        subprocess.run(["cp", self._empty_filename, self.accfile.name])        
        self.tmpfile = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        self.tmpfile.close()
        self.mixins = [] # list of (musicfilename, timestampe)
        self.retry_queue = Queue()
        self.temp_wav_files = []
        
        self.tagFunctions = {
            "sound" : self._soundTag,
            "mixin" : self._mixinTag}

    def temp_wav_file(self):
        f = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        # without fsync the file isn't guaranteed to be available to other threads
        # even if you .close() it
        # python is so great it just works /s
        os.fsync(f)
        f.close()
        self.temp_wav_files.append(f)
        return f

    def silence_filename(self) -> str:
        return self._silence_filename
    
    def cleanup(self):
        for tf in self.temp_wav_files:
            os.remove(tf.name)
        
        os.remove(self.tmpfile.name)
        if not(self._keep_acc):
            os.remove(self.accfile.name)
            
        
        
    def getVoiceSampleFile(self):
        return self.args.voice
        
    def processMsg(self, msg):
        # returns (newMsg, continue_bool, errormsg)

        if msg == "":
            return ("", True, "")

        if self.args.model == "xtts":
            msg = fixDivZero(msg)
        
        (tag, tagArgs) = maybeGetTag(msg)
        if tag in self.tagFunctions:
            return self.tagFunctions[tag](tagArgs)

        return (msg, False, "")

    def _soundTag(self, argv):
        if argv == []:
            return ("", True, "")

        sndFile = self.args.sound_dir + "/" + " ".join(argv)
        if not(os.path.isfile(sndFile)):
            return ("", False, "Warning: Could not find sound file: " + sndFile)

        self.accumulateSound(sndFile)
        return ("", True, "")

    def _mixinTag(self, argv):
        if argv == []:
            return ("", True, "")        

        mixfile = argv[0]
        clip = AudioFileClip(self.accfile)
        self.mixins.append((mixfile, clip.duration))
        return ("", True, "")

    def handleMixins(self):
        # adds background music etc. to the accumulated file
        clips = [AudioFileClip(self.accfile.name)]
        for (mixfile, timestamp) in self.mixins:
            clips.append(AudioFileClip(mixfile).with_start(timestamp))

        outclip = CompositeAudioClip(clips)
        # this is due to a bug (I think) in moviepy with fps not being defined
        if "fps" not in outclip.__dict__:
            outclip.fps = 44100
        outclip.write_audiofile(self.tmpfile.name)
        subprocess.run(["cp", self.tmpfile.name, self.accfile.name])        
        
    def accumulateSound(self, sndFile):
        subprocess.run(["sox", self.accfile.name, sndFile, self.tmpfile.name])
        subprocess.run(["cp", self.tmpfile.name, self.accfile.name])
        
    def addPause(self):
        if self.args.pause_duration == 0:
            return

        for n in range(0, self.args.pause_duration):
            self.accumulateSound(self._silence_filename)

    def isRetrying(self):
        return not(self.retry_queue.empty())
                
    def retry(self, msg):
        # called when e.g. a messsage is too long, which we can only know after the fact (thanks api designers)
        # first attempt of fixing is by removing all quotation marks. This seems to confuse the tokenizer, when quotation marks aren'tr balanced
        w = msg.replace('"', "")
        if w != msg:
            self.retry_queue.put(w)
            return
        # most really long run-on sentences are due to many commas. We find a comma in the middle and split the string in two
        ws = msg.split(",")
        if len(ws) > 1:
            i = (len(ws) // 2) - 1
            v1 = ",".join(ws[0:i])
            v2 = ws[i] + "." + ",".join(ws[i:])
            self.retry_queue.put(v1)
            self.retry_queue.put(v2)
            return

        # now the gloves are off. Brutally split the string in half.
        i = len(msg) // 2
        self.retry_queue.put(msg[:i])
        self.retry_queue.put(msg[i:])
        return

    def popRetryMSG(self):
        return self.retry_queue.get()
    
    def clearRetries(self):
        with self.retry_queue.mutex:
            self.retry_queue.queue.clear()
            
        
        

```

## tts_util.py

```python
import subprocess, tempfile, os
import datetime
#from TTS.tts.layers.xtts.tokenizer import split_sentence

def getAccumulatorFile(filepath=""):
    if filepath:
        f = open(filepath, "w")
    else:
        f = tempfile.NamedTemporaryFile(suffix=datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S") + ".wav", delete=False)
    return f

def maybeGetTag(w):
    # returns pair of (tag, tagargs) as (string,. list of strings)
    delim = "#"
    if w.startswith(delim):
        ws = w.split(" ")
        tag = ws[0][len(delim):]
        args = ws[1:]
        return (tag, args)
    return ("", [])
        


    



def dir2(x):
    for k in dir(x):
        if k.startswith("_"):
            continue
        print(str(k))


def fixDivZero(w):
    # so, weirdly, the TTS will crash when it encounters xml tags like <test> or <begin> or really anythingin <> brackets. (division by zero crash)
    # easy fix is just to replace the symbols with spelled out words, since at this point we only care about the spoken part anyway. Fucks up languages other than english, but oh well
    return w.replace("<", " less than ").replace(">", " greater than ")



```

## util.py

```python
import os, getpass, shutil, base64, requests, re, csv, glob, time

# removed tortoise dependency because it will require torch, which import multiprocess, which won't work with renpy
# FIXME: not a big deal because tortoise and all tts are spawned with subprocess. However, we will have to find a better way to get the voices.
# import tortoise.utils.audio

from colorama import Fore, Back, Style
import appdirs
import sys
from functools import *



def getAITime() -> str:
    """Returns current time in a format that is unlikely to invalidate the cache when put into the system prompt of an AI."""
    return time.strftime("%A, %B %d, %Y")
def getErrorPrefix():
    return " # "


def stringToColor(w):
    """Maps normal strings like 'red' to ANSI control codes using colorama package."""
    w = w.upper()
    for color in Fore.__dict__.keys():
        if w == color:
            return Fore.__dict__[color]
    return Fore.RESET


def stringToStyle(w):
    """Maps normal strings to ANSI control codes for style, like 'bright' etc. using colorama."""
    w = w.upper()
    for s in Style.__dict__.keys():
        if w == s:
            return Style.__dict__[s]
    return Style.RESET_ALL


def wrapColorStyle(w, color, style):
    return style + color + w + Fore.RESET + Style.RESET_ALL


# this can be used to modify printerr behaviour. It can be a function that accepts one argument -> the to be printed string
printerr_callback = None
printerr_disabled = False


def printerr(w, prefix=getErrorPrefix(), color=Fore.GREEN):
    global printerr_disabled
    if printerr_disabled:
        return

    if w == "":
        return

    if w.startswith("error:"):
        color = Fore.RED

    if w.startswith("warning:"):
        color = Fore.YELLOW

    # prepend all lines with prefix
    ws = w.split("\n")
    new_w = ("\n" + prefix).join(ws)
    formatted_w = color + prefix + new_w + Fore.RESET
    print(formatted_w, file=sys.stderr)
    if printerr_callback is not None:
        printerr_callback(color + w + Fore.RESET)


def getArgument(argname, argv):
    ws = argv.split(argname)
    if len(ws) < 2:
        return None
    return ws[1].split(" ")[1]


def trimOn(stopword, w):
    return w.split(stopword)[0]


def trimChatUser(chatuser, w):
    if chatuser:
        return trimOn(mkChatPrompt(chatuser), trimOn(mkChatPrompt(chatuser).strip(), w))
    return w


def assertNotStartWith(assertion, w):
    # ensures w doesn't start with assertion
    l = len(assertion)
    if w.startswith(assertion):
        return w[l:]
    return w


def assertStartWith(assertion, w):
    # makes sure w starts with assertion. This is intended for making sure strings start with a chat prompt, i.e. Bob: bla bla bla, without duplicating it, as in Bob: Bob: bla bla
    if not (w.startswith(assertion)):
        return assertion + w
    return w


def mkChatPrompt(username, space=True):
    # turns USERNAME into USERNAME:, or, iuf we decide to change it, maybe <USERNAME> etc.
    if username == "":
        return ""
    if space:
        return username + ": "
    return username + ":"


def ensureColonSpace(usertxt, generatedtxt):
    """So here's the problem: Trailing spaces in the prompt mess with tokenization and force the backend to create emoticons, which isn't always what we want.
        However, for chat mode, trailing a colon (:) means the backend will immediately put a char behind it, which looks ugly. There doesn't seem to be a clean way to fix that, short of retraining the tokenizer. So we let it generate text behind the colon, and then add a space in this step manually. This is complicated by the way we split user and generated text.
        usertxt - User supplied text, which may end in something like 'Gary:'
    generatedtxt - Text generated by backend. This immediately follows usertxt for any given pormpt / backend interaction. It may or may not start with a newline.
        Returns - The new usertxt as a string, possibly with a newline added to it."""
    if generatedtxt.startswith(" "):
        return usertxt

    if usertxt.endswith(":"):
        return usertxt + " "
    return usertxt


def ensureFirstLineSpaceAfterColon(w):
    # yes its a ridiculous name but at least it's descriptive
    if w == "":
        return w

    if len(w) <= 2:
        if w == "::":
            return ": :"
        elif w.endswith(":"):
            return w + " "

    ws = w.split("\n")
    v = ws[0]
    for i in range(0, len(v) - 1):
        if v[i] == ":":
            if v[i + 1] == " ":
                return w
            else:
                ws[0] = v[: i + 1] + " " + v[i + 1 :]
                break
    return "\n".join(ws)


def filterPrompt(prompt, w):
    # filters out prompts like "Assistant: " at the start of a line, which can sometimes be generated by the LLM on their own
    return w.replace("\n" + prompt, "\n")


def filterLonelyPrompt(prompt, w):
    # this will filter out prompts like "Assistant: ", but only if it's the only thing on the line. This can happen after trimming. Also matches the prompt a little fuzzy, since sometimes only part of the prompt remains.
    ws = w.split("\n")
    return "\n".join(filter(lambda v: not (v in prompt), ws))


def discardFirstLine(w):
    return "\n".join(w.split("\n")[1:])


def filterLineBeginsWith(target, w):
    """Returns w with all lines that start with target removed. Matches target a little fuzzy."""
    acc = []
    targets = [target, " " + target, target + " "]
    for line in w.split("\n"):
        dirty = False
        for t in targets:
            if line.startswith(t):
                dirty = True
        if not (dirty):
            acc.append(line)
    return "\n".join(acc)


def saveFile(filename, w, depth=0):
    # saves w in filename, but won't overwrite existing files, appending .new; returns the successful filename, if at all possible
    if depth > 10:
        return ""  # give up

    if os.path.isfile(filename):
        parts = filename.split(".")
        if len(parts) > 1:
            newfilename = ".".join([parts[0], "new"] + parts[1:])
        else:
            newfilename = filename + ".new"
        return saveFile(newfilename, w, depth=depth + 1)

    f = open(filename, "w")
    f.write(w)
    f.flush()
    return filename


def stripLeadingHyphens(w):
    # FIXME: this is hacky
    w = w.split("=")[0]

    if w.startswith("--"):
        return w[2:]

    if w.startswith("-"):
        return w[1:]

    return w


def userConfigFile(force=False):
    # return location of ~/.ghostbox.conf.json, or "" if not found
    userconf = "ghostbox.conf"
    path = appdirs.user_config_dir() + "/" + userconf
    if os.path.isfile(path) or force:
        return path
    return ""


def userCharDir():
    return appdirs.user_data_dir() + "/ghostbox/chars"


def ghostboxdir():
    return appdirs.user_data_dir() + "/ghostbox"


def userTemplateDir():
    return ghostboxdir() + "/templates"


def inputChoice(msg, choices):
    while True:
        w = input(msg)
        if w in choices:
            return w


def userSetup():
    if not (os.path.isfile(userConfigFile())):
        print("Creating config file " + userConfigFile(force=True))
        f = open(userConfigFile(force=True), "w")
        f.write('{\n"chat_user" : "' + getpass.getuser() + '"\n}\n')
        f.flush()

    if not (os.path.isdir(userCharDir())):
        print("Creating char dir " + userCharDir())
        os.makedirs(userCharDir())

    # try copying some example chars
    chars = "dolphin dolphin-kitten joshu minsk scribe command-r".split(" ")
    copyEntity("char", "chars", chars)
    templates = "chat-ml alpaca raw mistral user-assistant-newline vacuna command-r llama3 phi3-instruct".split(
        " "
    )
    copyEntity("template", "templates", templates)


def copyEntity(entitynoun, entitydir, entities):
    chars = entities
    choice = ""
    try:
        for char in chars:
            chardir = ghostboxdir() + "/" + entitydir + "/" + char
            if os.path.isdir(chardir) and choice != "a":
                choice = inputChoice(
                    chardir + " exists. Overwrite? (y/n/a): ", "y n a".split(" ")
                )
                if choice == "n":
                    continue
            print("Installing " + entitynoun + " " + char)
            shutil.copytree(entitydir + "/" + char, chardir, dirs_exist_ok=True)
    except:
        print("Warning: Couldn't copy example " + entitynoun + "s.")


def getJSONGrammar():
    return r"""root   ::= object
value  ::= object | array | string | number | ("true" | "false" | "null") ws

object ::=
  "{" ws (
            string ":" ws value
    ("," ws string ":" ws value)*
  )? "}" ws

array  ::=
  "[" ws (
            value
    ("," ws value)*
  )? "]" ws

string ::=
  "\"" (
    [^"\\] |
    "\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes
  )* "\"" ws

number ::= ("-"? ([0-9] | [1-9] [0-9]*)) ("." [0-9]+)? ([eE] [-+]? [0-9]+)? ws

# Optional space: by convention, applied in this grammar after literal chars when allowed
ws ::= ([ \t\n] ws)?
"""


def loadImageData(image_path):
    with open(image_path, "rb") as image_file:
        base64_bytes = base64.b64encode(image_file.read())
        return base64_bytes


def packageImageDataLlamacpp(data_base64, id):
    return {"data": data_base64.decode("utf-8"), "id": id}


# def repackImages(images):
#    """Takes the plumbing.images object and makes sure its base64 encoded."""
#    return [{"data" : data_base64.decode("utf-8"), "id" : id}


def mkImageEmbeddingString(image_id):
    return "[img-" + str(image_id) + "]"


def maybeReadInt(w):
    try:
        n = int(w)
    except:
        return None
    return n


def isImageFile(file):
    # good enuff
    return file.endswith(".png")


def getImageExtension(url, default="png"):
    ws = url.split(".")
    if len(ws) < 2:
        return default
    return ws[-1]


def dirtyGetJSON(url):
    r = requests.get(url)
    if r.status_code == 200:
        return r.json()
    return {}


def replaceFromDict(w, d, key_func=lambda k: k):
    def replace_and_check(v, pair):
        v_new = v.replace(key_func(pair[0]), pair[1])
        if type(v_new) != str:
            return str(v_new)
        return v_new

    return reduce(replace_and_check, d.items(), w)


ansi_escape = re.compile(r"\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])")


def stripANSI(w):
    return ansi_escape.sub("", w)


def getLayersFile():
    return appdirs.user_config_dir() + "/llm_layers"


def loadLayersFile():
    """Returns a list of dictionaries, one for each row in the layers file."""
    f = open(getLayersFile(), "r")
    return list(csv.DictReader(filter(lambda row: row[0] != "#", f), delimiter="\t"))


def envFromDict(d):
    """Returns a kstandard shell environment with variables added from the provided dictionary d."""
    return os.environ | {k: str(v) for (k, v) in d.items()}


def explodeIncludeDir(include, extradir):
    """So that we can turn 'char/' into ['/include/path/char/', 'char/'] etc."""
    include = os.path.normpath(include)
    extradir = os.path.normpath(extradir)
    return [include, extradir, include + "/" + extradir]


def ultraglob(include_dirs, specific_dir):
    # everyone hates nested listcomprehensions
    acc = []
    for include_dir in include_dirs:
        acc += [
            glob.glob(dir + "/*")
            for dir in explodeIncludeDir(include_dir, specific_dir)
        ]
    return reduce(lambda xs, ys: xs + ys, acc, [])


def getVoices(prog):
    """Returns a list of strings with voice names for a given Program object."""
    pollyvoices = "Lotte, Maxim, Ayanda, Salli, Ola, Arthur, Ida, Tomoko, Remi, Geraint, Miguel, Elin, Lisa, Giorgio, Marlene, Ines, Kajal, Zhiyu, Zeina, Suvi, Karl, Gwyneth, Joanna, Lucia, Cristiano, Astrid, Andres, Vicki, Mia, Vitoria, Bianca, Chantal, Raveena, Daniel, Amy, Liam, Ruth, Kevin, Brian, Russell, Aria, Matthew, Aditi, Zayd, Dora, Enrique, Hans, Danielle, Hiujin, Carmen, Sofie, Gregory, Ivy, Ewa, Maja, Gabrielle, Nicole, Filiz, Camila, Jacek, Thiago, Justin, Celine, Kazuha, Kendra, Arlet, Ricardo, Mads, Hannah, Mathieu, Lea, Sergio, Hala, Tatyana, Penelope, Naja, Olivia, Ruben, Laura, Takumi, Mizuki, Carla, Conchita, Jan, Kimberly, Liv, Adriano, Lupe, Joey, Pedro, Seoyeon, Emma, Niamh, Stephen".split(
        ", "
    )

    kokoro_voices = """af_alloy
af_aoede
af_bella
af_heart
af_jessica
af_kore
af_nicole
af_nova
af_river
af_sarah
af_sky
am_adam
am_echo
am_eric
am_fenrir
am_liam
am_michael
am_onyx
am_puck
am_santa
bf_alice
bf_emma
bf_isabella
bf_lily
bm_daniel
bm_fable
bm_george
bm_lewis
ef_dora
em_alex
em_santa
ff_siwis
hf_alpha
hf_beta
hm_omega
hm_psi
if_sara
im_nicola
jf_alpha
jf_gongitsune
jf_nezumi
jf_tebukuro
jm_kumo
pf_dora
pm_alex
pm_santa
zf_xiaobei
zf_xiaoni
zf_xiaoxiao
zf_xiaoyi
zm_yunjian
zm_yunxi
zm_yunxia
zm_yunyang""".split(
        "\n"
    )


    # hardcoded for random voice selection. user gets the full list from ghostbox-tts after about 3 seconds
    orpheus_voices = ["tara", "leah", "jess", "leo", "dan", "mia", "zac", "zoe"]
    vs = []
    if prog.getOption("tts_model") == "polly":
        for voice in pollyvoices:
            vs.append(voice)
    elif prog.getOption("tts_program") == "ghostbox-tts-tortoise":
        # vs = list(tortoise.utils.audio.get_voices(extra_voice_dirs=list(filter(bool, [prog.getOption("tts_voice_dir")]))))
        # FIXME: find another way to get the list of voices
        vs = []
    elif prog.getOption("tts_model") == "kokoro":
        english_voices = [
            voice
            for voice in kokoro_voices
            if voice.startswith("af_")
            or voice.startswith("am_")
            or voice.startswith("bm_")
            or voice.startswith("bf_")
        ]
        if prog.getOption("tts_language") == "en":
            return english_voices
        elif prog.getOption("tts_language") == "":
            return kokoro_voices
        else:
            return [voice for voice in kokoro_voice if voice not in english_voices]

    elif prog.getOption("tts_model") == "orpheus":
        return orpheus_voices
    else:
        # for file in ultraglob(prog.getOption("include"), prog.getOption("tts_voice_dir")):
        vs = [
            os.path.split(file)[1]
            for file in glob.glob(prog.tryGetAbsVoiceDir() + "/*")
            if os.path.isfile(file)
        ]
    return vs


def time_ms():
    return round(time.time() * 1000)


def compose2(f, g):
    return lambda x: f(g(x))

```

