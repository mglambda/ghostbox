## README.md

```md
# ghostbox

Ghostbox is a command line interface to local LLM (large language model) server applications, such as koboldcpp or llama.cpp. It's primary purpose is to give a simple, stripped-down way to engage with AI chat bots.
Ghostbox was made to be blind-accessible, and is fully compatible with any screen reader that can read a shell.

## Features

 - Command line interface that plays well with standard input and output
 - Supports various backends, including llama.cpp, llama.box, as well as anything using the OpenAI API
 - Character templates to define AI characters in a simple, file based manner
 - Includes prompt templates for many popular formats (like chat-ml)
 - Chat functionality, including retrying or rewriting prompts, and a chat history that can be saved and reloaded
 - Live audio transcription using OpenAI's Whisper model
 - Multimodal support for images (llava, qwen2-vl), including automatic descriptions for screenshots or arbitrary image files.
 - Painless JSON output (just do --json)
 - Grammars to arbitrarily restrict token output 
 - TTS (text-to-speech) with various backends, including Zonos, Kokoro and Amazon Polli, using ghostbox-tts.
 - Token streaming
 - Model Sampling parameter control (temperature, top p, etc.), including with config-files
 - small web ui with --http on localhost:5050
 - Simple design. By default, ghostbox let's you just send raw text to your LLM, and returns the generated output. This is great if you want to learn about proper prompting, which was part of my motivation for making this.

## Documentation
### Character Templates

Coming soon!

### Additional Documentation

Try `ghostbox --help` or type `/help` at the CLI for additional information.

## Installation
### Requirements

Ghostbox requires an LLM backend. Currently supported backends are
 - Llama.cpp (currently prefered, find it at https://github.com/ggerganov/llama.cpp )  
 - Koboldcpp ( https://github.com/LostRuins/koboldcpp )

Clone either repository, build the project and run the backend server. Make sure you start ghostbox with the --endpoint parameter to the endpoint provided by the backend. This is http://localhost:8080 for Llama.cpp (default), or http://localhost:5001 for koboldcpp, at least by default.


### Amazon Polly

The default TTS script provided allows the use of amazon web services (aws) polly voices. To use them, you must create an aws account, which will give you API keys. You can then do (example for arch-linux)

```
pacman -S aws
aws configure
```

and you will be asked for your keys. To test if it works, you can either run ghostbox with the `--tts` option, activate TTS by typing `/tts` at the ghostbox CLI, or try the `scripts/ghostbox-tts-polly` script in the repository.
### Multimodal Image Description

Acquire the latest Llava model from huggingface. You will need a gguf file and a mmproj file with model projections for the underlying LLM (e.g. `mmproj-model-f16.gguf`). You can then start Llama.cpp like this

```
cd ~/llama.cpp/build/bin 
./server --ctx-size 2048 --parallel 1 --mlock --no-mmap -m llava-v1.6-mistral-7b.Q5_K_M.gguf --mmproj mmproj-model-f16.gguf
```

assuming you built the project in `llama.cpp/build`. You may then do either 
```
ghostbox -cdolphin
/image ~/Pictures.paris.png
Here is a picture of paris. [img-1] Can you please describe it?
```

or, for automatically describing the screen when you make a screenshot

```
ghostbox -cdolphin --image_watch
```

assuming screenshots are stored in `~/Pictures/Screenshots/`, which is the default for the gnome-screenshot utility.
Note that multimodal support is currently working with Llama.cpp only.

### Python Package
The repository can be installed as a python package.

```
git clone https://github.com/mglambda/ghostbox
cd ghostbox
python -m venv env
. env/bin/activate
pip install .
```

I try to keep the setup.py up-to-date, but the installation might fail due to one or two missing python packages. If you simply `pip install <package>` for every missing package while in the environment created above, ghostbox will eventually install.

This should make the `ghostbox` command available. Alternatively, it can be found in `scripts/ghostbox`.

### Data

Run
```
./scripts/ghostbox-install
```

To create data folders with some example characters in `~/.local/share/ghostbox` and a default config file `~.config/ghostbox.conf`.

After a successful installation, while a backend is running, try
```
ghostbox -cdolphin
```

to begin text chat with the helpful dolphin assistant, or try

```
ghostbox -cjoshu --tts --audio --hide
```

for an immersive chat with a zen master.

```

## LICENSE

```
                   GNU LESSER GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.


  This version of the GNU Lesser General Public License incorporates
the terms and conditions of version 3 of the GNU General Public
License, supplemented by the additional permissions listed below.

  0. Additional Definitions.

  As used herein, "this License" refers to version 3 of the GNU Lesser
General Public License, and the "GNU GPL" refers to version 3 of the GNU
General Public License.

  "The Library" refers to a covered work governed by this License,
other than an Application or a Combined Work as defined below.

  An "Application" is any work that makes use of an interface provided
by the Library, but which is not otherwise based on the Library.
Defining a subclass of a class defined by the Library is deemed a mode
of using an interface provided by the Library.

  A "Combined Work" is a work produced by combining or linking an
Application with the Library.  The particular version of the Library
with which the Combined Work was made is also called the "Linked
Version".

  The "Minimal Corresponding Source" for a Combined Work means the
Corresponding Source for the Combined Work, excluding any source code
for portions of the Combined Work that, considered in isolation, are
based on the Application, and not on the Linked Version.

  The "Corresponding Application Code" for a Combined Work means the
object code and/or source code for the Application, including any data
and utility programs needed for reproducing the Combined Work from the
Application, but excluding the System Libraries of the Combined Work.

  1. Exception to Section 3 of the GNU GPL.

  You may convey a covered work under sections 3 and 4 of this License
without being bound by section 3 of the GNU GPL.

  2. Conveying Modified Versions.

  If you modify a copy of the Library, and, in your modifications, a
facility refers to a function or data to be supplied by an Application
that uses the facility (other than as an argument passed when the
facility is invoked), then you may convey a copy of the modified
version:

   a) under this License, provided that you make a good faith effort to
   ensure that, in the event an Application does not supply the
   function or data, the facility still operates, and performs
   whatever part of its purpose remains meaningful, or

   b) under the GNU GPL, with none of the additional permissions of
   this License applicable to that copy.

  3. Object Code Incorporating Material from Library Header Files.

  The object code form of an Application may incorporate material from
a header file that is part of the Library.  You may convey such object
code under terms of your choice, provided that, if the incorporated
material is not limited to numerical parameters, data structure
layouts and accessors, or small macros, inline functions and templates
(ten or fewer lines in length), you do both of the following:

   a) Give prominent notice with each copy of the object code that the
   Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the object code with a copy of the GNU GPL and this license
   document.

  4. Combined Works.

  You may convey a Combined Work under terms of your choice that,
taken together, effectively do not restrict modification of the
portions of the Library contained in the Combined Work and reverse
engineering for debugging such modifications, if you also do each of
the following:

   a) Give prominent notice with each copy of the Combined Work that
   the Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the Combined Work with a copy of the GNU GPL and this license
   document.

   c) For a Combined Work that displays copyright notices during
   execution, include the copyright notice for the Library among
   these notices, as well as a reference directing the user to the
   copies of the GNU GPL and this license document.

   d) Do one of the following:

       0) Convey the Minimal Corresponding Source under the terms of this
       License, and the Corresponding Application Code in a form
       suitable for, and under terms that permit, the user to
       recombine or relink the Application with a modified version of
       the Linked Version to produce a modified Combined Work, in the
       manner specified by section 6 of the GNU GPL for conveying
       Corresponding Source.

       1) Use a suitable shared library mechanism for linking with the
       Library.  A suitable mechanism is one that (a) uses at run time
       a copy of the Library already present on the user's computer
       system, and (b) will operate properly with a modified version
       of the Library that is interface-compatible with the Linked
       Version.

   e) Provide Installation Information, but only if you would otherwise
   be required to provide such information under section 6 of the
   GNU GPL, and only to the extent that such information is
   necessary to install and execute a modified version of the
   Combined Work produced by recombining or relinking the
   Application with a modified version of the Linked Version. (If
   you use option 4d0, the Installation Information must accompany
   the Minimal Corresponding Source and Corresponding Application
   Code. If you use option 4d1, you must provide the Installation
   Information in the manner specified by section 6 of the GNU GPL
   for conveying Corresponding Source.)

  5. Combined Libraries.

  You may place library facilities that are a work based on the
Library side by side in a single library together with other library
facilities that are not Applications and are not covered by this
License, and convey such a combined library under terms of your
choice, if you do both of the following:

   a) Accompany the combined library with a copy of the same work based
   on the Library, uncombined with any other library facilities,
   conveyed under the terms of this License.

   b) Give prominent notice with the combined library that part of it
   is a work based on the Library, and explaining where to find the
   accompanying uncombined form of the same work.

  6. Revised Versions of the GNU Lesser General Public License.

  The Free Software Foundation may publish revised and/or new versions
of the GNU Lesser General Public License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns.

  Each version is given a distinguishing version number. If the
Library as you received it specifies that a certain numbered version
of the GNU Lesser General Public License "or any later version"
applies to it, you have the option of following the terms and
conditions either of that published version or of any later version
published by the Free Software Foundation. If the Library as you
received it does not specify a version number of the GNU Lesser
General Public License, you may choose any version of the GNU Lesser
General Public License ever published by the Free Software Foundation.

  If the Library as you received it specifies that a proxy can decide
whether future versions of the GNU Lesser General Public License shall
apply, that proxy's public statement of acceptance of any version is
permanent authorization for you to choose that version for the
Library.

```

## pyproject.toml

```toml
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "ghostbox"
version = '0.21.0'
description = "Ghostbox is a command line interface to local LLM (large language model) server applications, such as koboldcpp or llama.cpp. It's primary purpose is to give a simple, stripped-down way to engage with AI chat bots."
authors = [
    { name = "Marius Gerdes", email = "integr@gmail.com" }
]
readme = "README.md"
license = { file = "LICENSE" }
dependencies = [
    "requests",
    "boto3",
    "appdirs",
    "lazy-object-proxy",
    "openai-whisper",
    "pyaudio",
    "pydub",
    "colorama",
    "automodinit",
    "deepspeed",
    "docstring_parser",
    "jsonpickle",
    "shutils",
    "nltk",
    "wget",
    "kokoro_onnx[gpu]",
    "websockets",
    "duckduckgo-search",
    "markdownify",
    "h2",
    "pyperclip"
]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: GNU General Public License v2 (GPLv2)",
    "Operating System :: OS Independent",
]
requires-python = ">=3.7"

[tool.setuptools]
packages = ["ghostbox"]
package-data = { "ghostbox" = ["data/**"] }
script-files = ["scripts/ghostbox", "scripts/ghostbox-tts"]

[project.urls]
"Homepage" = "https://github.com/mglambda/ghostbox"

```

## agency.py

```python
# allows for use of tools with tools.py in char directory.

import os, importlib, inspect, docstring_parser, json, re, traceback
from pydantic import BaseModel
from ghostbox.util import *
from typing import *
from ghostbox.definitions import *

# # example of tool dict
# "tools": [
#     {
#     "type":"function",
#     "function":{
#         "name":"python",
#         "description":"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.",
#         "parameters":{
#         "type":"object",
#         "properties":{
#             "code":{
#             "type":"string",
#             "description":"The code to run in the ipython interpreter."
#             }
#         },
#         "required":["code"]
#         }
#     }
#     }
# ],

def type_to_json_schema(type_):
    type_map = {
        int: {"type": "integer"},
        float: {"type": "number"},
        str: {"type": "string"},
        bool: {"type": "boolean"},
        list: {"type": "array", "items": {}},  # You might need to specify the item type
        dict: {"type": "object"},
        type(None): {"type": "null"}
    }
    if type_ in type_map:
        return type_map[type_]
    elif hasattr(type_, "__origin__"):
        if type_.__origin__ is list:
            return {"type": "array", "items": type_to_json_schema(type_.__args__[0])}
        elif type_.__origin__ is dict:
            return {"type": "object", "additionalProperties": type_to_json_schema(type_.__args__[1])}
        elif type_.__origin__ is Optional:
            return {"oneOf": [type_to_json_schema(type_.__args__[0]), {"type": "null"}]}
    return {}


def makeTools(filepath, display_name="tmp_python_module", tools_forbidden=[]) -> Tuple[List[Tool], Any]:
    """Reads a python file and returns a pair with all the top level functions parsed as tools, and the corresponding module for the file."""
    if not(os.path.isfile(filepath)):
        printerr("warning: Failed to generate tool dictionary for '" + filepath + "' file not found.")
        return ({}, None)


    tools = []
    spec = importlib.util.spec_from_file_location(display_name, filepath)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    for name, value in vars(module).items():
        if name.startswith("_") or not callable(value) or name in tools_forbidden:
            continue
        doc = inspect.getdoc(value)
        if doc is None:
            printerr("error: Missing docstring in function '" + name + "' in file '" + filepath + "'. Aborting tool generation.")
            return ({}, None)
        fulldoc = docstring_parser.parse(doc)
        if fulldoc.description is None:
            printerr("warning: Missing description in function '" + name + "' in file '" + filepath + "'. Please make sure you adhere to standard python documentation syntax.")
            description = doc
        else:
            description = fulldoc.description


        sig = inspect.signature(value)
        paramdocs = {p.arg_name : {"type" : p.type_name, "description" : p.description, "optional" : p.is_optional} for p in fulldoc.params}
        properties = {}
        required_params = []
        for (param_name, param) in sig.parameters.items():
            if param.annotation == inspect._empty:
                printerr("warning: Missing type annotations for function '" + name + "' and parameter '" + param_name + "' in '" + filepath + "'. This will significantly degrade AI tool use performance.")
                # default to str
                param_type = "str"
            else:
                param_type = param.annotation.__name__

            # defaults
            param_description = ""
            param_required = True
            if param_name not in paramdocs:
                printerr("warning: Missing documentation for parameter '" + param_name + "' in function '" + name + "' in '" + filepath + "'. This will significantly degrade AI tool use performance.")
            else:
                p = paramdocs[param_name]
                if p["description"] is None:
                    printerr("warning: Missing description for parameter '" + param_name + "' in function '" + name + "' in '" + filepath + "'. This will significantly degrade AI tool use performance.")
                else:
                    param_description = p["description"]

                #if p["type"] != param_type:
                    #printerr("warning: Erroneous type documentation for parameter '" + param_name + "' in function '" + name + "' in '" + filepath + "'. Stated type does not match function annotation. This will significantly degrade AI tool use performance.")

                if p["optional"] is not None:
                    param_required = not(p["optional"])

            # finally set the payload
            if param_required:
                required_params.append(param_name)
            properties[param_name] = Property(type=type_to_json_schema(param_type).get("type", "string"),
                                              description=param_description)


        parameters = Parameters(required=required_params,
                                properties=properties)
        new_function = Function(name=name,
                                    description=description,
                                    parameters=parameters)
        if new_function.name not in tools_forbidden:
            tools.append(Tool(function=new_function))



    return (tools, module)

def tryParseToolUse(w, start_string = "```json", end_string = "```", magic_word="Action:"):
    """Process AI output to see if tool use is requested. Returns a dictionary which is {} if parse failed, and the input string with json removed on a successful parse.
    :param w: The input string, e.g. AI generated response.
    :param predicate: Optional boolean filter function which takes tool names as input.
    :return: A pair of (list(dict), str), with the parsed json and the input string with json removed if parse succeeded."""
    m = re.match(magic_word + ".*" + start_string + "(.*)" + end_string + ".*", w, flags=re.DOTALL)
    if not(m):
        return {}, w

    try:
        capture = m.groups(1)[-1]
        tools_requested = json.loads(capture)
    except:
        printerr("warning: Exception while trying to parse AI tool use.\n```" + w + "```")
        printerr(traceback.format_exc())
        return {}, w


    if type(tools_requested) != list:
        printerr("warning: Wrong type of tool request. Parse succeeded but no tool application possible.")
        printerr("Dump: \n" + json.dumps(tools_requested, indent=4))
        
    # parse succeeded, clean the input
    w_clean = w.replace(start_string + capture + end_string, "").replace(magic_word, "")
    return (tools_requested, w_clean)


def tryParseAllowedToolUse(w : str,
                           tools_allowed : dict):
    return tryParseToolUse(w, predicate=lambda tool_name: tool_name in allowed_tools.keys())


def getPositionalArguments(func):
    return [param.name for (k, param) in inspect.signature(func).parameters.items() if param.default == inspect._empty]

def getOptionalArguments(func):
    return [param.name for (k, param) in inspect.signature(func).parameters.items() if param.default != inspect._empty]

def makeToolResult(tool_name, result, tool_call_id) -> ChatMessage:
    """Packages a tool call result as a ChatMessage."""
    return ChatMessage(
        role="tool",
        tool_name= tool_name,
        tool_call_id= tool_call_id,
        content=str(result))
            


def makeToolSystemMsg(tools):
    """Deprecated"""
    w = ""
    w += "    ## Available Tools\nHere is a list of tools that you have available to you:\n\n"
    w += json.dumps(tools, indent= 4)
    w += "\n\n"
    return w


def makeToolInstructionMsg():
    """Deprecated"""
    #FIXME: this is currently designed only for command-r, other llms will use different special tokens, for which we have to extend the templating, probably iwth tool_begin and tool_end 
    w = """<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>
Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:  

Action:```json
[
    {
        "tool_name": title of the tool in the specification,
        "parameters": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters
    }
]
```

<|END_OF_TURN_TOKEN|>"""
    return w


def showToolResult(tool_result, indent=0):
    """Takes a tool result of any type and returns a string that can be passed to an AI. Contains no special tokens. Expects whatever is in the 'output' field of the tool use dictionary. If tool_result is a list or dictionary, this function will be recursively appplied."""
    x = tool_result
    pad = " " * indent
    if type(x) == type(None):
        return pad + "output: None\n"
    elif type(x) == str:
        return pad + x + "\n"
    elif type(x) == list:
        if x == []:
            # FIXME: not sure what's best here
            return "[]\n"
        return (pad + "\n").join([showToolResult(y, indent=indent) for y in x])
    elif type(x) == dict:
        return (pad + "\n").join([k + ": " + showToolResult(v, indent=indent+4) for (k, v) in x.items()])
    # default to json. if you pass something that isn't json serializable to the AI, we crash and it's your own fault
    # FIXME: also this won't respect indent. maybe that's ok
    try:
        w = json.dumps(x, indent=4)
    except:
        printerr("warning: Couldn't show the result of a tool call.\nHere's the result dump:\n" + str(x) + "\n and here's the traceback:\n")
        printerr(traceback.format_exc())
        return ""
    return w
    

```

## api_internal.py

```python
import os, datetime, glob, sys, requests, traceback, random, json
from typing import *
from ghostbox.session import Session
from ghostbox.util import ultraglob

def start_session(plumbing, filepath: str, keep=False) -> str:
    
    allpaths = [filepath] + [p + "/" + filepath for p in plumbing.getOption("include")]    
    for path in allpaths:
        path = os.path.normpath(path)
        failure = False
        try:
            s = Session(dir=path, chat_user=plumbing.getOption("chat_user"), chat_ai=plumbing.getOption("chat_ai"), additional_keys=plumbing.getOption("var_file"), tools_forbidden=plumbing.getOption("tools_forbidden"))
            break
        except FileNotFoundError as e:
            # session will throw if path is not valid, we keep going through the includes
            failure = e


    if failure:
        return "error: " + str(failure)

    # constructing new session worked
    if not(keep):
        plumbing.session = s
    else:
        # something like /switch happened, we want to keep some stuff
        plumbing.session.merge(s)

    w = ""
    # try to load config.json if present
    configpath = path + "/config.json"
    if os.path.isfile(configpath):
        w += load_config(plumbing, configpath, override=False) + "\n"
    plumbing.options["character_folder"] = path

    # this might be very useful for people to debug their chars, so we are a bit verbose here by default
    w += "Found vars " + ", ".join([k for k in plumbing.session.getVars().keys() if k not in Session.special_files]) + "\n"


    # by convention, the initial message is stored in initial_msg
    if plumbing.session.hasVar("initial_msg") and not(keep):
        plumbing.initial_print_flag = True

    # enable tools if any are found
    if plumbing.session.tools:
        plumbing.setOption("use_tools", True)
        w += "Tool dictionary generated from tools.py, setting use_tools to True. Beware, the AI will now call functions.\n"
        if (callback := plumbing.getOption("tools_inject_dependency_function")):
            if plumbing.session.tools_module             is not None:
                callback(plumbing.session.tools_module            )

        if plumbing.getOption("tools_inject_ghostbox"):
            if plumbing.session.tools_module             is not None:
                plumbing.session.tools_module._ghostbox_plumbing = plumbing

            
        if plumbing.getOption("verbose"):
            w += "Dumping tool dictionary. Run with --no-verbose to disable this."
            w += json.dumps([tool.model_dump() for tool in plumbing.session.tools], indent=4) + "\n"
        else:
            w += "AI tools: " + ", ".join([t.function.name for t in plumbing.session.tools]) + "\n"


    # hide if option is set
    if plumbing.getOption("hide"):
        hide_some_output(plumbing)

    w += "Ok. Loaded " + path
    return w


def load_config(plumbing, filepath, override=True) -> str:
    try:
        w = open(filepath, "r").read()
    except Exception as e:
        return str(e)
    err = plumbing.loadConfig(w, override=override)
    if err:
        return err
    return "Loaded config " + filepath
    


def hide_some_output(plumbing):
    plumbing.options["cli_prompt"] = "\n"
    plumbing.options["audio_show_transcript"] = False
    plumbing.options["tts_subtitles"] = False
    #plumbing.options["stream"] = False
    plumbing.options["chat_show_ai_prompt"] = False
    plumbing.options["color"] = False
    

def toggle_tts(plumbing) -> str:
    prog = plumbing
    prog.options["tts"] = not(prog.options["tts"])
    w = ""
    if prog.options["tts"]:
        err = prog.initializeTTS()
        if err:
            return err
        w += "Try /hide for a more immersive experience.\n"
        prog.setOption("stream_flush", "sentence")
    else:
        # disabled tts
        prog.tts.close()
        prog.tts = None
    return w + "TTS " + {True : "on.", False : "off."}[prog.options["tts"]]
    


def all_chars(prog) -> List[str]:
    allchars = []
    for dir in prog.getOption("include"):
        if not(os.path.isdir(dir)):
            printerr("warning: Include path '" + dir + "' is not a directory.")
            continue
        for charpath in glob.glob(dir + "/*"):
            if os.path.isfile(charpath):
                continue
            allchars.append(os.path.split(charpath)[1])

    return allchars

```

## api.py

```python
from __future__ import annotations
import traceback
from pydantic import BaseModel, ValidationError
from typing import *
from dataclasses import dataclass
from contextlib import contextmanager
from typing import Callable, Dict
from typing_extensions import Self
import json
from ghostbox.main import Plumbing, setup_plumbing
from ghostbox.StoryFolder import StoryFolder
from ghostbox._argparse import makeDefaultOptions
from ghostbox.util import printerr
from ghostbox import commands
from ghostbox.definitions import *
from ghostbox import definitions
from ghostbox.api_internal import *
from ghostbox.agency import Tool, Function, Property, Parameters


def from_generic(endpoint="http://localhost:8080", **kwargs):
    """Returns a Ghostbox instance that connects to an OpenAI API compatible endpoint.
    This generic backend adapter works with many backends, including llama.cpp, llama-box, ollama, as well as online providers, like OpenAI, Anthropic, etc. However, to use features specific to a given backend, that are not part of the OpenAI API, you may need to use a more specific backend.
    Note: Expects ENDPOINT to serve /v1/chat/completions and similar, so e.g. http://localhost:8080/v1/chat/completions should be reachable.
    """
    return Ghostbox(backend=LLMBackend.generic, endpoint=endpoint, **kwargs)


def from_openai_legacy(endpoint="http://localhost:8080", **kwargs):
    """Returns a Ghostbox instance that connects to an OpenAI API compatible endpoint using the legacy /v1/completions interface.
    This generic backend adapter works with many backends, including llama.cpp, llama-box, ollama, as well as online providers, like OpenAI, Anthropic, etc. However, to use features specific to a given backend, that are not part of the OpenAI API, you may need to use a more specific backend.
    Note: There is usually no reason to use this over the generic variant."""
    return Ghostbox(backend=LLMBackend.legacy, endpoint=endpoint, **kwargs)


def from_llamacpp(endpoint="http://localhost:8080", **kwargs):
    """Returns a Ghostbox instance bound to the formidable LLama.cpp. See https://github.com/ggml-org/llama.cpp .
    This uses endpoints described in the llama-server documentation, and will make use of Llama.cpp specific features.
    """
    return Ghostbox(backend=LLMBackend.llamacpp, endpoint=endpoint, **kwargs)


# FIXME: temporarily disabled due to being untested
# ndef from_koboldcpp(endpoint="http://localhost:5001", **kwargs):
#    return Ghostbox(backend="llama.cpp", endpoint=endpoint, **kwargs)


def from_openai_official():
    """Returns a Ghostbox instance that connects to the illustrious OpenAI API at their official servers.
    The endpoint is hardcoded for this one. Use the 'generic' backend to connect to arbitrary URLs using the OpenAI API.
    """
    return Ghostbox(backend=LLMBackend.openai, **kwargs)


class Ghostbox:
    def __init__(self, endpoint: str, backend: LLMBackend, **kwargs):
        self._ct = None
        kwargs["endpoint"] = endpoint
        kwargs["backend"] = backend.name

        self.__dict__ |= kwargs
        default_options, tags = makeDefaultOptions()
        self.__dict__["_plumbing"] = Plumbing(
            options=default_options.__dict__
            | {
                k: v
                for k, v in self.__dict__.items()
                if not (k.startswith("_")) or k in kwargs.keys()
            },
            tags=tags,
        )

        # override with some API defaults
        # FIXME: only if not specified by user
        self.__dict__["_plumbing"].options |= definitions.api_default_options

        if self.config_file:
            self.load_config(self.config_file)
        setup_plumbing(self._plumbing)

        # for arcane reasons we must startthe tts after everything else
        if self._plumbing.tts_flag:
            self._plumbing.tts_flag = False
            self._plumbing.options["tts"] = False
            printerr(toggle_tts(self._plumbing))

    @contextmanager
    def options(self, **kwargs):
        # copy old values
        tmp = {k: v for (k, v) in self._plumbing.options.items() if k in kwargs}
        # this has to be done one by one as setoptions has sideffects
        for new_k, new_v in kwargs.items():
            self._plumbing.setOption(new_k, new_v)
        yield self
        # now unwind, also one by one
        for old_k, old_v in tmp.items():
            self._plumbing.setOption(old_k, old_v)

    @contextmanager
    def option(self, name, value):
        with self.options({name: value}):
            yield self

    def set(option_name: str, value) -> None:
        if option_name in self.__dict__:
            self.__dict__[option_name] = value
        self._plumbing.setOption(option_name, value)

    def get(self, option_name: str) -> object:
        return self._plumbing.getOption(option_name)

    def set_vars(self, injections: Dict[str, str]) -> Self:
        for k, v in injections.items():
            self._plumbing.session.setVar(k, v)
        return self

    def __getattr__(self, k):
        return self.__dict__["_plumbing"].getOption(k)

    def __setattr__(self, k, v):
        if k == "_plumbing":
            return

        if k in self.__dict__:
            self.__dict__[k] = v

        if "_plumbing" in self.__dict__:
            self.__dict__["_plumbing"].setOption(k, v)

    # diagnostics
    def is_busy(self) -> bool:
        """Returns true if an interaction with a backend is currently in progress.
        While busy, all changes to the state of options (e.g. via set or the options context manager) will be buffered and applied when the ghostbox is no longer busy.
        """
        return self._plumbing._frozen

    def timings(self) -> Optional[Timings]:
        """Provides timing and usage statistics for the last request to the backend.
        Returns none if no timings are available, which may happen if either no request has been sent yet, or the backend doesn't support timing.
        :return: Either None or an object with timing statistics.
        """
        return self._plumbing.getBackend().timings()

    # these are the payload functions
    def text(
        self,
        prompt_text: str,
        timeout: Optional[float] = None,
        options: Dict[str, Any] = {},
    ) -> str:
        """Generate text based on a prompt.
        This function blocks until either the generation finishes, or a provided timeout is reached.
        :param prompt_text: The prompt given to the LLM backend.
        :param timeout: Number of seconds to wait before the generation is canceled.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: A string that was generated by the backend, based on the provided prompt.
        """
        with self.options(stream=False, **options):
            return self._plumbing.interactBlocking(prompt_text, timeout=timeout)

    def text_async(
        self,
        prompt_text: str,
        callback: Callable[[str], None],
        options: Dict[str, Any] = {},
    ) -> None:
        """Generate text based on a prompt asynchronously.
        This function does not block. It returns immediately and calls the provided callback when the generation finishes.
        :param prompt_text: The prompt given to the LLM backend.
        :param callback: A function that accepts a string. This will be called when the generation finishes, with the generated string as the only parameter.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: The ghostbox instance.
        """
        with self.options(stream=False, **options):
            # FIXME: this is tricky as we immediately return and set stream = True again ??? what to do
            self._plumbing.interact(prompt_text, user_generation_callback=callback)
        return

    def text_stream(
        self,
        prompt_text: str,
        chunk_callback: Callable[[str], None],
        generation_callback: Callable[[str], None] = lambda x: None,
        options: Dict[str, Any] = {},
    ) -> None:
        """Generate text based on a prompt and stream the response.
        This function does not block. It returns immediately and invokes the provided callbacks when their respective events procure.
        :param prompt_text: The prompt given to the LLM backend.
        :param chunk_callback: This is called for each token that the backend generates, with the token as sole parameter.
        :param generation_callback: This is called exactly once, upon completion of the response, with the entire generation as sole parameter. If you are thinking of concatenating all the tokens in chunk_callback, use this instead.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: The ghostbox instance.
        """
        with self.options(stream=True, **options):
            self._plumbing.interact(
                prompt_text,
                user_generation_callback=generation_callback,
                stream_callback=chunk_callback,
            )
        return

    @staticmethod
    def _make_json_schema(schema: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        if schema is None:
            return {"type": "json_object"}
        return {"type": "json_object", "schema": schema}

    def json(
        self,
        prompt_text: str,
        schema: Optional[Dict] = None,
        timeout: Optional[float] = None,
        options: Dict[str, Any] = {},
    ) -> str:
        """Given a prompt, returns structured output as a string that is json deserializable.
        Output is structured but somewhat unpredictable, unless you provide a json schema. If you are thinking about using pydantic objects and using their model_json_schema method, consider using the ghostbox.new method directly.
        :param prompt_text: The prompt text as natural language.
        :param schema: A dict representing a json schema, which will further restrict the generation.
        :param timeout: Number of seconds to wait before generation is canceled.
        :param options: Additional options that will be passed to the backend, e.g. `{"min_p": 0.01}`.
        :return: A string that contains json, hopefully adequately satisfying the prompt.
        Example use:
        noises = json.loads(box.json("Can you list some animal noises? Please give key/value pairs."))
        noises is now e.g. {"dog": "woof", "cat":"meow", ...}
        """
        with self.options(response_format=self._make_json_schema(schema), **options):
            return self.text(prompt_text, timeout=timeout)

    def json_async(
        self,
        prompt_text: str,
        callback: Callable[[dict], None],
        schema: Optional[Dict[str, Any]] = None,
        options: Dict[str, Any] = {},
    ) -> None:
        """This is an asyncrhonous version of the json method. See its documentation for more. This method returns nothing but expects an additional callback parameter, which will be called with the generated json string as argument once the backend is done generating.
        This function does not block, but returns immediately."""
        with self.options(response_format=self._make_json_schema(schema), **options):
            self.text_async(prompt_text, callback=callback)
        return

    def new(
        self,
        pydantic_class,
        text_prompt: str,
        timeout: Optional[float] = None,
        retries: int = 0,
        options: Dict[str, Any] = {},
    ) -> Any:
        """Given a subclass of pydantic.BaseModel, returns a python object of that type, with its fields filled in by the LLM backend with adherence to a given text prompt.
        This function will block until either generation finishes or a provided timeout is reached.
        This function may raise a pydantic error if the object cannot be validated. Although the LLM will be forced to adhere to the pydantic data model, this can still happen occasionally, for example, in the case of refusals. Either use the retries argument, or wrap a call to new in a try block accordingly.
        :param pydantic_class: The type of object that should be created.
        :param text_prompt: Prompt given to the LLM that should aid in object creation. This will influence how the pydantic object's fields will be filled in. Depending on the model used, certain prompts may lead to refusal, even with object creation, so be careful.
        :param timeout: A timeout in seconds after which generation is canceled.
        :param retries: In case of a validation error, how often should the generation be retried? Since sampling, especially with temperature > 0.0, is not deterministic, retries can eventually yield valid results. A value of 0 means no retries will be performed and the function raises an error on invalid data or refusal. A value of -1 means retry forever or until the timeout is reached.
        :param options: Additional options to pass to ghostbox and possibly the backend, e.g. `{"min_p": 0.01}`. This is an alternative to the `with options` context manager.
        :return: A valid python object of the provided pydantic type, with its fields filled in.

        Example:

        ## animal.py
        ```python
        from pydantic import BaseModel
        import ghostbox, json

        class Animal(BaseModel):
            name: str
            cute_name: str
            number_of_legs: int
            friendly: bool
            favorite_foods: List[str]

        box = ghostbox.from_generic(...)
        cat = box.new(Animal, "Please generate a cute housecat.")
        print(json.dumps(cat.model_dump(), indent=4))
        ```

        ## Output

        ```bash
        $ python animal.py
        {
            "name" : "Cat",
            "cute_name" : "Dr. Kisses",
            "number_of_legs" : 4,
            "friendly" : true,
            "favorite_foods" : ["Tuna", "Cat Treats", "Water from the toilet"]
        }
        ```
        """
        with self.options(stream=False, **options):
            while True:
                try:
                    return pydantic_class(
                        **json.loads(
                            self.json(
                                text_prompt,
                                schema=pydantic_class.model_json_schema(),
                                timeout=timeout,
                            )
                        )
                    )
                except ValidationError as e:
                    if retries == 0:
                        raise e
                    retries -= 1

    # images
    @contextmanager
    def images(self, image_urls: List[str]):
        """Creates a context in which ghostbox queries can refer to images.
        This context manager will not raise an error if an image cannot be found on the file system.
        :param image_urls: A list of either file system paths or web URLs (or both) that point to images.
        :return: A ghostbox instance.
        """
        for i in range(len(image_urls)):
            url = image_urls[i]
            self._plumbing.loadImage(url, i)
        yield self
        self._plumbing.images = {}

    # audio transcription

    def audio_start_transcription(
        self,
        transcription_callback: Callable[[str], None],
        threshold_activation_callback: Callable[[], None],
    ):
        """Start to continuously record and transcribe audio above a certain threshold.
        This method is for rolling your own transcriber. If you want to simply have the ghostbox react to a user saying things, just use the "audio"=True option. E.g.
        ```
        box = ghostbox.from_generic(character_folder="some_helpful_assistant",
            audio=True)
        while True:
            # the box will be transcribing and answering the transcribed text on a seperate thread
            # as if you had called box.text with the transcription
            # (provided silence threshold and activation phrase are met and given)
            # so we idle here
            time.sleep(0.3)
        ```

        You can configure this transcriber with the options context manager. See the various audio_* options for more. Especially the audio_silence_threshold may be interesting.
        If the transcription seems to do nothing, make sure the threshold is low enough and that no activation phrase is set.
        :param transcription_callback: A function that will be called whenever a successful transcription was made. For a transcription to be successful, three conditions must be met: 1. The microhphone must record sound above the activation threshold. 2. If an activation phrase is set, a fuzzy version of the phrase must be found in the transcription. 3. The recording must eventually register audio below the silence threshold again, ending the transcription. If all things are true, the callback will be invoked with the finished transcription. Note that this can be quiet a long time (tens of seconds) away from the beginning of recording.
        :param activation_callback: This function will be called whenever the audio recording goes above the silence threshold. It is called immediately when transcription begins. This is useful, if e.g. you are trying to build a responsive AI that stops the tts instantly when the user speaks.
        :return: The ghostbox instance.
        """
        self.audio_stop_transcription()
        self._ct = self._plumbing.whisper.transcribeContinuously(
            callback=transcription_callback,
            on_threshold=threshold_activation_callback,
            websock=self._plumbing.getOption("audio_websock"),
            websock_host=self._plumbing.getOption("audio_websock_host"),
            websock_port=self._plumbing.getOption("audio_websock_port"),
            silence_threshold=self._plumbing.getOption("audio_silence_threshold"),
        )
        return self

    def audio_stop_transcription(self):
        """Stops an ongoing continuous transcription."""
        if self._ct is not None:
            self._ct.stop()

    # managing ghostbox operation

    def start_session(self, filepath: str, keep=False) -> Self:
        """Start a completely new session with a given character folder.
        This function wipes all history and context variables. It's a clean slate. If you want to switch characters while retaining context, use set_char instead.
        :param filepath: Path to a character folder.
        :return: The ghostbox instance.
        """
        printerr(start_session(self._plumbing, filepath))
        return self

    def load_config(self, config_file: str) -> Self:
        """Loads a config file and applies the option/value pairs in it to this ghostbox instance.
        A config file contains a valid json string that describes an options dictionary.
        Configs loaded this way are intended to be user profiles or similar. AI character folders have their own config.json files that, by convention, are loaded automatically. If you want to simply configure an AI char, you should use their dedicated config file as it generally does *the right thing* when it comes to annoying stuff of overriding options and option precedence etc.
        Example:

        ## alice_config.json

        ```json
        {
            "chat_user" : "alice",
            "tts": true,
            "tts_model": "kokoro",
            "tts_voice": "af_sky",
            "log_time": false
        }
        ```
        """
        printerr(load_config(self._plumbing, config_file))
        # FIXME: update self.__dict__?
        return self

    def tools_inject_dependency(self, symbol_name: str, obj: object) -> Self:
        """Make a python object available in the python tool module of a running ghostbox AI, without having defined it in the tools.py.
        This can be used to inject dependencies from the 'outside'. This is useful in cases where you must make an object available to the AI that can not be acquired during initialization of the tool module, for example, a resource manager, or a network connection.
        Note that the AI will not be aware of an injected dependency, and be unable to reference it. However, you can refer to the symbol_name in the functions you define for the AI, which will be a bound reference as soon as you inject the dependency. The AI may then use the functions that previously didn't refer to the object now bound by symbol_name.
        :param symbol_name: The name of the identifier to be injected. If this is e.g. 'point', point will be bound to obj in tools.py.
        :param obj: An arbitrary python object. A reference to object will be bound to symbol_name and be available in tools.py.
        :return: Ghostbox
        Here's an example use of dependency injection:
        ## human_resources.py
        ```python
        box = ghostbox.from_generic(...)
        # imaginary database connection e.g.
        # we don't want this in the tools.py of employee_assistant
        database_handle = EmployeeDatabase.open_connection(user="bob", password="bruce_schneier")
        # stuff happens, maybe we start a read/evaluate loop
        # ...

        # start our assistant
        box.start_session("employee_assistant")
        #without this, the tool calls would fail
        box.tools_inject_dependency("DBHANDLE", database_handle)
        with options{use_tools=True):
            good_employees = box.text("Can you give me a quick summary of all the employees in the database that have overperformed last motnh?")
        # do something with good_employees
        # ...
        ```

        ## employee_assistant/tools.py
        ```python
        # many tool definitions here
        # ...

        # then the one the AI will probably choose for our request above
        def query_database(sql_query: str) -> Dict:
            # this would fail without dependency injection
            # since DBHANDLE is not defined in the module
            results = DBHANDLE.query(sql)
            # do stuff with results to bring it into a form the AI likes
            final = some_repackaging(results)
            return final
        ```

        At the point above where tools_inject_dependency is called, the DBHANDLE identified in the tools.py module becoems defined. Without the injection, referencing it would raise an exception.
        """
        module = self._plumbing.session.tools_module
        if module is None:
            printerr(
                "warning: Unable to inject dependency '"
                + symbol_name
                + "'. Tool module not initialized."
            )
            return Self

        # FIXME: should we warn users if they overriade an existing identifier? Let's do ti since if they injected once why do they need to do it again?
        if symbol_name in module.__dict__:
            printerr(
                "warning: While trying to inject a dependency: '"
                + symbol_name
                + "' already exists in tool module."
            )

        module.__dict__[symbol_name] = obj

    def tts_say(self, text: str, interrupt: bool = True) -> Self:
        """Speak something with the underlying TTS engine.
        You can modify the tts behaviour by wrapping this call in a `with options(...)` context. For possible options, see the ghostbox documentation of the various tts_* options.
        :param text: The text to be spoken.
        :param interrupt: If true, this call will interrupt speech output that is currently in progress (the default). Otherwise, text will be queue and spoken in sequence.
        :return: The ghostbox instance.
        """
        self._plumbing.communicateTTS(text, interrupt=interrupt)
        return self

    def tts_stop(self) -> Self:
        """Stops speech output that is in progress.
        :return: The ghostbox instance.
        """
        self._plumbing.stopTTS()
        return self

    def set_char(
        self,
        character_folder: str,
        chat_history: Optional[List[ChatMessage | Dict[str, Any]]] = None,
    ) -> Self:
        """Set an active character_folder, which may be the same one, and optionally set the chat history.
        This method differs from start_session in that it doesn't wipe various vars that may be set in the session, and preserves the chat history by default.
        Note: This will wipe the previous history unless chat_history is None.
        :param character_folder: The new character folder to load.
        :param chat_history: A list of ChatHistory items, valid JSON dictionaries that parse as ChatHistoryItems, or a mix of both. If None, chat history will be retained.
        :return: Ghostbox instance."""
        if character_folder != self._plumbing.getOption("character_folder"):
            printerr(start_session(self._plumbing, character_folder))

        if chat_history is None:
            return self

        self._plumbing.session.stories.reset()
        story = self._plumbing.session.stories.get()
        for item in chat_history:
            if type(item) == ChatMessage:
                story.appendMessage(item)
            else:
                # try to parse the item as ChatMessage
                try:
                    story.addRawJSON(item)
                except:
                    printerr(
                        "warning: Couldn't parse chat history. Not a valid ChatMessage. Skipping message. Traceback below."
                    )
                    printerr(traceback.format_exc())
                    continue

        return self

        return self

    def history(self) -> List[ChatMessage]:
        """Returns the current chat history for this ghostbox instance.
        :return: The chat history.
        """
        return self._plumbing.session.stories.get().getData()

```

## _argparse.py

```python
import argparse, os
from ghostbox.util import *
from ghostbox import backends
from ghostbox.definitions import *
from ghostbox.__init__ import get_ghostbox_data

class TaggedArgumentParser():
    """Creates an argument parser along with a set of tags for each argument.
    Arguments to the constructor are passed on to argparse.ArgumentParser.__init__ .
    You can then use add_arguments just like with argparse, except that there is an additional keyword argument 'tags', which is a dictionary that will be associated with that command line argument."""

    def __init__(self, **kwargs):
        self.parser = argparse.ArgumentParser(**kwargs)
        self.tags = {}
        
    def add_argument(self, *args, **kwargs):
        if "tag" in kwargs:
            # this is a bit tricky, argparse does a lot to find the arg name, but this might do
            # find the longest arg, strip leading hyphens, replace remaining hyphens with _
            arg = sorted(args, key = lambda w: len(w), reverse=True)[0].strip("-").replace("-", "_")
            self.tags[arg] = kwargs["tag"]
            self.tags[arg].name = arg
            # and if there is no help then let it blow up
            self.tags[arg].help = kwargs["help"]
            del kwargs["tag"]
            
        self.parser.add_argument(*args, **kwargs)

    def get_parser(self):
        return self.parser

    def get_tags(self):
        return self.tags
        
        
def makeTaggedParser(default_params) -> TaggedArgumentParser:
    parser = TaggedArgumentParser(description="LLM Command Line Interface", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    # we'll be typing these a lot so buckle up
    mktag = ArgumentTag
    AT = ArgumentType
    AG = ArgumentGroup
    parser.add_argument("-I", '--include', action="append", default=[userCharDir(), get_ghostbox_data("chars/"), "chars/"], help="Include paths that will be searched for character folders named with the /start command or the --character_folder command line argument.",
                        tag=mktag(type=AT.Porcelain, group=AG.Characters, motd=True))
    parser.add_argument('--template_include', action="append", default=[userTemplateDir(), get_ghostbox_data("templates/"), "templates/"], help="Include paths that will be searched for prompt templates. You can specify a template to use with the -T option.",
                        tag=mktag(type=AT.Plumbing, group=AG.Templates))
    parser.add_argument("-T", '--prompt_format', type=str, default="auto", help="Prompt format template to use. The default is 'auto', which means ghostbox will .let the backend handle templating, which is usually the right choice. You can still use other settings, like 'raw', to experiment. This is ignored if you use the generic or openai backend. Note: Prompt format templates used to be more important in the early days of LLMs, as confusion was rampant and mistakes were not uncommon even in official releases. Nowadays, it is quite safe to use the official templates. You may still want to use this option for experimentation, however.",
                        tag=mktag(type=AT.Plumbing, group=AG.Templates))
    parser.add_argument("-s", "--stop", action="append", default=[], help="Forbidden strings that will stop the LLM backend generation.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation, motd=True))
    parser.add_argument("-c", '--character_folder', type=str, default="", help="character folder to load at startup. The folder may contain a `system_msg` file, a `config.json`, and a `tools.py`, as well as various other files used as file variables. See the examples and documentation for more.",
                        tag=mktag(type=AT.Porcelain, group=AG.Characters, very_important=True, motd=True))
    parser.add_argument("-p", '--prompt', type=str, default=None, help="If provided, process the prompt and exit.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation))    
    parser.add_argument("--endpoint", type=str, default="http://localhost:8080", help="Address of backend http endpoint. This is a URL that is dependent on the backend you use, though the default of localhost:8080 works for most, including Llama.cpp and Kobold.cpp. If you want to connect to an online provider that is not part of the explicitly supported backends, this is where you would supply their API address.",
                        tag=mktag(type=AT.Porcelain, group=AG.Backend))
    parser.add_argument("--backend", type=str, default=LLMBackend.generic.name, help="Backend to use. The default is `generic`, which conforms to the OpenAI REST API, and is supported by most LLM providers. Choosing a more specific backend may provide additional functionality. Other possible values are " + ", ".join([e.name for e in LLMBackend]) + ".",
                        tag=mktag(type=AT.Porcelain, group=AG.Backend, very_important=True))
    parser.add_argument("--openai_api_key", type=str, default="", help="API key for OpenAI. Without the `--backend openai` option, this has no effect.",
                        tag=mktag(type=AT.Plumbing, group=AG.OpenAI))    
    parser.add_argument("--max_length", type=int, default=300, help="Number of tokens to request from backend for generation. Generation is stopped when this number is exceeded. Negative values mean generation is unlimited and will terminate when the backend generates a stop token.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation, very_important=True))
    parser.add_argument("--max_context_length", type=int, default=32768, help="Maximum number of tokens to keep in context.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation, very_important=True))
    parser.add_argument("-u", "--chat_user", type=str, default="user", help="Username you wish to be called when chatting in 'chat' mode. It will also replace occurrences of {chat_user} anywhere in the character files. If you don't provide one here, your username will be determined by your OS session login.",
                        tag=mktag(type=AT.Porcelain, group=AG.General, very_important=True))
    parser.add_argument("-M", "--mode", type=str, default="default", help="Mode of operation. Changes various things behind-the-scenes. Values are currently 'default', or 'chat'.",
                        tag=mktag(type=AT.Plumbing, group=AG.Templates))
    parser.add_argument("--force_params", action=argparse.BooleanOptionalAction, type=bool, default=False, help="Force sending of sample parameters, even when they are seemingly not supported by the backend (use to debug or with generic",
                        tag=mktag(type=AT.Plumbing, group=AG.Backend))    
    parser.add_argument("-m", "--model", type=str, help="LLM to use for requests. This only works if the backend supports choosing models.",
                        tag=mktag(type=AT.Porcelain, group=AG.Backend, very_important=True))
    parser.add_argument("-g", "--grammar_file", type=str, default="", help="Grammar file used to restrict generation output. Grammar format is GBNF.",
                        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=True))
    parser.add_argument("--chat_ai", type=str, default="", help="Name  the AI will have when chatting. Has various effects on the prompt when chat mode is enabled. This is usually set automatically in the config.json file of a character folder.",
                        tag=mktag(type=AT.Plumbing, group=AG.Characters))
    parser.add_argument("--stream", action=argparse.BooleanOptionalAction, default=True, help="Enable streaming mode. This will print generations by the LLM piecemeal, instead of waiting for a full generation to complete. Results may be printed per-token, per-sentence, or otherwise, according to --stream_flush.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation, very_important=True))
    parser.add_argument("--http", action=argparse.BooleanOptionalAction, default=False, help="Enable a small webserver with minimal UI. By default, you'll find it at localhost:5050.",
                        tag=mktag(type=AT.Porcelain, group=AG.Interface, service=True))
    parser.add_argument("--websock", action=argparse.BooleanOptionalAction, default=False, help="Enable sending and receiving commands on a websock server running on --websock_host and --websock_port. This is enabled automatically with --http.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface, service=True, motd=True))
    parser.add_argument("--websock_host", type=str, default="localhost", help="The hostname that the websocket server binds to.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--websock_port", type=int, default=5150, help="The port that the websock server will listen on. By default, this is the http port +100.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--http_host", type=str, default="localhost", help="Hostname to bind to if --http is enabled.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--http_port", type=int, default=5050, help="Port for the web server to listen on if --http is provided. By default, the --audio_websock_port will be --http_port+1, and --tts_websock_port will be --http_port+2, e.g. 5051 and 5052.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--http_override", action=argparse.BooleanOptionalAction, default=True, help="If enabled, the values of --audio_websock, --tts_websock, --audio_websock_host, --audio_websock_port, --tts_websock_host, --tts_websock_port will be overriden if --http is provided. Use --no-http_override to disable this, so you can set your own host/port values for the websock services or disable them entirely.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))                        
    parser.add_argument("--multiline", action=argparse.BooleanOptionalAction, default=False, help="Makes multiline mode the dfault, meaning that newlines no longer trigger a message being sent to the backend. instead, you must enter the value of --multiline_delimiter to trigger a send.",
                        tag=mktag(type=AT.Porcelain, group=AG.Interface))
    parser.add_argument("--multiline_delimiter", type=str, default="\\", help="String that signifies the end of user input. This is only relevant for when --multiline is enabled. By default this is a backslash, inverting the normal behaviour of backslashes allowing to enter a newline ad-hoc while in multiline mode. This option is intended to be used by scripts to change the delimiter to something less common.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--color", action=argparse.BooleanOptionalAction, default=True, help="Enable colored output.",
                        tag=mktag(type=AT.Porcelain, group=AG.Interface, motd=True))
    parser.add_argument("--text_ai_color", type=str, default="none", help="Color for the generated text, as long as --color is enabled. Most ANSI terminal colors are supported.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface, motd=True))
    parser.add_argument("--text_ai_style", type=str, default="bright", help="Style for the generated text, as long as --color is enabled. Most ANSI terminal styles are supported.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--dynamic_file_vars", action=argparse.BooleanOptionalAction, default=True, help="Dynamic file vars are strings of the form {[FILE1]}. If FILE1 is found, the entire expression is replaced with the contents of FILE1. This is dynamic in the sense that the contents of FILE1 are loaded each time the replacement is encountered, which is different from the normal file vars with {{FILENAME}}, which are loaded once during character initialization. Replacement happens in user inputs only. In particular, dynamic file vars are ignored in system messages or saved chats. If you want the LLM to get file contents, use tools. disabling this means no replacement happens. This can be a security vulnerability, so it is disabled by default on the API.",
                        tag=mktag(type=AT.Plumbing, group=AG.Templates, motd=True))
    parser.add_argument("--warn_trailing_space", action=argparse.BooleanOptionalAction, default=True, help="Warn if the prompt that is sent to the backend ends on a space. This can cause e.g. excessive emoticon use by the model.",
                        tag=mktag(type=AT.Plumbing, group=AG.Generation))
    parser.add_argument("--warn_unsupported_sampling_parameter", action=argparse.BooleanOptionalAction, default=True, help="Warn if you have set an option that is usually considered a sampling parameter, but happens to be not supported by the chose nbackend.",
                        tag=mktag(type=AT.Plumbing, group=AG.SamplingParameters))
    parser.add_argument("--warn_audio_activation_phrase", action=argparse.BooleanOptionalAction, default=True, help="Warn if audio is being transcribed, but no activation phrase is found. Normally this only will warn once. Set to -1 if you want to be warned every time.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))
    parser.add_argument("--warn_hint", action=argparse.BooleanOptionalAction, default=True, help="Warn if you have a hint set.",
                        tag=mktag(type=AT.Plumbing, group=AG.Generation))
    parser.add_argument("--json", action=argparse.BooleanOptionalAction, default=False, help="Force generation output to be in JSON format. This is equivalent to using -g with a json.gbnf grammar file, but this option is provided for convenience.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation, motd=True)) 
    parser.add_argument("--stream_flush", type=str, default="token", help="When to flush the streaming buffer. When set to 'token', will print each token immediately. When set to 'sentence', it will wait for a complete sentence before printing. This can be useful for TTS software. Default is 'token'.",
                        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=True))
    parser.add_argument("--cli_prompt", type=str, default=" {{current_tokens}} > ", help="String to show at the bottom as command prompt. Can be empty.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface, motd=True))
    parser.add_argument("--cli_prompt_color", type=str, default="none", help="Color of the prompt. Uses names of standard ANSI terminal colors. Requires --color to be enabled.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--hint", type=str, default="", help="Hint for the AI. This string will be appended to the prompt behind the scenes. It's the first thing the AI sees. Try setting it to 'Of course,' to get a more compliant AI. Also refered to as 'prefill'.",
                        tag=mktag(type=AT.Porcelain, group=AG.Generation))
    parser.add_argument("--hint_sticky", action=argparse.BooleanOptionalAction, default=True, help="If disabled, hint will be shown to the AI as part of prompt, but will be omitted from the story.",
                        tag=mktag(type=AT.Plumbing, group=AG.Generation))
    parser.add_argument("--tts", action=argparse.BooleanOptionalAction, default=False, help="Enable text to speech on generated text.",
                        tag=mktag(type=AT.Porcelain, group=AG.TTS, very_important=True, service=True))
    parser.add_argument("--tts_model", type=str, default="kokoro", help="The TTS model to use. This is ignored unless you use ghostbox-tts as your tts_program.",
                        tag=mktag(type=AT.Porcelain, group=AG.TTS))
    parser.add_argument("--tts_output_method", type=str, choices=[om.name for om in TTSOutputMethod], default=TTSOutputMethod.default.name, help="How to play the generated speech. Using the --http argument automatically sets this to websock.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))
    parser.add_argument("--tts_websock", action=argparse.BooleanOptionalAction, default=False, help="Enable websock as the output method for TTS. This is equivalent to `--tts_output_method websock`.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))
    parser.add_argument("--tts_websock_host", type=str, default="localhost", help="The address to bind to for the underlying TTS program when using websock as output method. ghostbox-tts only. This option is normally overriden by --http.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_websock_port", type=int, default=5052, help="The port to listen on for the underlying TTS program when using websock as output method. ghostbox-tts only. This option is normally overriden by --http.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_interrupt", action=argparse.BooleanOptionalAction, default=True, help="Stop an ongoing TTS whenever a new generation is spoken. When set to false, will queue messages instead.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_program", type=str, default="ghostbox-tts", help="Path to a TTS (Text-to-speech) program to verbalize generated text. The TTS program should read lines from standard input. Many examples are provided in scripts/ghostbox-tts-* . The ghostbox-tts script offers a native solution using various supported models.",
                        tag=mktag(type=AT.Porcelain, group=AG.TTS))                        
    parser.add_argument("--tts_voice_dir", type=str, default="voices", help="Directory to check first for voice file. Note: This doesn't currently work with all TTS engines, as some don't use files for voices.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_tortoise_quality", type=str, default="fast", help="Quality preset. tortoise-tts only. Can be 'ultra_fast', 'fast' (default), 'standard', or 'high_quality'",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_volume", type=float, default=1.0, help="Volume for TTS voice program. Is passed to tts_program as environment variable.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_rate", type=int, default=50, help="Speaking rate for TTS voice program. Is passed to tts_program as environment variable. Note that speaking rate is not supported by all TTS engines.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--tts_additional_arguments", type=str, default="", help="Additional command line arguments that will be passed to the tts_program.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                        
    parser.add_argument("--image_watch", action=argparse.BooleanOptionalAction, default=False, help="Enable watching of a directory for new images. If a new image appears in the folder, the image will be loaded with id 0 and sent to the backend. works with multimodal models only (like llava).",
                        tag=mktag(type=AT.Porcelain, group=AG.Images, service=True, motd=True))
    parser.add_argument("--image_watch_dir", type=str, default=os.path.expanduser("~/Pictures/Screenshots/"), help="Directory that will be watched for new image files when --image_watch is enabled.",
                        tag=mktag(type=AT.Plumbing, group=AG.Images))
    parser.add_argument("--image_watch_msg", type=str, default="Can you describe this image?", help="If image_watch is enabled, this message will be automatically send to the backend whenever a new image is detected. Set this to '' to disable automatic messages, while still keeping the automatic update the image with id 0.",
                        tag=mktag(type=AT.Plumbing, group=AG.Images, motd=True))                        
    parser.add_argument("--image_watch_hint", type=str, default="", help="If image_watch is enabled, this string will be sent to the backend as start of the AI response whenever a new image is detected and automatically described. This allows you to guide or solicit the AI by setting it to e.g. 'Of course, this image show' or similar. Default is ''",
                        tag=mktag(type=AT.Plumbing, group=AG.Images, motd=True))                        
    parser.add_argument("--whisper_model", type=str, default="base.en", help="Name of the model to use for transcriptions using the openai whisper model. Default is 'base.en'. For a list of model names, see https://huggingface.co/openai/whisper-large",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("-y", "--tts_voice", type=str, default="random", help="Voice file to use for TTS. Default is 'random', which is a special value that picks a random available voice for your chosen tts_program. The value of tts_voice will be changed at startup if random is chosen, so when you find a voice you like you can find out its name with /lsoptions and checking tts_voice.",
                        tag=mktag(type=AT.Porcelain, group=AG.TTS))                                                
    parser.add_argument("--tts_subtitles", action=argparse.BooleanOptionalAction, default=True, help="Enable printing of generated text while TTS is enabled.",
                        tag=mktag(type=AT.Plumbing, group=AG.TTS))                                                                        
    parser.add_argument("--config_file", type=str, default="", help="Path to a config fail in JSON format, containing a dictionary with OPTION : VALUE pairs to be loaded on startup. Same as /loadconfig or /loadoptions. To produce an example config-file, try /saveconfig example.json.",
                        tag=mktag(type=AT.Porcelain, group=AG.General))
    parser.add_argument("--chat_show_ai_prompt", action=argparse.BooleanOptionalAction, default=True, help="Controls wether to show AI prompt in chat mode. Specifically, assuming chat_ai = 'Bob', setting chat_show_ai_prompt to True will show 'Bob: ' in front of the AI's responses. Note that this is always sent to the back-end (in chat mode), this parameter merely controls wether it is shown.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--smart_context", action=argparse.BooleanOptionalAction, default=True, help="Enables ghostbox version of smart context, which means dropping text at user message boundaries when the backend's context is exceeded. If you disable this, it usually means the backend will truncate the raw message. Enabling smart context means better responses and longer processing time due to cache invalidation, disabling it means worse responses with faster processing time. Note from marius: Beware I haven't looked at this in a while, since newer models all have very large contexts.",
                        tag=mktag(type=AT.Plumbing, group=AG.Generation, motd=True))
    parser.add_argument("--hide", action=argparse.BooleanOptionalAction, default=False, help="Hides some unnecessary output, providing a more immersive experience. Same as typing /hide.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface, motd=True))
    parser.add_argument("--audio", action=argparse.BooleanOptionalAction, default=False, help="Enable automatic transcription of audio input using openai whisper model. Obviously, you need a mic for this.",
                        tag=mktag(type=AT.Porcelain, group=AG.Audio, service=True, very_important=True))
    parser.add_argument("--audio_silence_threshold", type=int, default=2000, help="An integer value denoting the threshold for when automatic audio transcription starts recording. (default 2000)",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio, motd=True))
    parser.add_argument("--audio_activation_phrase", type=str, default="", help="When set, the phrase must be detected in the beginning of recorded audio, or the recording will be ignored. Phrase matching is fuzzy with punctuation removed.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio, motd=True))                    
    parser.add_argument("--audio_activation_period_ms", type=int, default=0, help="Period in milliseconds where no further activation phrase is necessary to trigger a response. The period starts after any interaction with the AI, spoken or otherwise.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))
    parser.add_argument("--audio_interrupt", action=argparse.BooleanOptionalAction, default=True, help="Stops generation and TTS  when you start speaking. Does not require activation phrase.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("--audio_activation_phrase_keep", action=argparse.BooleanOptionalAction, default=True, help="If false and an activation phrase is set, the triggering phrase will be removed in messages that are sent to the backend.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("--audio_show_transcript", action=argparse.BooleanOptionalAction, default=True, help="Show transcript of recorded user speech when kaudio transcribing is enabled. When disabled, you can still see the full transcript with /log or /print.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("--audio_websock", action=argparse.BooleanOptionalAction, default=False, help="Enable to listen for audio on an HTTP websocket at the given `--websock_url`, instead of recording audio from a microphone. This can be used to stream audio through a website. This is enabled by default with the --http option unless you also supply --no-http_overrid.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("--audio_websock_host", type=str, default="localhost", help="The address to bind to when `--audio_websock` is enabled. You can stream audio to this endpoint using the websocket protocol for audio transcription. Normally overriden by --http.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("--audio_websock_port", type=int, default=5051, help="The port to listen on when `--audio_websock` is enabled. You can stream audio to this endpoint using the websocket protocol for audio transcription. Normally overriden by --http.",
                        tag=mktag(type=AT.Plumbing, group=AG.Audio))                        
    parser.add_argument("--verbose", action=argparse.BooleanOptionalAction, default=False, help="Show additional output for various things.",
                        tag=mktag(type=AT.Plumbing, group=AG.General))
    parser.add_argument("--log_time", action=argparse.BooleanOptionalAction, default=False, help="Print timing and performance statistics to stderr with every generation. Auto enabled for the API.",
                        tag=mktag(type=AT.Plumbing, group=AG.General))
    parser.add_argument("-q", "--quiet", action=argparse.BooleanOptionalAction, default=False, help="Prevents printing and TTS vocalization of generations. Often used with the API when you want to handle generation results yourself and don't want printing to console.",
                        tag=mktag(type=AT.Porcelain, group=AG.Interface))
    parser.add_argument("--stderr", action=argparse.BooleanOptionalAction, default=True, help="Wether printing to stderr is enabled. You may want to disable this when building terminal applications using the API.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))    
    parser.add_argument("--expand_user_input", action=argparse.BooleanOptionalAction, default=True, help="Expand variables in user input. E.g. {$var} will be replaced with content of var. Variables are initialized from character folders (i.e. file 'memory' will be {$memory}), or can be set manually with the /varfile command or --varfile option. See also --dynamic_file_vars.",
                        tag=mktag(type=AT.Plumbing, group=AG.Interface))
    parser.add_argument("--tools_unprotected_shell_access", action=argparse.BooleanOptionalAction, default=False, help="Allow an AI to run shell commands, even if not logged in to their own account. The safe way of doing this is to create an account on your system with the same name as the AI, and then run this program under their account. If you don't want to do that, and you are ok with an AI deleting your files through accident or malice, set this flag to true.",
                        tag=mktag(type=AT.Plumbing, group=AG.Tools))
    #parser.add_argument("--tools_reflection", action=argparse.BooleanOptionalAction, default=True, help="Continue generation after tools have been applied. This allows the AI to reflect on the results, e.g. summarize a file it retrieved. Only applies when use_tools is true.",
                        #tag=mktag(type=AT.Plumbing, group=AG.Tools))
    parser.add_argument("-d", "--tools_forbidden", action=argparse.BooleanOptionalAction, default=["List", "Dict", "launch_nukes"] , help="Blacklist certain tools. Specify multiple times to forbid several tools. The default blacklist contains some common module imports that can pollute a tools.py namespace. You can override this in a character folders config.json if necessary.",
                        tag=mktag(type=AT.Plumbing, group=AG.Tools, very_important=True, motd=True))                        
    parser.add_argument("--tools_hint", type=str, default="", help="Text that will be appended to the system prompt when use_tools is true.",
                        tag=mktag(type=AT.Plumbing, group=AG.Tools))                        
    parser.add_argument("--tools_inject_dependency_function", type=str, default="", help="API only. Set a callback function to be called whenever an tool-using Ai is initialized. The callback will receive one argument: The tools.py module. You can use this to inject dependency or modify the module after it is loaded.",
                        tag=mktag(type=AT.Plumbing, group=AG.Tools, motd=True))
    parser.add_argument("--tools_inject_ghostbox", action=argparse.BooleanOptionalAction, default=True, help="Inject a reference to ghostbox itself into an AI's tool module. This will make the '_ghostbox_plumbing' identifier available in the tools module and point it to the running ghostbox Plumbing instance. Disabling this will break many of the standard AI tools that ship with ghostbox.",
                        tag=mktag(type=AT.Plumbing, group=AG.Tools))                            
    parser.add_argument("--use_tools", action=argparse.BooleanOptionalAction, default=False, help="Enable use of tools, i.e. model may call python functions. This will do nothing if tools.py isn't present in the char directory. If tools.py is found, this will be automatically enabled.",
                        tag=mktag(type=AT.Porcelain, group=AG.Tools, very_important=True, motd=True))                        
    parser.add_argument("-x", '--var_file', action="append", default=[], help="Files that will be added to the list of variables that can be expanded. E.g. -Vmemory means {$memory} will be expanded to the contents of file memory, provided expand_user_input is set. Can be used to override values set in character folders. Instead of using this, you can also just type {[FILENAME]} to have it be automatically expanded with the contents of FILENAME, provided --dynamic_file_vars is enabled.",
                        tag=mktag(type=AT.Porcelain, group=AG.Interface, motd=True))


    # don't show all possible parameters on command line, but do show some
    params = backends.supported_parameters | backends.sometimes_parameters
    for name, param in params.items():
        # we need to avoid some redefinitions
        if name in "stop max_length".split(" "):
            continue
        # others require special treatment
        if name == "samplers":
            parser.add_argument("-S", "--" + param.name, nargs="*", default=param.default_value, help=param.description,
                                tag=mktag(type=AT.Plumbing, group=AG.SamplingParameters, very_important=True, motd=True, is_option=True))
            continue
            
        parser.add_argument("--" + param.name, type=type(param.default_value), default=param.default_value, help=param.description,
                            tag=mktag(type=AT.Plumbing, group=AG.SamplingParameters, is_option=True))
    return parser


def makeDefaultOptions():
    """Returns a pair of default options and tags."""
    tp = makeTaggedParser(backends.default_params)
    parser = tp.get_parser()
    return parser.parse_args(args=""), tp.get_tags()

```

## autoimage.py

```python
import os, appdirs, threading, time, sys
from ghostbox.util import *
import operator
from stat import ST_MTIME

def mostRecentFile(path):
    """Returns pair of most recent file in directory PATH, as well as its modification date. Returns ("", 0) if path is not a directory."""
    if not(os.path.isdir(path)):
        return ("", 0)
    
    
    all_files = os.listdir(path);
    file_mtimes = dict();
    for file in all_files:
        e = path + "/" + file
        file_mtimes[e] = time.time() - os.stat(e).st_mtime;
    if not(file_mtimes):
        return ("", 0)
    winner =  sorted(file_mtimes.items(), key=operator.itemgetter(1))[0][0]
    return (winner, os.stat(winner).st_mtime)




class AutoImageProvider(object):
    """When instanciated, takes a directory and scans it periodically for a new image file. Once a new file is detected, the AutoImageProvider executes a callback with the image data as argument.
This allows people to take screenshots and have them automatically be described, without having to further input anything."""
    def __init__(self, watch_dir, on_new_image_func, update_period=0.25, image_id=0):
        """watch_dir - Directory that will be periodically checked for new images.
on_new_image_func - Function that takes one argument, a dictionary with keys 'data' and 'id'. This is in the format expected by Llamacpp.
        update_period - How often to check the directory.
image_id - The id of the image. This is relevant as it will inform the tokens used to invoke the image, e.g. with id=0 it would be '[img-0]'."""
        self.watch_dir = watch_dir
        self.update_period = update_period
        self.image_id = image_id
        self.callback = on_new_image_func
        self.latestFile = mostRecentFile(self.watch_dir) # this is pair (filename, modtime)
        self.running = False
        self._initWatchLoop()

    def _initWatchLoop(self):
        self.running = True
        t = threading.Thread(target=self._watchLoop, args=[])
        t.start()

    def stop(self):
        self.running = False
            
    def _watchLoop(self):
        while self.running:
            if not(os.path.isdir(self.watch_dir)):
                printerr("warning: AutoImageProvider cannot watch directory '" + self.watch_dir + "': Not a directory. Halting watch loop.")
                self.stop()
            else:
                (file, modtime) = mostRecentFile(self.watch_dir)
                if modtime > self.latestFile[1] and isImageFile(file):
                    self.latestFile = (file, modtime)
                    self.invokeCallback()
            time.sleep(self.update_period)
            
    def invokeCallback(self):
        (file, time) = self.latestFile
        self.callback(file, self.image_id)
                

```

## backends.py

```python
import time, requests, threading
from abc import ABC, abstractmethod
from functools import *
from pydantic import BaseModel
from ghostbox.util import *
from ghostbox.definitions import *
from ghostbox.streaming import *

# this list is based on the llamacpp server. IMO most other backends are subsets of this.
# the sampler values have been adjusted to more sane default options (no more top_p)
sampling_parameters = {
    "temperature": SamplingParameterSpec(
        name="temperature",
        description="Adjust the randomness of the generated text.",
        default_value=0.8,
    ),
    "dynatemp_range": SamplingParameterSpec(
        name="dynatemp_range",
        description="Dynamic temperature range. The final temperature will be in the range of `[temperature - dynatemp_range; temperature + dynatemp_range]`",
        default_value=0.0,
    ),
    "dynatemp_exponent": SamplingParameterSpec(
        name="dynatemp_exponent",
        description="Dynamic temperature exponent.",
        default_value=1.0,
    ),
    "top_k": SamplingParameterSpec(
        name="top_k",
        description="Limit the next token selection to the K most probable tokens.",
        default_value=40,
    ),
    "top_p": SamplingParameterSpec(
        name="top_p",
        description="Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P.",
        default_value=0.95,
    ),
    "min_p": SamplingParameterSpec(
        name="min_p",
        description="The minimum probability for a token to be considered, relative to the probability of the most likely token.",
        default_value=0.05,
    ),
    # this one is renamed from n_predict
    "max_length": SamplingParameterSpec(
        name="max_length",
        description="Set the maximum number of tokens to predict when generating text. **Note:** May exceed the set limit slightly if the last token is a partial multibyte character. When 0, no tokens will be generated but the prompt is evaluated into the cache.",
        default_value=-1,
    ),
    "n_indent": SamplingParameterSpec(
        name="n_indent",
        description="Specify the minimum line indentation for the generated text in number of whitespace characters. Useful for code completion tasks.",
        default_value=0,
    ),
    "n_keep": SamplingParameterSpec(
        name="n_keep",
        description="Specify the number of tokens from the prompt to retain when the context size is exceeded and tokens need to be discarded. The number excludes the BOS token. By default, this value is set to `0`, meaning no tokens are kept. Use `-1` to retain all tokens from the prompt.",
        default_value=0,
    ),
    #    "stream": SamplingParameterSpec(
    #        name="stream",
    #        description="Allows receiving each predicted token in real-time instead of waiting for the completion to finish (uses a different response format). To enable this, set to `true`.",
    #        default_value=False
    #    ),
    "stop": SamplingParameterSpec(
        name="stop",
        description="Specify a JSON array of stopping strings. These words will not be included in the completion, so make sure to add them to the prompt for the next iteration.",
        default_value=[],
    ),
    "typical_p": SamplingParameterSpec(
        name="typical_p",
        description="Enable locally typical sampling with parameter p.",
        default_value=1.0,
    ),
    "repeat_penalty": SamplingParameterSpec(
        name="repeat_penalty",
        description="Control the repetition of token sequences in the generated text.",
        default_value=1.1,
    ),
    "repeat_last_n": SamplingParameterSpec(
        name="repeat_last_n",
        description="Last n tokens to consider for penalizing repetition.",
        default_value=64,
    ),
    "presence_penalty": SamplingParameterSpec(
        name="presence_penalty",
        description="Repeat alpha presence penalty.",
        default_value=0.0,
    ),
    "frequency_penalty": SamplingParameterSpec(
        name="frequency_penalty",
        description="Repeat alpha frequency penalty.",
        default_value=0.0,
    ),
    "dry_multiplier": SamplingParameterSpec(
        name="dry_multiplier",
        description="Set the DRY (Don't Repeat Yourself) repetition penalty multiplier.",
        default_value=0.8,
    ),
    "dry_base": SamplingParameterSpec(
        name="dry_base",
        description="Set the DRY repetition penalty base value.",
        default_value=1.75,
    ),
    "dry_allowed_length": SamplingParameterSpec(
        name="dry_allowed_length",
        description="Tokens that extend repetition beyond this receive exponentially increasing penalty: multiplier * base ^ (length of repeating sequence before token - allowed length).",
        default_value=2,
    ),
    "dry_penalty_last_n": SamplingParameterSpec(
        name="dry_penalty_last_n",
        description="How many tokens to scan for repetitions.",
        default_value=-1,
    ),
    "dry_sequence_breakers": SamplingParameterSpec(
        name="dry_sequence_breakers",
        description="Specify an array of sequence breakers for DRY sampling. Only a JSON array of strings is accepted.",
        default_value=["\n", ":", '"', "*"],
    ),
    "xtc_probability": SamplingParameterSpec(
        name="xtc_probability",
        description="Set the chance for token removal via XTC sampler.\nXTC means 'exclude top choices'. This sampler, when it triggers, removes all but one tokens above a given probability threshold. Recommended for creative tasks, as language tends to become less stereotypical, but can make a model less effective at structured output or intelligence-based tasks.\nSee original xtc PR by its inventor https://github.com/oobabooga/text-generation-webui/pull/6335",
        default_value=0.5,
    ),
    "xtc_threshold": SamplingParameterSpec(
        name="xtc_threshold",
        description="Set a minimum probability threshold for tokens to be removed via XTC sampler.\nXTC means 'exclude top choices'. This sampler, when it triggers, removes all but one tokens above a given probability threshold. Recommended for creative tasks, as language tends to become less stereotypical, but can make a model less effective at structured output or intelligence-based tasks.\nSee original xtc PR by its inventor https://github.com/oobabooga/text-generation-webui/pull/6335",
        default_value=0.1,
    ),
    "mirostat": SamplingParameterSpec(
        name="mirostat",
        description="Enable Mirostat sampling, controlling perplexity during text generation.",
        default_value=0,
    ),
    "mirostat_tau": SamplingParameterSpec(
        name="mirostat_tau",
        description="Set the Mirostat target entropy, parameter tau.",
        default_value=5.0,
    ),
    "mirostat_eta": SamplingParameterSpec(
        name="mirostat_eta",
        description="Set the Mirostat learning rate, parameter eta.",
        default_value=0.1,
    ),
    "grammar": SamplingParameterSpec(
        name="grammar",
        description="Set grammar for grammar-based sampling.",
        default_value=None,
    ),
    "json_schema": SamplingParameterSpec(
        name="json_schema",
        description='Set a JSON schema for grammar-based sampling (e.g. `{"items": {"type": "string"}, "minItems": 10, "maxItems": 100}` of a list of strings, or `{}` for any JSON). See [tests](../../tests/test-json-schema-to-grammar.cpp) for supported features.',
        default_value=None,
    ),
    "seed": SamplingParameterSpec(
        name="seed",
        description="Set the random number generator (RNG) seed.",
        default_value=-1,
    ),
    "ignore_eos": SamplingParameterSpec(
        name="ignore_eos",
        description="Ignore end of stream token and continue generating.",
        default_value=False,
    ),
    "logit_bias": SamplingParameterSpec(
        name="logit_bias",
        description='Modify the likelihood of a token appearing in the generated text completion. For example, use `"logit_bias": [[15043,1.0]]` to increase the likelihood of the token \'Hello\', or `"logit_bias": [[15043,-1.0]]` to decrease its likelihood. Setting the value to false, `"logit_bias": [[15043,false]]` ensures that the token `Hello` is never produced. The tokens can also be represented as strings, e.g. `[["Hello, World!",-0.5]]` will reduce the likelihood of all the individual tokens that represent the string `Hello, World!`, just like the `presence_penalty` does.',
        default_value=[],
    ),
    "n_probs": SamplingParameterSpec(
        name="n_probs",
        description="If greater than 0, the response also contains the probabilities of top N tokens for each generated token given the sampling settings. Note that for temperature < 0 the tokens are sampled greedily but token probabilities are still being calculated via a simple softmax of the logits without considering any other sampler settings.",
        default_value=0,
    ),
    "min_keep": SamplingParameterSpec(
        name="min_keep",
        description="If greater than 0, force samplers to return N possible tokens at minimum.",
        default_value=0,
    ),
    "t_max_predict_ms": SamplingParameterSpec(
        name="t_max_predict_ms",
        description="Set a time limit in milliseconds for the prediction (a.k.a. text-generation) phase. The timeout will trigger if the generation takes more than the specified time (measured since the first token was generated) and if a new-line character has already been generated. Useful for FIM applications.",
        default_value=0,
    ),
    # this doesn't even work in llama
    #    "image_data": SamplingParameterSpec(
    #        name="image_data",
    #        description="An array of objects to hold base64-encoded image `data` and its `id`s to be reference in `prompt`. You can determine the place of the image in the prompt as in the following: `USER:[img-12]Describe the image in detail.\nASSISTANT:`. In this case, `[img-12]` will be replaced by the embeddings of the image with id `12` in the following `image_data` array: `{..., \"image_data\": [{\"data\": \"<BASE64_STRING>\", \"id\": 12}]}`. Use `image_data` only with multimodal models, e.g., LLaVA.",
    #        default_value=[]
    #    ),
    "id_slot": SamplingParameterSpec(
        name="id_slot",
        description="Assign the completion task to an specific slot. If is -1 the task will be assigned to a Idle slot.",
        default_value=-1,
    ),
    "cache_prompt": SamplingParameterSpec(
        name="cache_prompt",
        description="Re-use KV cache from a previous request if possible. This way the common prefix does not have to be re-processed, only the suffix that differs between the requests. Because (depending on the backend) the logits are **not** guaranteed to be bit-for-bit identical for different batch sizes (prompt processing vs. token generation) enabling this option can cause nondeterministic results.",
        default_value=True,
    ),
    "return_tokens": SamplingParameterSpec(
        name="return_tokens",
        description="Return the raw generated token ids in the `tokens` field. Otherwise `tokens` remains empty.",
        default_value=False,
    ),
    "samplers": SamplingParameterSpec(
        name="samplers",
        description="The order the samplers should be applied in. An array of strings representing sampler type names. If a sampler is not set, it will not be used. If a sampler is specified more than once, it will be applied multiple times.",
        default_value=["min_p", "xtc", "dry", "temperature"],
    ),
    "timings_per_token": SamplingParameterSpec(
        name="timings_per_token",
        description="Include prompt processing and text generation speed information in each response.",
        default_value=False,
    ),
    "post_sampling_probs": SamplingParameterSpec(
        name="post_sampling_probs",
        description="Returns the probabilities of top `n_probs` tokens after applying sampling chain.",
        default_value=None,
    ),
    "response_fields": SamplingParameterSpec(
        name="response_fields",
        description='A list of response fields, for example: `"response_fields": ["content", "generation_settings/n_predict"]`. If the specified field is missing, it will simply be omitted from the response without triggering an error. Note that fields with a slash will be unnested; for example, `generation_settings/n_predict` will move the field `n_predict` from the `generation_settings` object to the root of the response and give it a new name.',
        default_value=None,
    ),
    "lora": SamplingParameterSpec(
        name="lora",
        description='A list of LoRA adapters to be applied to this specific request. Each object in the list must contain `id` and `scale` fields. For example: `[{"id": 0, "scale": 0.5}, {"id": 1, "scale": 1.1}]`. If a LoRA adapter is not specified in the list, its scale will default to `0.0`. Please note that requests with different LoRA configurations will not be batched together, which may result in performance degradation.',
        default_value=[],
    ),
    # new - only  got this from the git logs
    "add_generation_prompt": SamplingParameterSpec(
        name="add_generation_prompt",
        description="Include the prompt used to generate in the result.",
        default_value=True,
    ),
}
### end of big list

# this is for fast copy and send to backend
default_params = {hp.name: hp.default_value for hp in sampling_parameters.values()}

# some reference lists for convenience
supported_parameters = {
    p: sampling_parameters[p]
    for p in "temperature frequency_penalty presence_penalty max_length repeat_penalty top_p stop".split(
        " "
    )
}
sometimes_parameters = {
    p: sampling_parameters[p]
    for p in "cache_prompt xtc_probability dry_multiplier min_p mirostat mirostat_tau mirostat_eta samplers".split(
        " "
    )
}
sampling_parameter_tags = {
    p.name: ArgumentTag(
        name=p.name,
        type=ArgumentType.Plumbing,
        group=ArgumentGroup.SamplingParameters,
        is_option=True,
        default_value=p.default_value,
        help=p.description,
    )
    for p in sampling_parameters.values()
}
# some special ones
for p in supported_parameters.keys():
    sampling_parameter_tags[p].very_important = True

sampling_parameter_tags["temperature"].type = ArgumentType.Porcelain
sampling_parameter_tags["top_p"].type = ArgumentType.Porcelain


# These don't fit anywhere else and don't really need documentation
special_parameters = {"response_format": {"type": "text"}}


class AIBackend(ABC):
    """Abstract interface to a backend server, like llama.cpp or openai etc. Use the Program.make*Payload methods to make corresponding payloads. All backends must handle those dictionaries, but not all backends may support all features."""

    @abstractmethod
    def __init__(self, endpoint):
        self.endpoint = endpoint
        self.stream_done = threading.Event()
        self.last_error = ""
        self._last_request = {}
        self._last_result = {}
        self._config = {}

    def configure(self, config: Dict[str, Any] = {}) -> Dict[str, Any]:
        """Configure backend specific options thourhg a key -> value dictionary. Returns the current config of a backend.
        To see a list of possible keys, consult the backend specific configure method's documentation, or call this method without arguments. Passing keys to backends that do not support them will have no effect but is otherwise safe.
        Note that this is *not* the way to set various backend server options, like cache_prompt or temperature etc. Those should go in the payload.
        """
        self._config |= config
        return self._config

    def getLastError(self):
        return self.last_error

    def getLastJSON(self) -> Dict:
        """Returns the last json result sent by the backend."""
        return self._last_result

    def getLastRequest(self) -> Dict:
        """Returns the last payload dictionary that was sent to the server."""
        return self._last_request

    def waitForStream(self):
        self.stream_done.wait()

    @abstractmethod
    def getName(self):
        """Returns the name of the backend. This can be compared to the --backend command line option."""
        pass

    @abstractmethod
    def getMaxContextLength(self):
        """Returns the default setting for maximum context length that is set serverside. Often, this should be the maximum context the model is trained on.
        This should return -1 if the backend is unable to determine the maximum context (some backends don't let you query this at all).
        """
        pass

    @abstractmethod
    def generate(self, payload):
        """Takes a payload dictionary similar to default_params. Returns a result object specific to the backend. Use handleResult to unpack its content."""
        pass

    @abstractmethod
    def handleGenerateResult(self, result) -> Optional[Dict]:
        """Takes a result from the generate method and returns the generated string."""
        pass

    @abstractmethod
    def generateStreaming(self, payload, callback=lambda w: print(w)):
        """Takes a payload dictionary similar to default_params and begins streaming generated tokens to a callback function. Returns True if there was a HTTP error, which you can check with getLastError().
        callback - A function taking one string argument, which will be the generated tokens.
        payload - A dictionary similar to default_params. If this doesn't contain "stream" : True, this function may fail or have no effect.
        returns - True on status code != 200"""
        pass

    @abstractmethod
    def tokenize(self, w):
        """Takes a string w and returns a list of tokens as ints."""
        pass

    @abstractmethod
    def detokenize(self, ts):
        """Takes a list of tokens as ints, and returns a string consisting of the tokens."""
        pass

    @abstractmethod
    def health(self):
        """Returns a string indicating the status of the backend."""
        pass

    @abstractmethod
    def timings(self, result_json=None) -> Optional[Timings]:
        """Returns performance statistics for this backend.
        The method can take a json parameter, which may be a return value of the getLastJSON method. In this case, timings for that result are returned.
        Otherwise, if called without the json parameter, timings for the last request are returned.
        The method may return none if there hasn't been a request yet to determine timings for and no json is provided.
        """
        pass

    @abstractmethod
    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        """Returns a dictionary of sampling_parameters that are supported by the model.
        The dictionary has the parameter names as keys. If a sampling_parameter is present in the dict, it is expected to be supported by the various generation methods.
        """
        pass


class LlamaCPPBackend(AIBackend):
    """Bindings for the formidable Llama.cpp based llama-server program."""

    def __init__(self, endpoint="http://localhost:8080"):
        super().__init__(endpoint)
        self._config |= {
            # this means we will use /chat/completions, which applies the jinja templates etc. This is most often what we want.
            # if this option is false, the generate methods will use the /completions endpoint, which requires us to apply our own templates, which is great for experimentation.
            "llamacpp_use_chat_completion_endpoint": True
        }

    def getName(self):
        return LLMBackend.llamacpp.name

    def getMaxContextLength(self):
        return -1

    def generate(self, payload):
        super().generate(payload)
        # adjust slightly for our renames
        llama_payload = payload | {"n_predict": payload["max_length"]}

        if self._config["llamacpp_use_chat_completion_endpoint"]:
            endpoint_suffix = "/chat/completions"
            # /chat/completions expects a more OAI like payload
            llama_payload |= OpenAIBackend.dataFromPayload(llama_payload)
        else:
            endpoint_suffix = "/completion"
            if "tools" in payload:
                printerr(
                    "warning: Tool use with a custom prompt_format and using llama.cpp backend is currently experimental. Set your prompt_format to 'auto' or use the generic backend for a stable experience."
                )

        if "tools" in llama_payload:
            # FIXME: this is because using tools seems to invalidate the cache in llamacpp. probably because they are putting tool instructions in the system prompt. this is an attempt to fix or at least mitigate that.
            # i.e. we can just cache the inevitable streaming, non-tool generation that follows tool use.
            # however this will still suck for multi-turn tool use
            llama_payload |= {"cache_prompt": False}
        self._last_request = llama_payload
        return requests.post(self.endpoint + endpoint_suffix, json=llama_payload)

    def handleGenerateResult(self, result):
        if result.status_code != 200:
            self.last_error = "HTTP request with status code " + str(result.status_code)
            return None
        self._last_result = result.json()

        if self._config["llamacpp_use_chat_completion_endpoint"]:
            # this one wants more oai like results
            return OpenAIBackend.handleGenerateResultOpenAI(result.json())

        # handling the /completion endpoint
        if (payload := result.json()["content"]) is not None:
            return payload
        if (payload := result.json().get("tool_calls", None)) is not None:
            return payload
        return None

    def _makeLlamaCallback(self, callback):
        def f(d):
            if d["stop"]:
                self._last_result = d
            callback(d["content"])

        return f

    def generateStreaming(self, payload, callback=lambda w: print(w)):
        self.stream_done.clear()
        llama_payload = payload | {"n_predict": payload["max_length"], "stream": True}

        def one_line_lambdas_for_python(r):
            # thanks guido
            self._last_result = r

        if self._config["llamacpp_use_chat_completion_endpoint"]:
            endpoint_suffix = "/chat/completions"
            # /chat/completions expects a more OAI like payload
            llama_payload |= OpenAIBackend.dataFromPayload(llama_payload)
            final_callback = OpenAIBackend.makeOpenAICallback(
                callback, last_result_callback=one_line_lambdas_for_python
            )
        else:
            endpoint_suffix = "/completion"
            final_callback = self._makeLlamaCallback(callback)

        self._last_request = llama_payload

        r = streamPrompt(
            final_callback,
            self.stream_done,
            self.endpoint + endpoint_suffix,
            llama_payload,
        )
        if r.status_code != 200:
            self.last_error = "streaming HTTP request with status code " + str(
                r.status_code
            )
            self.stream_done.set()
            return True
        return False

    def tokenize(self, w):
        r = requests.post(self.endpoint + "/tokenize", json={"content": w})
        if r.status_code == 200:
            return r.json()["tokens"]
        return []

    def detokenize(self, ts):
        r = requests.post(self.endpoint + "/detokenize", json={"tokens": ts})
        if r.status_code == 200:
            return r.json()["content"]
        return []

    def health(self):
        r = requests.get(self.endpoint + "/health")
        if r.status_code != 200:
            return "error " + str(r.status_code)
        return r.json()["status"]

    def timings(self, result_json=None) -> Optional[Timings]:
        if result_json is None:
            if (json := self._last_result) is None:
                return None
        else:
            json = result_json

        if "timings" not in json:
            printerr("warning: Got weird server result: " + str(json))
            return

        time = json["timings"]
        # these are llama specific fields which aren't always available on the OAI endpoints
        truncated, cached_n = json.get("truncated", None), json.get(
            "tokens_cached", None
        )
        if (verbose := json.get("__verbose", None)) is not None:
            truncated, cached_n = verbose["truncated"], verbose["tokens_cached"]

            return Timings(
                prompt_n=time["prompt_n"],
                predicted_n=time["predicted_n"],
                prompt_ms=time["prompt_ms"],
                predicted_ms=time["predicted_ms"],
                predicted_per_token_ms=time["predicted_per_token_ms"],
                predicted_per_second=time["predicted_per_second"],
                truncated=truncated,
                cached_n=cached_n,
                original_timings=time,
            )

    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        # llamacpp params are the default
        return sampling_parameters


class OpenAILegacyBackend(AIBackend):
    """Backend for the official OpenAI API. The legacy version routes to /v1/completions, instead of the regular /v1/chat/completion."""

    def __init__(self, api_key, endpoint="https://api.openai.com"):
        super().__init__(endpoint)
        self.api_key = api_key

    def getName(self):
        return LLMBackend.openai.name

    def getMaxContextLength(self):
        return -1

    def generate(self, payload):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = payload | {"stream": False}
        self._last_request = data
        response = requests.post(
            self.endpoint + "/v1/completions", headers=headers, json=data
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            return None
        self._lastResult = response.json()
        return response.json()

    def handleGenerateResult(self, result):
        if not result:
            return None

        if (payload := result["choices"][0]["message"]["content"]) is not None:
            return payload
        if (payload := result["choices"][0]["message"]["tool_calls"]) is not None:
            return payload
        return None

    def generateStreaming(self, payload, callback=lambda w: print(w)):
        self.stream_done.clear()
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = payload | {"stream": True, "stream_options": {"include_usage": True}}
        self._last_request = data

        def openaiCallback(d):
            callback(d["choices"][0]["text"])

        response = streamPrompt(
            openaiCallback,
            self.stream_done,
            self.endpoint + "/v1/completions",
            json=data,
            headers=headers,
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            self.stream_done.set()
            return True
        return False

    def tokenize(self, w):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"prompt": w}
        response = requests.post(
            self.endpoint + "/v1/tokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["tokens"]
        return []

    def detokenize(self, ts):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"tokens": ts}
        response = requests.post(
            self.endpoint + "/v1/detokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["content"]
        return []

    def health(self):
        # OpenAI API does not have a direct health check endpoint
        return "OpenAI API is assumed to be healthy."

    def timings(self, result_json=None) -> Optional[Timings]:
        # FIXME: not implemented yet
        return None

    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        # restricted set
        # i just can't be bothered to test this
        supported = supported_parameters.keys()
        return {
            hp.name: hp for hp in sampling_parameters.values() if hp.name in supported
        }


class OpenAIBackend(AIBackend):
    """Backend for the official OpenAI API. This is used for the company of Altman et al, but also serves as a general purpose API suported by various backends, including llama.cpp, llama-box, and many others."""

    def __init__(self, api_key, endpoint="https://api.openai.com"):
        super().__init__(endpoint)
        self.api_key = api_key
        self._memoized_params = None

    def getName(self):
        return LLMBackend.openai.name

    def getMaxContextLength(self):
        return -1

    def generate(self, payload):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = payload | {"max_tokens": payload["max_length"], "stream": False}
        # the /V1/chat/completions endpoint expects structured data of user/assistant pairs
        data |= self.dataFromPayload(payload)

        if "tools" in data:
            # see the llamacpp generate method fixme
            # this has no effect on the official OAI api anyway
            data |= {"cache_prompt": False}

        self._last_request = data
        response = requests.post(
            self.endpoint + "/v1/chat/completions", headers=headers, json=data
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            return None
        self._last_result = response.json()
        return response.json()

    def handleGenerateResult(self, result):
        # this is just so that others can use the openai specific handling, which is kind of an industry standard
        return self.handleGenerateResultOpenAI(result)

    @staticmethod
    def handleGenerateResultOpenAI(result: Dict[str, Any]) -> Any:
        # used to be Optional[Dict[str, Any]]:
        # now it's Optional[str|Dict]
        # it's not completely terrible, since it makes sense - either return the text that the AI generated, or a dict if it was tools, or none on error
        # but still, needs a rework FIXME
        if not result:
            return None

        if (
            payload := result["choices"][0]["message"].get("content", None)
        ) is not None:
            return payload
        if result["choices"][0]["message"].get("tool_calls", None) is not None:
            # consumers of this like applyTools expect a dict here
            # FIXME: this is a bad function since it returns sometimes str sometimes dict. this is a big refactor though, as it would involve rewriting a bunch of internal types in pydantic
            return result
        return None

    @staticmethod
    def makeOpenAICallback(callback, last_result_callback=lambda x: x):
        def openAICallback(d):
            last_result_callback(d)
            choice = d["choices"][0]
            maybeChunk = choice["delta"].get("content", None)
            if maybeChunk is not None:
                callback(maybeChunk)

        return openAICallback

    def generateStreaming(self, payload, callback=lambda w: print(w)):
        self.stream_done.clear()
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = payload | {"stream": True, "stream_options": {"include_usage": True}}
        # the /V1/chat/completions endpoint expects structured data of user/assistant pairs
        data |= self.dataFromPayload(payload)
        self._last_request = data

        def one_line_lambdas_for_python(r):
            self._last_result = r

        response = streamPrompt(
            self.makeOpenAICallback(
                callback, last_result_callback=one_line_lambdas_for_python
            ),
            self.stream_done,
            self.endpoint + "/v1/chat/completions",
            json=data,
            headers=headers,
        )
        if response.status_code != 200:
            self.last_error = (
                f"HTTP request with status code {response.status_code}: {response.text}"
            )
            self.stream_done.set()
            return True
        return False

    @staticmethod
    def dataFromPayload(payload: Dict[str, Any]) -> Dict[str, Any]:
        """Take a payload dictionary from Plumbing and return dictionary with elements specific to the chat/completions endpoint.
        This expects payload to include the messages key, with various dictionaries in it, unlike other backends.
        """
        messages = [{"role": "system", "content": payload["system"]}]
        # story is list of dicts with role and content keys
        # we go through story one by one, mostly because of images
        for story_item in payload["story"]:
            if "image_id" in story_item:
                # images is more complicated, see https://platform.openai.com/docs/guides/vision
                # API wants the content field of an image message to be a list of dicts, not a string
                # the dicts have the type field, which determines wether its a user msg (text) or image (image-url)
                image_id = story_item["image_id"]
                image_content_list = []
                image_content_list.append(
                    {"type": "text", "content": story_item["content"]}
                )
                if "images" not in payload or image_id not in payload["images"]:
                    printerr("warning: image with id " + str(image_id) + " not found.")
                    continue

                # actually packaging the image
                image_data = payload["images"][image_id]
                ext = getImageExtension(image_data["url"], default="png")
                base64_image = image_data["data"].decode("utf-8")
                image_content_list.append(
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/{ext};base64,{base64_image}"},
                    }
                )

                messages.append(
                    {"role": story_item["role"], "content": image_content_list}
                )
            else:
                messages.append(story_item)

        return {"messages": messages}

    def tokenize(self, w):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"prompt": w}
        response = requests.post(
            self.endpoint + "/v1/tokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["tokens"]
        return []

    def detokenize(self, ts):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        data = {"tokens": ts}
        response = requests.post(
            self.endpoint + "/v1/detokenize", headers=headers, json=data
        )
        if response.status_code == 200:
            return response.json()["content"]
        return []

    def health(self):
        # OpenAI API does not have a direct health check endpoint
        return "OpenAI API is assumed to be healthy."

    def timings(self, result_json=None) -> Optional[Timings]:
        if result_json is None:
            if (json := self._last_result) is None:
                return None
        else:
            json = result_json


        if "__verbose" in json:
            verbose = json["__verbose"]
            time = verbose["timings"]
            truncated = verbose["truncated"]
            cached = verbose["tokens_cached"]
        elif "timings" in json:
            time = json["timings"]
            truncated = False
            cached = None            
        else:
            return None

        return Timings(
            prompt_n=time["prompt_n"],
            predicted_n=time["predicted_n"],
            prompt_ms=time["prompt_ms"],
            predicted_ms=time["predicted_ms"],
            predicted_per_token_ms=time["predicted_per_token_ms"],
            predicted_per_second=time["predicted_per_second"],
            # unfortunately openai don't reveal these, unless we got __verbose
            truncated=truncated,
            cached_n=cached,
            original_timings=time,
        )

    def sampling_parameters(self) -> Dict[str, SamplingParameterSpec]:
        # I don't like doing this everytime
        if self._memoized_params is not None:
            return self._memoized_params

        # this is tricky because it really depends on the actual backend.
        # the openai class really is not specific enough for this
        supported = supported_parameters.keys()
        sometimes = sometimes_parameters.keys()
        d = {hp.name: hp for hp in sampling_parameters.values() if hp.name in supported}
        for param in sometimes:
            sp = sampling_parameters[param]
            d[param] = SamplingParameterSpec(
                name=sp.name,
                default_value=sp.default_value,
                description=sp.description
                + "\nNote: May not be supported in this backend. For full support, try out llama.cpp https://github.com/ggml-org/llama.cpp",
            )

            self._memoized_params = d
            return d

```

## commands.py

```python
import os, datetime, glob, sys, requests, traceback, random, json
from ghostbox.session import Session
from ghostbox.util import *
from ghostbox.StoryFolder import *
from ghostbox.definitions import *
from ghostbox.api_internal import *

def newSession(program, argv, keep=False):
    """CHARACTER_FOLDER
Start a new session with the character or template defined in CHARACTER_FOLDER. You can specify a full path, or just the folder name, in which case the program will look for it in all folders specified in the 'include' paths. See /lsoptions include."""
    if argv == []:
        if program.getOption("character_folder"):
            # synonymous with /restart
            argv.append(program.getOption("character_folder"))
        else:
            return "No path provided. Cannot Start."


    filepath = " ".join(argv)
    return start_session(program, filepath, keep=keep)

def printStory(prog, argv, stderr=False, apply_filter=True):
    """[FILENAME]
Print the current story.
If FILENAME is provided, save the story to that file."""
    # apply_filter basically means make it pretty
    if apply_filter:
        w = prog.formatStory(with_color=prog.getOption("color"))
    else:
        w = prog.showStory(append_hint=False)

    if stderr:
        printerr(w, prefix="")
        return ""
    elif argv != []:
        filename = " ".join(argv)
        if os.path.isdir(filename):
            return "error: Cannot write to file " + filename + ": is a directory." 
        if os.path.isfile(filename):
            printerr("warning: File " + filename + " exists. Appending to file.")
            pre = open(filename, "r").read() + "\n--- new story ---"
        else:
            pre = ""
        f = open(filename, "w")
        f.write(pre + w)
        return ""
    else:
        # if stderr=False, we actually want this to go to stdout
        print(w, end="")
        return ""
    
def doContinue(prog, argv):
    """
Continue generating without new input. Use this whenever you want the AI to 'just keep talking'.
Specifically, this will send the story up to this point verbatim to the LLM backend. You can check where you are with /log.
If there are templated tokens that are normally inserted as part of the prompt format when a user message is received, they are not inserted. Existing end tokens, like <|im_end|> will be removed before sending the prompt back.
This command is also executed when you hit enter without any text."""

    prog.setOption("continue", "1")
    # FIXME: often we just get EOS. this is sort of risky though.
    #prog.setOption("ignore_eos", True)
    if prog.session.stories.empty():
        return ""
    
    return ""

def setOption(prog, argv):
    """OPTION_NAME [OPTION_VALUE]
    Set options during program execution. To see a list of all possible values for OPTION_NAME, do /lsoptions. OPTION_VALUE must be a valid python expression, so if you want to set e.g. chat_user to Bob, do /set chat_user \"Bob\".
    When OPTION_VALUE is omitted, the value is set to \"\". This is equivalent to /unset OPTION_NAME."""

    if argv == []:
        return showOptions(prog, [])
    name = argv[0]    
    if len(argv) == 1:
        prog.setOption(name, "")
        return ""

    w = " ".join(argv[1:])
    try:
        prog.setOption(name, eval(w))
    except:
        printerr(traceback.format_exc())
        return "Couldn't set " + name + " to " + w + ". Couldn't evaluate."
    return ""
    
def showOptions(prog, argv):
    """[OPTION_NAME]
Displays the list of program options, along with their values. Provide OPTION_NAME to see just its value.
The options displayed can all be set using /set OPTION_NAME.
Almost all of them may also be provided as command line arguments with preceding dashes, e.g. include as --include=/some/path.
Finally, they may be set in various config files, such as in character folders, or in a config file loaded with /load."""
    if argv == []:
        target = ""
    elif argv[0] == "--emacs":
        # undocumented, outputs all options in a neat form to put as keywords in ghostbox.el
        w = "`("
        for name in prog.options.keys():
            w += '"' + name + '", '
        w = w[:-2] + ")"
        printerr(w)
        return ""
    else:
        target = " ".join(argv)
    
        
    w = ""
    for (k, v) in prog.options.items():
        if target in k:
            w += k + ": " + str(v) + "\n"
    return w.strip()

def showVars(prog, argv):
    """[VARIABLE_NAME]
    Shows all session variables and their respective values. Show a single variable if VARIABLE_NAME is provided.
    Variables are automatically expanded within text that is provided by the user or generated by the LLM backend.
    E.g. if variable favorite_fruit is set to 'Tomatoe', any occurrence of {{favorite_fruit}} in the text will be replaced by 'Tomatoe'.
    There are three ways to set variables:
      1. Loading a character folder (/start). All files in a character folder automatically become variables, with the respective filename becoming the name of the variable, and the file's content becoming the value of the variable.
      2. The -x or --varfile command line option. This argument provides additional files that may serve as variables, similar to (1). The argument may be repeated multiple times.
      3. API only: Using the set_vars method on a Ghostbox object.

    You can also do {[FILENAME]} to ad-hoc splice the contents of FILENAME into a prompt. However, due to security reasons, this only works at the CLI.
    """

    if argv == []:
        return "\n".join([f"{key}\t{value}" for key, value in prog.session.fileVars.items()])

    k = " ".join(argv)
    return f"{prog.session.fileVars.get(k, f"Could not find var '{k}'")}"

def showChars(prog, argv):
    """
    Lists all available character folders. These can be used with the /start command or the -c command line argument, and are valid values for the character_folder parameter in the Ghostbox API.
    To see what places are searched for characters, see the value of the 'include' option."""
    return "\n".join(sorted(all_chars(prog)))

def showTemplates(prog, argv):
    """
Lists all available templates for prompt formats.
To see the places searched, check the value of the template_include option."""
    allchars = []
    for dir in prog.getOption("template_include"):
        if not(os.path.isdir(dir)):
            printerr("warning: template Include path '" + dir + "' is not a directory.")
            continue
        for charpath in glob.glob(dir + "/*"):
            if os.path.isfile(charpath):
                continue
            allchars.append(os.path.split(charpath)[1])
    return "\n".join(sorted(allchars))
    

def showVoices(prog, argv):
    """
List all available voices for the TTS program. These can be used with /set tts_voice.
To see the places searched for voices, see the value of tts_voice_dir.
Depending on the value of tts_program, that location may be meaningless, as the voice won't be file based, like with amazon polly voices. In this case /lsvoices tries to give a helpful answer if it can."""
    return "\n".join(getVoices(prog))

        

def toggleMode(prog, argv):
    """[MODE_NAME}
Put the program into the specified mode, or show the current mode if MODE_NAME is omitted.
Possible values are 'default', or 'chat'.
Setting a certain mode has various effects on many aspects of program execution. Currently, most of this is undocumented :)"""    
    if argv == []:
        return "Currently in " + prog.getOption("mode") + " mode."

    mode = " ".join(argv)
    if prog.isValidMode(mode):
        prog.setMode(mode)
        return mode + " mode on"
    return "Not a valid mode. Possible values are 'default' or 'chat'"

def toggleTTS(prog, argv):
    """
This turns the TTS (text-to-speech) module on or off. When TTS is on, text that is generated by the LLM backend will be spoken out loud.
The TTS service that will be used depends on the value of tts_program. The tts_program can be any executable or shell script that reads lines from standard input. It may also support additional functionality.
An example tts program for amazon polly voices is provided with 'ghostbox-tts-polly'. Note that this requires you to have credentials with amazon web services.
On linux distributions with speech-dispatcher, you can set the value of tts_program to 'spd-say', or 'espeak-ng' if it's installed. This works, but it isn't very nice.
The voice used depends on the value of tts_voice, which you can either /set or provide with the -V or --tts_voice command line option. It can also be set in character folder's config.json.
ghostbox will attempt to provide the tts_program with the voice using a -V command line argument.
To see the list of supported voices, try /lsvoices.
Enabling TTS will automatically set stream_flush to 'sentence', as this works best with most TTS engines. You can manually reset it to 'token' if you want, though."""
    return toggle_tts(prog)


def ttsDebug(prog, argv):
    """
Get stdout and stderr from the underlying tts_program when TTS is enabled."""
    if not(prog.tts):
        return "TTS is None."

    w = "tts_program: " + prog.getOption("tts_program")
    w += "\ntts exit code: " + str(prog.tts.exit_code())
    printerr(w)
    
    w = "\n### STDOUT ###\n"
    w += "".join(prog.tts.get())
    w += "\n### STDERR ###\n"
    w += "".join(prog.tts.get_error())
    printerr(w, prefix="")
    return ""
    
def nextStory(prog, argv):
    """
    Go to next branch in story folder."""
    r = prog.session.stories.nextStory()
    if r == 1:
        return "Cannot go to next story branch: No more branches. Create a new branch with /new or /retry."
    return "Now on branch " + str(prog.session.stories.index)

def previousStory(prog, argv):
    """
    Go to previous branch in story folder."""
    r = prog.session.stories.previousStory()
    if r == -1:
        return "Cannot go to previous story branch: No previous branch exists."
    return "Now on branch " + str(prog.session.stories.index)    

def retry(prog, argv, predicate=lambda item: item.role == "user"):
    """
    Retry generation of the LLM's response.
This will drop the last generated response from the current story and generate it again. Use this in most cases where you want to regenerate. If you extended the LLM's repsone (with /cont or hitting enter), the entire repsonse will be regenerated, not just the last part.
Note that /retry is not destructive. It always creates a new story branch before regenerating a repsonse.
    See also: /rephrase."""
    prog.session.stories.cloneStory()
    prog.session.stories.get().dropUntil(predicate)
    doContinue(prog, [])
    printerr("Now on branch " + str(prog.session.stories.index) )
    return ""

def rephrase(prog, argv):
    """
This will rewind the story to just before your last input, allowing you to rephrase your query.
    Note that /rephrase is not destructive. It will always create a new story branch before rewinding.
    See also: /retry"""
    prog.session.stories.cloneStory()
    story = prog.session.stories.get()
    story.dropUntil(lambda item: item.role == "user")
    story.dropUntil(lambda item: item.role == "assistant")
    printerr("Now on branch " + str(prog.session.stories.index) )    
    return ""



def dropEntry(prog, argv):
    """
Drops the last entry from the current story branch, regardless of wether it was user provided or generated by the LLM backend."""
    prog.session.stories.cloneStory()
    prog.session.stories.get().drop()
    return "Now on branch " + str(prog.session.stories.index) + " with last entry dropped."

    
def newStory(prog, argv):
    """
    Create a new, empty branch in the story folder. You can always go back with /prev or /story."""
    prog.session.stories.newStory()
    return "Now on branch " + str(prog.session.stories.index) + " with a clean log."

def cloneStory(prog, argv):
    """
    Create a new branch in the story folder, copying the contents entirely from the current story."""
    prog.session.stories.cloneStory()
    return "Now on branch " + str(prog.session.stories.index) + " with a copy of the last branch."



def saveStoryFolder(prog, argv):
    """[STORY_FOLDER_NAME]
    Save the entire story folder in the file STORY_FOLDER_NAME. If no argument is provided, creates a file with the current timestampe as name.
    The file created is your accumulated chat history in its entirety, including the current story and all other ones (accessible with /prev and /next, etc). It is saved in the json format.
    A saved story folder may be loaded with /load."""
    if len(argv) > 0:
        name = " ".join(argv)
    else:
        name = datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S") + ".json"
            
    filename = saveFile(name, prog.session.stories.toJSON())
    if not(filename):
        return "Could not save story folder. Maybe provide a different filename?"
    return "Saved story folder as " + filename



def changeTemplate(prog, argv):
    """[PROMPT_TEMPLATE]
Shows the current prompt template. If PROMPT_TEMPLATE is supplied, tries to load that template.
LLMs usually work best when supplied with input that is formatted similar to their training data. Prompt templates apply some changes to your inputs in the background to get them into a shape that the LLM expects.
    To find out the right template to use, consult the model card of the LLM you are using. When in doubt, 'chat-ml' is a very common prompt format.
To disable this, set the template to 'raw' and you won't have anything done to your inputs. This can be useful for experimentation.
Templates are searched for in the directories specified by the template_include option, which can be supplied at the command line.
To get a full list of available templates, try /lstemplates ."""

    if argv == []:
        return prog.getOption("prompt_format")
        
    choice = " ".join(argv)
    prog.loadTemplate(choice)
    return ""
    
def loadStoryFolder(prog, argv):
    """STORY_FOLDER_NAME
    Loads a previously saved story folder from file STORY_FOLDER_NAME. See the /save command on how to save story folders.
    A story folder is a json file containing the entire chat history."""
    if len(argv) == 0:
        return "Please provide a legal json filename in the story-folder format."
    
    filename = " ".join(argv)
    try:
        w = open(filename, "r").read()
        s = StoryFolder(json_data=w)
    except FileNotFoundError as e:
        return "Could not load " + filename + ": file not found."
    except Exception as e:
        printerr(str(e))
        return "Could not load story folder: Maybe bogus JSON?"
    prog.session.stories = s
    return "Ok. Restored " + filename + "\nNow on branch " + str(prog.session.stories.index)
        
def gotoStory(prog, argv):
    """[BRANCH_NUMBER]
    Moves to a different branch in the story folder. If BRANCH_NUMBER is omitted, shows the current branch instead."""
    if argv == []:
        return "Currently on branch " + str(prog.session.stories.index)

    w = " ".join(argv)
    try:
        n = int(w)
    except:
        return "Cannot go to that branch: Invalid Argument."
    
    err = prog.session.stories.shiftTo(n)
    if not(err):
        return ""
    return "Could not go to branch " + str(n) + ": " + err

def loadConfig(prog, argv, override=True):
    """CONFIG_FILE
Loads a json config file at location CONFIG_FILE. A config file contains a dictionary of program options. 
You can create an example config with /saveconfig example.conf.json.
You can also load a config file at startup with the --config_file command line argument.
If it exists, the ~/.ghostbox.conf.json will also be loaded at startup.
The order of config file loading is as follows .ghostconf.conf.json > --config_file > conf.json (from character folder). Config files that are loaded later will override settings from earlier files."""

    # loads a config file, which may be a user config, or a config supplied by a character folder. if override is false, the config may not override command line arguments that have been manually supplied (in the long form)
    if argv == []:
        return "Please provide a filename for the config file to load."
    filename = " ".join(argv)
    return load_config(prog, filename, override=override)
def saveConfig(prog, argv):
    """CONFIG_FILE
    Save the current program options and their values to the file at location CONFIG_FILE. This will either create or overwrite the CONFIG_FILE, deleting all its previous contents."""
    
    if argv == []:
        name = "config-" + datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
    else:
        name = " ".join(argv)

    if not(name.endswith(".json")):
        name = name + ".json"

    filename = saveFile(name, json.dumps(prog.options, indent=4))
    if filename:
        return "Saved config to " + filename
    return "error: Could not save config."

    

def hide(prog, argv):
    """
    Hide various program outputs and change some variables for a less verbose display.
    This does nothing that you cannot achieve by manually setting several options, it just bundles an eclectic mix of them in one command.
    I like to use this with TTS for a more immersive experience."""
    # this is just convenient shorthand for when I want my screen reader to be less spammy
    hide_some_output(prog)
    return ""

def varfile(prog, argv):
    # $FIXME:
    return "Not implemented yet. Use the -V option for now."

def exitProgram(prog, argv):
    """
    Quit the program. Chat history (story folder) is discarded. All options are lost.
    See also /save, /saveoptions, /saveconfig"""
    prog.stopAudioTranscription()
    prog.stopImageWatch()
    prog.running = False
    return ""

    
def transcribe(prog, argv):
    """Records using the microphone until you hit enter. The recording is then transcribed using openai's whisper, and inserted into the current line at the CLI.
THe precise model to be used for transcribing is determined using the 'whisper_model' option. Larger models transcribe more accurately, and may handle more languages, but also consume more resources. See https://huggingface.co/openai/whisper-large for a list of model names. The default is 'base.en'.
The model will be automatically downloaded the first time you transcribe with it. This may take a moment, but will only happen once for each model."""
    w = prog.whisper.transcribeWithPrompt()
    sys.stdout.write("\r" + w + "\n")
    prog.continueWith(w)
    return ""

def toggleAudio(prog, argv):
    """Enable/disable audio input. This means the program will automatically record audio and transcribe it using the openai whisper model. A query is send to the backend with transcribed speech whenever a longish pause is detected in the input audio.
See whisper_model for the model used for transcribing."""
    if prog.isAudioTranscribing():
        prog.stopAudioTranscription()
    else:
        prog.startAudioTranscription()
    return ""

def image(prog, argv):
    """[--id=IMAGE_ID] IMAGE_PATH
Add images for multimodal models that can handle them. You can refer to images by their id in the form of `[img-ID]`. If --id is omitted, id= 1 is assumed. Examples:
```
    /image ~/Pictures.test.png
Please describe [img-1].
    ```

    Alternatively, with multiple images:
    ```
    /image --id=1 ~/Pictures.paris.png
    /image --id=2 ~/Pictures/berlin.png
Can you compare [img-1] and [img-2]?
    ```"""

    if argv == []:
        ws = []
        for (id, imagedata) in sorted(prog.images.items()):
            ws.append(mkImageEmbeddingString(id) + "\t" + imagedata["url"])
        # FIXME: secret, undocumented function: /image without arguments dirties the image cache
        prog._images_dirty = True
        return "\n".join(ws)

    if argv[0].startswith("--id="):
        id = maybeReadInt(argv[0].replace("--id=", ""))
        if id is None:
            return "error: Please specify a valid integer as id."
        url = " ".join(argv[1:])
    else:
        id = 1
        url = " ".join(argv)

    prog.loadImage(url, id)
    return ""
        
            
def debuglast(prog, argv):
    """
    Dumps a bunch of information about the last result received. Note that this won't do anything if you haven't sent a request to a working backend server that answered you."""
    if not(r := prog.lastResult):
        return "Nothing."

    acc = []
    for (k, v) in r.items():
        acc.append(k + ": " + str(v))
    return "\n".join(acc)


def lastRequest(prog, argv):
    """
    Dumps a bunch of information about the last request send. Note that this won't do anything if you haven't sent a request to a working backend server that answered you."""
    r = prog.getBackend().getLastRequest()
    if not(r):
        return "Nothing."

    return json.dumps(r, indent=4)

def showTime(prog, argv):
    """
Show some performance stats for the last request."""
    r = prog.getBackend().timings()
    if not(r):
        return "No time statistics. Either no request has been sent yet, or the backend doesn't support timing."
    
    w = ""
    # timings: {'predicted_ms': 4115.883, 'predicted_n': 300, 'predicted_per_second': 72.88836927580303, 'predicted_per_token_ms': 13.71961, 'prompt_ms': 25.703, 'prompt_n': 0, 'prompt_per_second': 0.0, 'prompt_per_token_ms': None}    
    #caching etc
    w += "generated: " + str(r.predicted_n) 
    w += ", evaluated: " + str(r.prompt_n)
    w += ", cached: " + (str(r.cached_n) if r.cached_n is not None else "unknown") + "\n"
    w += "context: " + str(r.total_n()) + " / " +  str(prog.getOption("max_context_length")) + ", exceeded: " + str(r.truncated)
    if prog._smartShifted:
        w += "(smart shifted)\n"
    else:
        w += "\n"

    factor = 1/1000
    unit = "s"
    prep = lambda u: str(round(u*factor, 2)) 
    w += prep(r.prompt_ms) + unit + " spent evaluating prompt.\n"
    w += prep(r.predicted_ms) + unit + " spent generating.\n"
    w += prep(r.total_ms()) + unit + " total processing time.\n"  
    w += str(round(r.predicted_per_second, 2)) + "T/s, " + prep(r.predicted_per_token_ms) + unit + "/T"
    return w

def showStatus(prog, argv):
    """
Give an overall report about the program and some subprocesses."""
    if argv == []:
        topics = set("backend mode tts audio image_watch streaming".split(" "))
    else:
        topics = set(argv)

    w = ""
    if "backend" in topics:
        w += "backend: " + prog.getOption("backend") + " at " + prog.getOption("endpoint") + "\n"
        w += "backend status: " + str(prog.getBackend().health()) + "\n"
        w += "max_context_length: " + str(prog.getOption("max_context_length"))
        if prog._dirtyContextLlama:
            # context has been set by server
            w += " (set by llama.cpp)\n"
        else:
            w += "\n"
        
        w += " models\n"
        models =         dirtyGetJSON(prog.getOption("endpoint") + "/v1/models").get("data", [])
        for m in models:
            w += " .. " + m["id"] + "\n\n"

    if "mode" in topics:
        w += "mode: " + prog.getOption("mode") + "\n"
        w += "\n"
        # FIXME: more mode stuff here

    if "tts" in topics:
        w += "tts: " + str(prog.getOption("tts")) + "\n"
        w += "tts_program: " + prog.getOption("tts_program") + "\n"
        w += "tts status: "
        if prog.tts is None:
            w += "uninitialized"
        else:
            if prog.tts.is_running():
                w += "running"
            else:
                w += "exit code " + str(prog.tts.exit_code())
        w += "\n\n"
                
                
    if "audio" in topics:
        w += "audio transcription: " + str(prog.getOption("audio")) + "\n"
        w += "whisper_model: " + prog.getOption("whisper_model") + "\n"
        w += "continuous record / transcribe status: "
        if prog.ct is None:
            w += "N/A\n"
        else:
            if prog.ct.running:
                w += "running"
                if prog.ct.isPaused():
                    w += " (paused)"
                w += "\n"
            else:
                w += "stopped\n"
        w += "\n"
                
    if "image_watch" in topics:
        w += "image_watch: " + str(prog.getOption("image_watch")) + "\n"
        w += "image_watch_dir: " + prog.getOption("image_watch_dir") + "\n"
        w += "status: "
        
        if prog.image_watch is None:
            w += "N/A\n"
        else:
            if prog.image_watch.running:
                w += "running\n"
            else:
                w += "halted"
        w += "\n"

        if "streaming" in topics:
            w += "streaming: " + str(prog.getOption("stream")) + "\n\n"
        
    return w.strip()

def toggleImageWatch(prog, argv):
    """[DIR]
Enable / disable automatic watching for images in a specified folder.
    When new images are created / modified in image_watch_dir, a message (image_watch_msg)is automatically sent to the backend.
    If DIR is provided to this command, image_watch_dir will be set to DIR. Otherwise, the preexisting image_watch_dir is used, which defaults to the user's standard screenshot folder.
    This allows you to e.g. take screenshots and have the TTS automatically describe them without having to switch back to this program.
    Check status with /status image_watch."""
    dir = " ".join(argv)
    if dir != "":
        if not(os.path.isdir(dir)):
            return "error: Could not start image_watch: " + dir + " is not a directory."
        prog.setOption("image_watch", dir)
        
    # toggle
    prog.options["image_watch"] = not(prog.getOption("image_watch"))
    if prog.getOption("image_watch"):
        prog.startImageWatch()
    else:
        prog.stopImageWatch()
        return "image_watch off."
    return ""
        

def showRaw(prog, argv):
    """
    Displays the raw output for the last prompt that was sent to the backend."""
    printerr(prog._lastPrompt, prefix="")
    return ""


def switch(prog, argv):
    """ CHARACTER_FOLDER
Switch to another character folder, retaining the current story.
As opposed to /start or /restart, this will hot-switch the AI without wiping your story so far, or adding the initial message. This can e.g. allow the used of specialized AI's in certain situations.
Specifically, '/switch bob' will do the following:
 - Set your system prompt to bob/system_msg
 - Set all session variables defined in bob/, possibly overriding existing ones
 - Load bob/config.json, if present, not overriding command line arguments.
See also: /start, /restart, /lschars"""
    if argv == []:
        return "No character folder given. No switch occured."
    
    prog._lastChar = prog.session.dir
    w = newSession(prog, argv, keep=True)
    # FIXME: when people use this command they probably don't want to be spammed, so we discard w, but maybe we want a verbose mode?
    return w.split("\n")[-1]
                   
    
def tokenize(prog, argv):
    """[-c] MSG
Send a tokenize request to the server. Will print raw tokens to standard output, one per line. This is mostly used to debug prompts. With -c, it will print the number of tokens instead."""
    if argv != [] and argv[0] == "-c":
        count = True
        argv = argv[1:]        
    else:
        count = False
        
    ts = prog.getBackend().tokenize("".join(argv))
    if count:
        return str(len(ts))

    for t in ts:
        print(t)
    return ""
    
def detokenize(prog, argv):
    """[-n]
Turn a list of tokens into strings.
Tokens can be supplied like this /detokenize 23 20001 1
    If -n is supplied, reads one token per line until an empty line is found."""
    ts = []
    if argv != [] and argv[0] == "-n":
        # one per line
        argv = []
        while True:
            w = input()
            if w == "":
                break
            argv.append(w)
        
    for arg in argv:
        try:
            t = int(arg)
        except:
            return "Please specify tokens as integers, seperated by spaces, or newlines in case you supplied -n."
        ts .append(t)

    w = prog.backend.detokenize(ts)
    print(w)
    return ""
            
            
            
def testQuestion(prog, argv):
    """
Send a random test question to the backend.
The question is pulled from a random set of example questions. These are sometimes funny, but also turn out to be quite useful to get a general sense of a model."""

    questions ="""I have 8 eggs, 4 water bottles, and 1 laptop. Suggest to me a configuration of these objects with which to balance them on top of another.
How much does 1 kilogram of feathers weigh?
I have 3 apples. I give 2 to timmy. How many apples do I have?
Me and my friends have a rule: "If you borrow a sweater, then you have to return it." Last month I got Jane's sweater. I did not return it. What do I have to do now?
You’re in a desert walking along in the sand when all of the sudden you look down, and you see a tortoise, it’s crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?
Describe in single words, only the good things that come into your mind about your mother.
In a magazine you come across a full-page color picture of a nude girl. Your husband likes the picture. The girl is lying facedown on a large and beautiful bearskin rug. Your husband hangs the picture up on the wall of his study. How do you feel?
A young boy shows you his butterfly collection, including the killing jar. How do you feel?
You’re reading a novel written in the old days before the war. The characters are visiting Fisherman’s Wharf in San Francisco. They become hungry and enter a seafood restaurant. One of them orders lobster, and the chef drops the lobster into the tub of boiling water while the characters watch. What do you think about this?
You are watching an old movie on TV, a movie from before the war. It shows a banquet in progress; the guests are enjoying raw oysters. The entrée consists of boiled dog, stuffed with rice. Are raw oysters more acceptable to you than a dish of boiled dog?""".split("\n")

    w = random.choice(questions)
    # no prefix
    print(w)
    prog.continueWith(w)
    return ""

cmds_additional_docs = {
    "/log" : """
    Prints the raw log of the current story branch.
    This includes prompt-format tokens and other stuff that is normally filtered out. For a prettier display, see /print.
    Also, /log prints to stderr, while /print will output to stdout.""",
    "/resend" : retry.__doc__,
    "/rephrase" : retry.__doc__,
    "/restart" : """
    Restarts the current character folder. Note that this will wipe the current story folder, i.e. your chat history, so you may want to /save.
    /restart is equivalent to /start CURRENT_CHAR_FOLDER."""    
    }

```

## definitions.py

```python
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field
from pydantic.types import Json
from typing import *

class Property(BaseModel):
    description: str
    type: str

class Parameters(BaseModel):
    type: str = "object"
    properties: Dict[str, Property]
    required: List[str] = []
    
class Function(BaseModel):
    name: str
    description: str
    # this wants jsonschema object
    parameters: Parameters

class Tool(BaseModel):
    type: str = "function"
    function: Function

class ChatContentComplex (BaseModel):
    """Contentfield of a ChatMessage, at least when the content is not a mere string."""
    type: Literal["text", "image-url"]
    content: str
    
ChatContent = str | List[ChatContentComplex] | Dict

class FunctionCall(BaseModel):
    name: str
    # this is weird but it really is str, no idea why not dict
    arguments: str
    
class ToolCall(BaseModel):
    type: str = "function"
    function: FunctionCall
    # FIXME: I don't quite understand id field yet
    id: str = ""
    
class ChatMessage(BaseModel):
    role: Literal["system", "assistant", "user", "tool"]
    content: Optional[ChatContent] = None
    tool_calls: List[ToolCall] = []
    tool_name: str = ""
    tool_call_id: str = ""
    

LLMBackend = Enum("LLMBackend", "generic legacy llamacpp koboldcpp openai dummy")

# these are the models supported by ghostbox-tts
TTSModel = Enum("TTSModel", "zonos kokoro xtts")

# these are ways of playing sound that are supported by ghostbox-tts
TTSOutputMethod = Enum("TTSOutputMethod", "default websock")


# this isn't used yet anywhere, but it's nice to have here already for documentation
PromptFormatTemplateSpecialValue = Enum("PromptFormatTemplateSpecialValue", "auto guess raw")


ArgumentType = Enum("ArgumentType", "Porcelain Plumbing")
ArgumentGroup = Enum("ArgumentGroup", "General Generation Interface Characters Templates TTS Audio Images Tools Backend SamplingParameters LlamaCPP OpenAI")

class ArgumentTag(BaseModel):
    """Metadata associated with a command line argument."""
    name: str = ""
    type: ArgumentType
    group: ArgumentGroup

    # this is for e.g. streaming or temperature.
    very_important: bool = False
    
    # wether changing the value of this argument may start a service
    service: bool = False

    # for inclusion in the message of the day/tip
    motd: bool = False

    # basically, if its a command or option
    is_option: bool = True
    default_value: Optional[Any] = None
    
    # same help that is printed in terminal on --help
    help: str = ""


    
    def show_type(self) -> str:
        if self.is_option:
            return "It is a " + self.type.name.lower() + " option in the " + self.group.name.lower() + " group."
        return "It is a " + self.type.name.lower() + " command in the " + self.group.name.lower() + " group."
    
    def show_description(self) -> str:
        w = ""
        w += self.show_type()
        w += "\nYou can set it with `/set " + self.name + " VALUE` or provide it as a command line parameter with `--" + self.name + "`"
        return w
        
    
class SamplingParameterSpec(BaseModel):
    """Sampling parameters can be provided to backends to influence a model's inference behaviour.
    Most commonly this is temperature, presence penalty etc. However, here we take sampling parameter in the broadest sense, including samplers, CFG and control vectors.
    Note that this class provides only the specification of a sampling_parameter. This is for documentation and to keep track of which backend supports which parameters."""

    name: str
    description: str
    default_value: Any

class Timings(BaseModel):
    """Performance statistics for LLM backends.
    Most backends give timing statistics, though the format and particular stats vary. This class unifies the interface and boils it down to only the stats we care about.
    """

    prompt_n: int
    predicted_n: int
    cached_n: Optional[int] = None
    truncated: bool
    prompt_ms: float
    predicted_ms: float
    predicted_per_second: float
    predicted_per_token_ms: float

    original_timings: Optional[Dict[str, Any]] = {}

    def total_n(self) -> int:
        return self.prompt_n + self.predicted_n

    def total_ms(self) -> float:
        return self.prompt_ms + self.predicted_ms

    
api_default_options = {
    "color" : False,
    "verbose" : False,
    "stderr": True,
    "log_time" : True,
    "cli_prompt" : "",
    "dynamic_file_vars": False,
    "max_context_length":32000
}



```

## __init__.py

```python
# __all__ = ["I will get rewritten"]
## Don't modify the line above, or this line!
# import automodinit
# automodinit.automodinit(__name__, __file__, globals())
# del automodinit

from ghostbox.api import *
from ghostbox.definitions import *
import os

_ROOT = os.path.abspath(os.path.dirname(__file__))


def get_ghostbox_data(path):
    """Returns PATH preceded by the location of the ghostbox data dir, which is part of the python site-package."""
    return os.path.join(_ROOT, "data", path)


def get_ghostbox_html_path():
    """Returns the path to the index.html and javascript files for the integrated HTTP server."""
    return os.path.join(_ROOT, "html")



__all__ = [
    "from_generic",
    "from_openai_legacy",
    "from_llamacpp",
    "from_openai_official",
    "Ghostbox",
    "ChatMessage",
    "LLMBackend",
    "TTSModel",
    "TTSOutputMethod",
    "PromptFormatTemplateSpecialValue",
    "Timings",
    "api_default_options",
    "get_ghostbox_data",
    "get_ghostbox_html_path"
]

```

## main.py

```python
import requests, json, os, io, re, base64, random, sys, threading, signal, tempfile, string, uuid, textwrap
from queue import Queue, Empty
from typing import *
import feedwater
from functools import *
from colorama import just_fix_windows_console, Fore, Back, Style
from lazy_object_proxy import Proxy
import argparse
from argparse import Namespace
from ghostbox.commands import *
from ghostbox.autoimage import *
from ghostbox.output_formatter import *
from ghostbox.util import *
from ghostbox import util
from ghostbox import agency
from ghostbox._argparse import *
from ghostbox import streaming
from ghostbox.session import Session
from ghostbox.pftemplate import *
from ghostbox.backends import *
from ghostbox import backends
import ghostbox


def showHelpCommands(prog, argv):
    """
    List commands, their arguments, and a short description."""

    w = ""
    candidate = "/" + "".join(argv).strip()
    failure = True
    for cmd_name, f in cmds:
        if candidate != "/" and cmd_name != candidate:
            continue
        failure = False
        if f.__doc__ is None:
            docstring = cmds_additional_docs.get(cmd_name, "")
        else:
            docstring = str(f.__doc__)
        w += cmd_name + " " + docstring + "\n"
    printerr(w, prefix="")

    if failure:
        return "Command not found. See /help for more."
    return ""


def showHelp(prog, argv):
    """[TOPIC] [-v|--verbose]
    List help on various topics. Use -v to see even more information."""
    w = ""
    if argv == []:
        # list topics
        w += "[Commands]\nUse these with a preceding slash like e.g. '/retry'. Do /help COMMANDNAME for more information on an individual command.\n\n"
        command_names = sorted([tuple[0].replace("/", "") for tuple in cmds])
        for i in range(len(command_names)):
            command_name = command_names[i]
            if i % 3 == 0:
                start = "\n  "
            else:
                start = "\t"

            w += start + command_name

        w += "\n\n[Options]\nSet these with '/set OPTIONNAME VALUE', where value is a valid python expression.\n List options and their values with /ls [OPTIONNAME].\n"
        tags = prog.getTags()
        options = sorted(
            tags.items(), key=lambda item: (item[1].group.name, item[1].name)
        )
        last_group = ""
        for i in range(len(options)):
            option, tag = list(options)[i]
            if last_group != tag.group.name:
                w += "\n\n  [" + tag.group.name + "]\n"
                last_group = tag.group.name
            option = (
                wrapColorStyle(option, "", Style.BRIGHT)
                if tag.very_important
                else option
            )
            if i % 3 == 0:
                w += "\n" + option
            else:
                w += "\t" + option

        return w

    topic = argv[0]
    if topic in prog.getTags():
        tag = prog.getTags()[topic]
        if tag.is_option:
            w += (
                topic
                + "\n"
                + "\n".join(textwrap.wrap(tag.help))
                + "\n"
                + "\n".join(textwrap.wrap(tag.show_description()))
                + "\nIts current value is "
                + str(prog.getOption(topic))
                + "."
            )
            if tag.default_value is not None:
                w += " Its default value is " + str(tag.default_value)
            if tag.service:
                w += "\nSetting it to True will start the corresponding service."
    else:
        # it's a command
        # FIXME: commands aren't tagged yet
        return showHelpCommands(prog, [topic])
    # else:
    # not found
    # return "Topic not found in help. Please choose one of the topics below.\n" + showHelp(prog, [])
    return w


# these can be typed in at the CLI prompt
cmds = [
    ("/help", showHelp),
    ("/helpcommands", showHelpCommands),
    ("/start", newSession),
    ("/switch", switch),
    ("/quit", exitProgram),
    ("/test", testQuestion),
    ("/restart", lambda prog, argv: newSession(prog, [])),
    ("/print", printStory),
    ("/next", nextStory),
    ("/prev", previousStory),
    ("/story", gotoStory),
    ("/retry", retry),
    ("/rephrase", rephrase),
    ("/drop", dropEntry),
    ("/new", newStory),
    ("/clone", cloneStory),
    ("/log", lambda prog, w: printStory(prog, w, stderr=True, apply_filter=False)),
    ("!", transcribe),
    ("/transcribe", transcribe),
    ("/audio", toggleAudio),
    ("/image_watch", toggleImageWatch),
    ("/image", image),
    ("/time", showTime),
    ("/status", showStatus),
    ("/detokenize", detokenize),
    ("/tokenize", tokenize),
    ("/raw", showRaw),
    ("/lastresult", debuglast),
    ("/lastrequest", lastRequest),
    ("/ttsdebug", ttsDebug),
    ("/tts", toggleTTS),
    ("/set", setOption),
    ("/unset", lambda prog, argv: setOption(prog, [argv[0]])),
    ("/template", changeTemplate),
    ("/saveoptions", saveConfig),
    ("/saveconfig", saveConfig),
    ("/loadconfig", loadConfig),
    ("/save", saveStoryFolder),
    ("/load", loadStoryFolder),
    ("/varfile", varfile),
    ("/lstemplates", showTemplates),
    ("/lsoptions", showOptions),
    ("/lschars", showChars),
    ("/lsvoices", showVoices),
    ("/lsvars", showVars),
    ("/mode", toggleMode),
    ("/hide", hide),
    ("/cont", doContinue),
    ("/continue", doContinue),
]

# the mode formatters dictionary is a mapping from mode names to tuples of formatters, wrapped in a lambda that supplies a dictionary with keyword options to the formatters.
# The tuple contains indices :
# 0 - Formats text sent to the console display
# 1 - Formats text sent to the TTS backend
# 2 - Formats text to be saved as user message in the chat history
# 3 - formats text to be saved as AI message in the chat history
mode_formatters = {
    "default": lambda d: (DoNothing, DoNothing, DoNothing, CleanResponse),
    "chat": lambda d: (
        DoNothing,
        NicknameRemover(d["chat_ai"]),
        NicknameFormatter(d["chat_user"]),
        ChatFormatter(d["chat_ai"]),
    ),
}


class Plumbing(object):
    def __init__(self, options={}, initial_cli_prompt="", tags={}):
        # the printerr stuff needs to happen early
        self._printerr_buffer = []
        self._initial_printerr_callback = lambda w: self._printerr_buffer.append(w)
        util.printerr_callback = self._initial_printerr_callback
        if not(options["stderr"]):
            util.printerr_disabled = True
        # this is for the websock clients
        self.stderr_token = "[|STDER|]:"
        self._stdout_ringbuffer = ""
        self._stdout_ringbuffer_size = 1024
        self._frozen = False
        self._freeze_queue = Queue()
        self.options = options
        self.tags = tags
        self.backend = None
        self.template = None        
        self.initializeBackend(self.getOption("backend"), self.getOption("endpoint"))
        self.session = Session(chat_user=options.get("chat_user", ""))
        # FIXME: make this a function returning backend.getlAStResult()
        self.lastResult = {}
        self._lastInteraction = 0
        self.tts_flag = False
        self.initial_print_flag = False
        self.initial_cli_prompt = initial_cli_prompt
        self.stream_queue = []
        self.stream_sentence_queue = []
        self._stream_only_once_token_bag = set()
        self.images = {}
        # flag to show wether image data needs to be resent to the backend
        self._images_dirty = False
        self._lastPrompt = ""
        self._dirtyContextLlama = False
        self._stop_generation = threading.Event()
        # indicates generation work being done, really only used to print the goddamn cli prompt
        self._busy = threading.Event()
        self._busy.set()
        self._triggered = False
        self._smartShifted = False
        self._systemTokenCount = None
        self.continue_with = ""
        self.tts = None
        self.multiline_buffer = ""
        if self.getOption("json"):
            self.setOption("grammar", getJSONGrammar())
            del self.options["json"]
        elif self.getOption("grammar_file"):
            self.loadGrammar(self.getOption("grammar_file"))
        else:
            self.setOption("grammar", "")
        # template
        self.loadTemplate(self.getOption("prompt_format"), startup=True)


        # whisper stuff. We do this with a special init function because it's lazy
        self.whisper = self._newTranscriber()
        self.ct = None
        self._defaultSIGINTHandler = signal.getsignal(signal.SIGINT)
        self._transcription_suspended = False

        # imagewatching
        self.image_watch = None

        # http server
        self.http_server = None
        self.http_thread = None

        # websock server
        self.websock_thread = None
        self.websock_server = None
        self.websock_clients = []
        self.websock_server_running = threading.Event()
        self.websock_msg_queue = Queue()

        # formatters is to be idnexed with modes
        self.setMode(self.getOption("mode"))
        self.running = True
        self._startCLIPrinter()

    def getTags(self):
        return self.tags | {
            param.name: param
            for param in backends.sampling_parameter_tags.values()
            if param.name in self.getBackend().sampling_parameters().keys()
        }

    def initializeBackend(self, backend, endpoint):
        api_key = self.getOption("openai_api_key")
        if backend == LLMBackend.llamacpp.name:
            self.backend = LlamaCPPBackend(endpoint)
        elif backend == LLMBackend.openai.name:
            if not api_key:
                printerr(
                    "error: OpenAI API key is required for the OpenAI backend. Did you forget to provide --openai_api_key?"
                )
                # this is rough but we are in init phase so it's ok
                sys.exit()
            self.backend = OpenAIBackend(api_key)
            self.setOption("prompt_format", "auto")
        elif backend == LLMBackend.generic.name:
            self.backend = OpenAIBackend(api_key, endpoint=endpoint)
            self.setOption("prompt_format", "auto")
        elif backend == LLMBackend.legacy.name:
            self.backend = OpenAILegacyBackend(api_key, endpoint=endpoint)
        else:
            # Handle other backends...
            pass

        # fill in the default sampler parameters that were not shown in the command line arguments, but are still supported
        for param in self.backend.sampling_parameters().values():
            if param.name not in self.options.keys():
                self.setOption(param.name, param.default_value)

    def getBackend(self):
        return self.backend

    def justUsedTools(self) -> bool:
        # FIXME: not sure if this is the place to do it/right way to do it
        # if the last message in the history has role 'tools', it means
        # we just send results, so we don't use tools for this interaction
        try:
            return self.session.stories.get().getData()[-1].role == "tool"
        except:
            printerr("warning: Tried to check for tools use with empty history.")
        return False

    def makeGeneratePayload(self, text):
        d = {}

        # gather set sampling parameters and warn user for unsupported ones
        all_params = backends.sampling_parameters.keys()
        supported_params = self.getBackend().sampling_parameters().keys()
        for param in all_params:
            if param in self.options:
                if param not in supported_params and not (
                    self.getOption("force_params")
                ):
                    if self.getOption("warn_unsupported_sampling_parameter"):
                        printerr(
                            "warning: Sampling parameter '"
                            + param
                            + "' is not supported by the "
                            + str(self.getBackend().getName())
                            + " backend. Set force_params to true to send it anyway."
                        )
                    else:
                        continue
                else:
                    d[param] = self.getOption(param)

        # some special ones
        for k, v in special_parameters.items():
            d[k] = v if not(self.getOption(k)) else self.getOption(k)

        # don't override grammar if it's null, rather not send it at all
        if "grammar" in d:
            if not(d["grammar"]):
                del d["grammar"]
            
        # just throw toools in for backends that can use them, unless user disabled
        if self.getOption("use_tools"):
            if not (self.justUsedTools()):
                d["tools"] = [tool.model_dump() for tool in self.session.tools]
                # FIXME: necessary currently with llamacpp
                if "grammar" in d:
                    del d["grammar"]

        if (
            self.getOption("backend") == LLMBackend.generic.name
            or self.getOption("backend") == LLMBackend.openai.name
            or self.getOption("prompt_format")
            == PromptFormatTemplateSpecialValue.auto.name
        ):
            # openai chat/completion needs the system prompt and story
            # we also do this for llama in "auto" template mode
            d["system"] = self.session.getSystem()
            d["story"] = copy.deepcopy(self.session.stories.get().to_json())
        else:
            # all others get the text
            # FIXME: there is some deep mishandling here because this uses the text parameter while oai endpoitns use the story. must investigate this
            d["prompt"] = text

        # images
        # currently only supporting /v1/chat/completions style endpoints
        if not (
            self.getOption("backend") == LLMBackend.generic.name
            or self.getOption("backend") == LLMBackend.openai.name
        ):
            return d
        d["images"] = self.images
        self._images_dirty = False
        # FIXME: experimental. keep the images only in story log?

        # FIXME: place image hint here maybe
        # d["image_message"] =

        # disabled because llama-server hasn't supported this in a while
        # d["image_data"] = [packageImageDataLlamacpp(d["data"], id) for (id, d) in self.images.items()]
        return d

    def isContinue(self):
        return self.getOption("continue")

    def resetContinue(self):
        self.setOption("continue", False)

    def _newTranscriber(self):
        # makes a lazy WhisperTranscriber, because model loading can be slow
        # yes this is hacky but some platforms (renpy) can't handle torch, which the whisper models rely on
        def delayWhisper():
            from ghostbox.transcribe import WhisperTranscriber

            return WhisperTranscriber(
                model_name=self.getOption("whisper_model"),
                silence_threshold=self.getOption("audio_silence_threshold"),
                input_func=lambda: printerr("Started Recording. Hit enter to stop."),
            )

        return Proxy(delayWhisper)

    def continueWith(self, newUserInput):
        # FIXME: the entire 'continue' architecture is a trashfire. This should be refactored along with other modeswitching/input rewriting stuff in the main loop
        self.setOption("continue", "1")
        self.continue_with = newUserInput

    def popContinueString(self):
        self.setOption("continue", False)
        tmp = self.continue_with
        self.continue_with = ""
        return tmp

    def loadGrammar(self, grammar_file):
        if os.path.isfile(grammar_file):
            w = open(grammar_file, "r").read()
            self.setOption("grammar", w)
        else:
            self.setOption("grammar", "")
            printerr(
                "warning: grammar file "
                + grammar_file
                + " could not be loaded: file not found."
            )

    def guessPromptFormat(self):
        """Uses any trick it can do guess the prompt format template. Returns a string like 'chat-ml', 'alpaca', etc."""
        # see if we can find llm_layers
        try:
            data = loadLayersFile()
        except:
            printerr("warning: Couldn't load layers file " + getLayersFile())

        try:
            models = dirtyGetJSON(self.getOption("endpoint") + "/v1/models").get(
                "data", []
            )
            # hope it's just one
            model = os.path.basename(models[0]["id"])
        except:
            printerr(traceback.format_exc())
            printerr("Failed to guess prompt format. Defaulting.")
            return "raw"

        # check if model is in layers file
        for d in data:
            if "name" not in d.keys():
                continue
            if d["name"].lower() == model.lower():
                if d["prompt_format"]:
                    # success
                    return d["prompt_format"]

        # FIXME: at this point it's not in the layers file, but we still have a model name. consider googling it on hugginface and grepping the html
        printerr(
            "Failed to guess prompt format after exhausting all options 😦. Defaulting."
        )
        return "raw"

    def loadTemplate(self, name, startup=False):
        # this is so llamacpp uses the /chat/completions endpoint if we use auto templating
        # has no effect on other backends
        self.getBackend().configure(
            {
                "llamacpp_use_chat_completion_endpoint": (
                    True
                    if name == PromptFormatTemplateSpecialValue.auto.name
                    else False
                )
            }
        )

        # special cases
        if name == "auto":
            if startup and (self.template is not None):
                # don't set this twice
                return
            printerr(
                "Prompt format template set to 'auto': Formatting is handled server side."
            )
            self.template = RawTemplate()
            return
        if name == "guess":
            name = self.guessPromptFormat()

        allpaths = [p + "/" + name for p in self.getOption("template_include")]
        for path in allpaths:
            path = os.path.normpath(path)
            if not (os.path.isdir(path)):
                failure = (
                    "Could not find prompt format template '"
                    + name
                    + "'. Did you supply a --template_include option?"
                )
                continue
            failure = False
            try:
                template = FilePFTemplate(path)
                break
            except FileNotFoundError as e:
                failure = e

        if failure:
            printerr("warning: " + str(failure) + "\nDefaulting to 'raw' template.")
            self.template = RawTemplate()
            self.options["prompt_format"] = "raw"
            return
        # actually load template
        # first unload old stops
        self.options["stop"] = list(
            filter(lambda w: w not in self.template.stops(), self.options["stop"])
        )

        self.template = template
        for w in template.stops():
            if not (w):
                continue
            self.appendOption("stop", w)
        self.options["prompt_format"] = name
        printerr("Using '" + name + "' as prompt format template.")

    def loadConfig(self, json_data, override=True):
        """Loads a config file provided as json into options. Override=False means that command line options that have been provided will not be overriden by the config file."""
        d = json.loads(json_data)
        if type(d) != type({}):
            return "error loading config: Not a dictionary."
        if not (override):
            # drop keys in the config that can be found in the command line arguments
            # have to do the one letter options manually
            letterMap = {
                "u": "chat_user",
                "c": "character_folder",
                "V": "tts_voice",
                "T": "prompt_format",
            }
            for arg in sys.argv:
                if not (arg.startswith("-")):
                    continue
                key = stripLeadingHyphens(arg)
                # "no-" is for e.g. '--no-tts'
                if key in d or "no-" + key in d:
                    del d[key]
                for letter, full_arg in letterMap.items():
                    if key.startswith(letter):
                        if full_arg in d:
                            del d[full_arg]

        # now actually load the options, with a partial ordering
        items = sorted(
            d.items(), key=cmp_to_key(lambda a, b: -1 if a[0] == "mode" else 1)
        )
        for k, v in items:
            self.setOption(k, v)
        return ""

    def showCLIPrompt(self):
        if self.isMultilineBuffering():
            return ""

        if self.getOption("cli_prompt") == "":
            return ""

        f = IdentityFormatter()
        if self.getOption("color"):
            f = ColorFormatter(self.getOption("cli_prompt_color")) + f

        return self.session.expandVars(f.format(self.getOption("cli_prompt")))

    def getMode(self):
        w = self.getOption("mode")
        if not (self.isValidMode(w)):
            return "default"
        return w

    def isValidMode(self, mode):
        return mode in mode_formatters

    def setMode(self, mode):
        # FIXME: this isn't affected by freeze, but since most things go through setOption we should be fine, right? ... right?
        if not (self.isValidMode(mode)):
            return

        self.options["mode"] = mode
        if mode.startswith("chat"):
            userPrompt = mkChatPrompt(self.getOption("chat_user"))
            self.setOption("cli_prompt", "\n" + userPrompt)
            self.appendOption("stop", userPrompt, duplicates=False)
            self.appendOption("stop", userPrompt.strip(), duplicates=False)

        else:  # default
            self.options["cli_prompt"] = self.initial_cli_prompt

    def getOption(self, key):
        return self.options.get(key, None)

    def optionDiffers(self, name, newValue):
        if name not in self.options:
            return True
        return self.getOption(name) != newValue

    def getFormatters(self):
        mode = self.getOption("mode")
        if not (self.isValidMode(mode)):
            mode = "default"
            printerr("warning: Unsupported mode '" + mode + "'.. Using 'default'.")
        return mode_formatters[mode](self.options | self.session.getVars())

    def getDisplayFormatter(self):
        return self.getFormatters()[0]

    def getTTSFormatter(self):
        return self.getFormatters()[1]

    def getUserFormatter(self):
        return self.getFormatters()[2]

    def getAIColorFormatter(self):
        if self.getOption("color"):
            return ColorFormatter(
                self.getOption("text_ai_color"), self.getOption("text_ai_style")
            )
        return IdentityFormatter()

    def getAIFormatter(self, with_color=False):
        return self.getAIColorFormatter() + self.getFormatters()[3]

    def addUserText(self, w, image_id=None):
        if w:
            self.session.stories.get().addUserText(
                self.getUserFormatter().format(w), image_id=image_id
            )

    def addAIText(self, w):
        """Adds text toe the AI's history in a cleanly formatted way according to the AI formatter. Returns the formatted addition or empty string if nothing was added.."""
        if w == "":
            return ""

        if self.getOption("continue"):
            continuation = self.getAIFormatter().format(w)
            self.session.stories.get().extendAssistantText(continuation)
            return continuation

        # hint may have been sent to the backend but we have to add it to the story ourselves.
        if self.getOption("hint_sticky"):
            hint = self.getOption("hint")
        else:
            hint = ""


        if ((rformat := self.getOption("response_format")) and
            (rformat["type"] != "text")):
            # don't use formatters if user expects structured data
            addition = hint + w
        else:
            addition = self.getAIFormatter().format(hint + w)
        self.session.stories.get().addAssistantText(addition)
        return addition

    def addSystemText(self, w):
        """Add a system message to the chat log, i.e. add a message with role=system. This is mostly used for tool/function call results."""
        if w == "":
            return

        # FIXME: we're currently rawdogging system msgs. is this correct?
        self.session.stories.get().addSystemText(w)

    def applyTools(
        self, w: str = "", json={}
    ) -> Tuple[List[ChatMessage], ChatMessage]:
        """Takes an AI generated string w and optionally the json result from an OpenAI API compatible tool request. Tries to detect a tool request in both. If detected, will apply the tools and then return their results as structured data, as well as the fully parsed original tool call.
        Ideally the JSON result makes tool use obvious through the field
        json["choices"][0]["finish_reason"] == "tool_calls"
        However, not all models will do this consistently and those who can may fail at higher temperatures.
        So as a fallback we check the raw w for json data that corresponds to tools use.
        :param w: Unstructured AI generation string that may contain a tool request.
        :param json: Structured json that was returned by the AI that may contain a tool request.
        :return: A pair of tool results, and the original tool requesting message, both as json dicts. If no tool request was detected, both are empty dicts.
        """
        import json as j

        nothing = ([], {})

        if not (self.getOption("use_tools")):
            return nothing

        try:
            if json["choices"][0]["finish_reason"] != "tool_calls":
                return nothing
            tool_request = json["choices"][0]["message"]
            tool_calls = tool_request["tool_calls"]
        except:
            tool_calls = []
            printerr(traceback.format_exc())
        # FIXME: add heuristic for scanning w for possible tool request
        # ...
        # here, we know it was a tool request and we have tool_calls
        if tool_calls == []:
            # either none requested or something went wrong. anyhow.
            return nothing

        tool_results = []
        try:
            for tool_call in tool_calls:
                if self.getOption("verbose"):
                    printerr(j.dumps(tool_call, indent=4))

                tool = tool_call["function"]
                try:
                    params = j.loads(tool["arguments"])
                except:
                    if self.getOption("verbose"):
                        printerr(
                            "warning: Couldn't pass tool arguments as json: "
                            + tool["arguments"]
                        )
                        continue
                maybeResult = self.session.callTool(tool["name"], params)
                tool_results.append(
                    agency.makeToolResult(tool["name"], maybeResult, tool_call["id"])
                )
        except:
            printerr("warning: Caught the following exception while applying tools.")
            printerr(traceback.format_exc())

        if tool_results == []:
            return nothing

        # NOTE: the responses from the server come back more complex than what is later supposed to be sent in the chat history
        # specifically, a message with role "assistant", conten null, and tool_calls=... should *not* have the tool c<calls be a layered dictionary with type and function key, it's just a plain list of dicts of names and arguments
        # I have no idea why this is inconsistent but that's how it is, and that's the reason for the repackaging below.
        tool_request_msg = ChatMessage(role="assistant",
                                       tool_calls=[ToolCall(function=FunctionCall(name=fcall["function"]["name"], arguments=fcall["function"]["arguments"])) for fcall in tool_request["tool_calls"]])
        
        return tool_results, tool_request_msg



    def _formatToolResults(self, results):
        """Based on a list of dictionaries, returns a string that represents the tool outputs to an LLM."""
        # FIXME: currently only intended for command-r
        w = ""
        # w += "<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>"
        for tool in results:
            if len(results) > 1:
                # tell the LLM what tool has which results
                w += "## " + tool["tool_name"] + "\n\n"
            w += agency.showToolResult(tool["output"])
        # w += "<|END_OF_TURN_TOKEN|>"
        return w

    def appendOption(self, name, value, duplicates=True):
        if name not in self.options:
            printerr("warning: unrecognized option '" + name + "'")
            return

        xs = self.getOption(name)
        if type(xs) != type([]):
            printerr(
                "warning: attempt to append to '" + name + "' when it is not a list."
            )
            return

        if not (duplicates):
            if value in xs:
                return
        self.options[name].append(value)

    def freeze(self):
        self._frozen = True

    def unfreeze(self):
        self._frozen = False
        while not (self._freeze_queue.empty()):
            (name, value) = self._freeze_queue.get(block=False)
            self.setOption(name, value)
            if self.getOption("verbose"):
                printerr("unfreezing " + name + " : " + str(value))

    def setOption(self, name, value):
        # we freeze state during some parts of execution, applying options after we unfreeze
        if self._frozen:
            self._freeze_queue.put((name, value))
            return

        # mode gets to call dibs
        if name == "mode":
            self.setMode(value)
            return

        oldValue = self.getOption(name)
        differs = self.optionDiffers(name, value)
        self.options[name] = value
        # for some options we do extra stuff
        if (
            name == "tts_voice"
            or name == "tts_volume"
            or name == "tts_tortoise_quality"
        ) and self.getOption("tts"):
            # we don't want to restart tts on /restart
            if differs:
                # printerr("Restarting TTS.")
                self.tts_flag = True  # restart TTS
        elif name == "no-tts":
            self.setOption("tts", not (value))
        elif name == "whisper_model":
            self.whisper = self._newTranscriber()
        # elif name == "cli_prompt":
        # self.initial_cli_prompt = value
        elif name == "tts_websock":
            if value:
                self.setOption("tts_output_method", TTSOutputMethod.websock.name)
            else:
                self.setOption("tts_output_method", TTSOutputMethod.default.name)
            if differs and self.getOption("tts"):
                self.tts_flag = True  # restart TTS
        elif name == "max_context_length":
            self._dirtyContextLlama = False
        elif name == "prompt_format":
            self.loadTemplate(value)
        elif name == "chat_user":
            # userpormpt might be in stopwords, which we have to refresh
            prompt = mkChatPrompt(oldValue)
            badwords = [prompt, prompt.strip()]
            self.options["stop"] = list(
                filter(lambda w: w not in badwords, self.getOption("stop"))
            )

            # and this will add the new username if it's chat mode
            self.setMode(self.getMode())
        elif name == "stop":
            # user may change this in a char config.json. This may be buggy, but let's make sure at least the template is in.
            if self.template:
                self.options[name] = self.template.stops()
            else:
                self.options[name] = []
            self.options[name] += value
        elif name == "chat_ai":
            self.session.setVar(name, value)
        elif name == "http" and differs:
            if value:
                self._initializeHTTP()
            else:
                self._stopHTTP()
        elif name == "websock" and differs:
            if value:
                self.initializeWebsock()
            else:
                self.stopWebsock()
        elif name == "image_watch" and differs:
            if value:
                self.startImageWatch()
            else:
                self.stopImageWatch()
        elif name == "audio" and differs:
            if value:
                self.startAudioTranscription()
            else:
                self.stopAudioTranscription()
        elif name == "stderr":
            util.printerr_disabled = not(value)
        return ""

    def _ctPauseHandler(self, sig, frame):
        printerr("Recording paused. CTRL + c to resume, /text to stop.")
        self.ct.pause()
        signal.signal(signal.SIGINT, self._ctResumeHandler)

    def _ctResumeHandler(self, sig, frame):
        printerr("Recording resumed. CTRL + c to interrupt.")
        self.ct.resume()
        signal.signal(signal.SIGINT, self._ctPauseHandler)

    def _imageWatchCallback(self, image_path, image_id):
        newStory(self, [])
        w = self.getOption("image_watch_msg")
        if w == "":
            return

        self.loadImage(image_path, image_id)
        # FIXME: what if loadImage fails?
        (modified_w, hint) = self.modifyInput(w)
        self.addUserText(modified_w, image_id=image_id)
        image_watch_hint = self.getOption("image_watch_hint")
        self.addAIText(self.communicate(self.buildPrompt(hint + image_watch_hint)))
        # print(self.showCLIPrompt(), end="")

    def _print_generation_callback(self, result_str):
        """Pass this as callback to self.interact for a nice simple console printout of generations."""
        if result_str == "":
            return

        # self.print("\n\r" + (" " * len(self.showCLIPrompt())) + "\r", end="", tts=False)
        if not (self.getOption("stream")):
            self.print(
                self.getAIFormatter(with_color=self.getOption("color")).format(
                    result_str
                ),
                end="",
            )
        # self.print(self.showCLIPrompt(), end="", tts=False)
        return

    def suspendTranscription(self):
        """Renders transcription unresponsive until an activation phrase is heard or unsuspendTranscription is called."""
        self._transcription_suspended = True
        self.verbose("Suspending audio interaction.")

    def unsuspendTranscription(self):
        """Resumes interacting by audio after a suspension."""
        self._transcription_suspended = False
        self.verbose("Resuming audio interaction.")

    def modifyTranscription(self, w):
        """Checks wether an incoming user transcription by the whisper model contains activation phrases or is within timing etc. Returns the modified transcription, or the empty string if activation didn't trigger."""

        # want to match fuzzy, so strip of all punctuation etc.
        # FIXME: no need to do this here, again and again
        phrase = (
            self.getOption("audio_activation_phrase")
            .translate(str.maketrans("", "", string.punctuation))
            .strip()
            .lower()
        )
        if not (phrase):
            # user doesn't require phrase. proceed. unless we are suspended
            if not (self._transcription_suspended):
                return w

        # ok we require an activation phrase, but are we within the grace period where none is required?
        if (t := self.getOption("audio_activation_period_ms")) > 0:
            # grace period doesn't matter if we have been suspended.
            if (time_ms() - self._lastInteraction) <= t and not (
                self._transcription_suspended
            ):
                # FIXME: returning here means there might be a phrase in the input even when phrase_keep is false. Maybe it doesn't matter?
                return w

        # now we need an activation phrase or it ain't happenin
        # strip the transcription
        test = w.translate(str.maketrans("", "", string.punctuation)).strip().lower()
        try:
            n = test.index(phrase)
        except ValueError:
            # no biggie, phrase wasn't found. we're done here
            return ""

        # ok it was found
        self.unsuspendTranscription()
        if self.getOption("audio_activation_phrase_keep"):
            return w
        # FIXME: this is 100% not correct, but it may be good enough
        return w.replace(phrase, "").replace(
            self.getOption("audio_activation_phrase"), ""
        )

    def printActivationPhraseWarning(self):
        n = self.getOption("warn_audio_activation_phrase")
        if not (n):
            return
        w = (
            "warning: Your message was received but triggered no response, because the audio activation phrase was not found in it. Hint: The activation phrase is '"
            + self.getOption("audio_activation_phrase")
            + "'."
        )
        if n != -1:
            w += " This warning will only be shown once."
            self.setOption("warn_audio_activation_phrase", False)
        printerr(w)

    def _transcriptionCallback(self, w):
        """Supposed to be called whenever the whisper model has successfully transcribed a phrase."""
        if self.getOption("audio_show_transcript"):
            self.print(w, tts=False)

        w = self.modifyTranscription(w)
        if not (w):
            self.printActivationPhraseWarning()
            return

        self.interact(w, self._print_generation_callback)

    def _transcriptionOnThresholdCallback(self):
        """Gets called whenever the continuous transcriber picks up audio above the threshold."""
        if self.getOption("audio_interrupt"):
            self.stopAll()

    def _streamCallback(self, token, user_callback=None, only_once=None):
        if only_once not in self._stream_only_once_token_bag:
            # this is so that we can print tokens/sentences without interrupting the TTS every time
            # except that we want to interrupt the TTS exactly once -> when we start streaming
            # If you do this elsewhere, i.e. communicate(), we risk a race condition
            self._stream_only_once_token_bag.add(only_once)
            if self.getOption("tts_interrupt"):
                self.stopTTS()

        if user_callback is None:
            user_callback = lambda x: x
        f = lambda w: self.print(
            self.getAIColorFormatter().format(w), end="", flush=True, interrupt=False
        )

        self.stream_queue.append(token)
        method = self.getOption("stream_flush")
        if method == "token":
            f(token)
            user_callback(token)

        elif method == "sentence":
            self.stream_sentence_queue.append(token)
            if "\n" in token:
                w = "".join(self.stream_sentence_queue)
            else:
                w = IncompleteSentenceCleaner().format(
                    "".join(self.stream_sentence_queue)
                )
                if w.strip() == "":
                    # not a complete sentence yet, let's keep building it
                    return
            # w is a complete sentence, or a full line
            self.stream_sentence_queue = []
            f(w)
            user_callback(w)

    def flushStreamQueue(self):
        w = "".join(self.stream_queue)
        self.stream_queue = []
        self.stream_sentence_queue = []
        return w

    def isAudioTranscribing(self):
        return self.ct is not None and self.ct.running

    def startImageWatch(self):
        dir = self.getOption("image_watch_dir")
        printerr("Watching for new images in " + dir + ".")
        self.image_watch = AutoImageProvider(dir, self._imageWatchCallback)

    def startAudioTranscription(self):
        printerr("Beginning automatic transcription. CTRL + c to pause.")
        if self.ct:
            self.ct.stop()
        self.ct = self.whisper.transcribeContinuously(
            callback=self._transcriptionCallback,
            on_threshold=self._transcriptionOnThresholdCallback,
            websock=self.getOption("audio_websock"),
            websock_host=self.getOption("audio_websock_host"),
            websock_port=self.getOption("audio_websock_port"),
        )
        signal.signal(signal.SIGINT, self._ctPauseHandler)

    def stopImageWatch(self):
        if self.image_watch:
            printerr("Stopping watching of images.")
            self.image_watch.stop()

    def stopAudioTranscription(self):
        if self.ct:
            printerr("Stopping automatic audio transcription.")
            self.ct.stop()
        self.ct = None
        signal.signal(signal.SIGINT, self._defaultSIGINTHandler)

    def initializeTTS(self):
        tts_program = self.getOption("tts_program")
        candidate = os.getcwd() + "/" + tts_program
        if os.path.isfile(candidate):
            tts_program = candidate

        if not (tts_program):
            return "Cannot initialize TTS: No TTS program set."

        # pick a voice in case of random
        if self.getOption("tts_voice") == "random":
            # no setOption to avoid recursion
            voices = getVoices(self)
            if voices == []:
                return "error: Cannot initialize TTS: No voices available."
            self.options["tts_voice"] = random.choice(voices)
            printerr("Voice '" + self.getOption("tts_voice") + "' was chosen randomly.")

        if self.tts is not None:
            # restarting
            try:
                if not (self.tts.is_running()):
                    self.tts.close()
            except ProcessLookupError:
                printerr(
                    "warning: TTS process got lost somehow. Probably not a big deal."
                )

        # let's make the path issue absolutely clear. We only track tts_voice_dir, but to the underlying tts program, we expose the tts_voice_abs_dir environment variable, which contains the absolute path to the voice dir
        # FIXME: rewrite the entire path architecture
        tts_voice_abs_dir = self.tryGetAbsVoiceDir()
        self.tts = feedwater.run(
            tts_program,
            env=envFromDict(
                self.options
                | {
                    "tts_voice_abs_dir": tts_voice_abs_dir,
                    "ONNX_PROVIDER": "CUDAExecutionProvider",
                }
            ),
        )
        self.setOption("stream_flush", "sentence")
        if self.getOption("verbose"):
            printerr(
                " Automatically set stream_flush to 'sentence'. This is recommended with TTS. Manually reset it to 'token' if you really want."
            )
        return "TTS initialized."

    def tryGetAbsVoiceDir(self):
        # this is sort of a heuristic. The problem is that we allow multiple include dirs, but have only one voice dir. So right now we must pick the best from a number of candidates.
        if os.path.isabs(self.getOption("tts_voice_dir")) and os.path.isdir(
            self.getOption("tts_voice_dir")
        ):
            return self.getOption("tts_voice_dir")

        winner = ""
        ok = False
        for path in self.getOption("include"):
            file = path + "/" + self.getOption("tts_voice_dir")
            if os.path.isdir(file):
                winner = file
                ok = True
                break

        abs_dir = os.path.abspath(winner)
        if not (ok):
            printerr(
                "warning: Couldn't cleanly determine tts_voice_dir. Guessing it is '"
                + abs_dir
                + "'."
            )
        return abs_dir

    def stopTTS(self):
        # FIXME: not implemented for all TTS clients
        if self.tts is None:
            return

        self.tts.write_line("<clear>")

    def communicateTTS(self, w, interrupt=False):
        if not (self.getOption("tts")):
            return ""

        if interrupt:
            self.stopTTS()

        # strip color codes - this would be nicer by just disabling color, but the fact of the matter is we sometimes want color printing on console and tts at the same time. At least regex is fast.
        w = stripANSI(w)

        # strip whitespace, we especially don't want to send pure whitespace like ' \n' or '  ' to a tts, this is known to crash some of them. It also shouldn't change the resulting output.
        w = w.strip()

        
        if self.tts is None or not (self.tts.is_running()):
            self.setOption("tts", False)
            printerr(
                "error: TTS is dead. You may attempt to restart with /tts. Check errors with /ttsdebug ."
            )
            return ""
        self.tts.write_line(w)
        return w

    def verbose(self, w):
        """Prints to stderr but only in verbose mode."""
        if self.getOption("verbose"):
            printerr(w)

    def console(self, w):
        """Print something to stderr. This exists to be used in tools.py via dependency injection."""
        printerr(w)

    def console_me(self, w):
        """Prints w to stderr, prepended by the AI name. This exists because it is a very common pattern to be used in tools.py"""
        printerr(self.getOption("chat_ai") + w)

    def _ringbuffer(self, w: str) -> str:
        """Takes a string w and returns it unchanged, but copies it to a local ringbuffer.
        The purpose of this is solely to let the CLI worker check if we have a newline at the end of the output.
        Note: not actually a ringbuffer."""
        self._stdout_ringbuffer = (self._stdout_ringbuffer + w)[
            : self._stdout_ringbuffer_size
        ]
        return w

    def print(
        self,
        w,
        end="\n",
        flush=False,
        color="",
        style="",
        tts=True,
        interrupt=None,
        websock=True,
    ):
        # either prints, speaks, or both, depending on settings
        if w == "":
            return

        if self.getOption("websock"):
            self.websockSend(w)

        if self.getOption("quiet"):
            return

        if tts and self.getOption("tts") and w != self.showCLIPrompt():
            self.communicateTTS(
                self.getTTSFormatter().format(w) + end,
                interrupt=(
                    self.getOption("tts_interrupt") if interrupt is None else interrupt
                ),
            )
            if not (self.getOption("tts_subtitles")):
                return

        if not (color) and not (style):
            print(
                self._ringbuffer(self.getDisplayFormatter().format(w)),
                end=end,
                flush=flush,
            )
        else:
            print(
                self._ringbuffer(
                    style
                    + color
                    + self.getDisplayFormatter().format(w)
                    + Fore.RESET
                    + Style.RESET_ALL
                ),
                end=end,
                flush=flush,
            )

    def replaceForbidden(self, w):
        for forbidden in self.getOption("forbid_strings"):
            w = w.replace(forbidden, "")
        return w

    def bufferMultilineInput(self, w):
        if self.getOption("multiline"):
            # does not expect backslash at end
            self.multiline_buffer += w + "\n"
        else:
            # expects strings with \ at the end
            self.multiline_buffer += w[:-1] + "\n"

    def isMultilineBuffering(self):
        return self.multiline_buffer != ""

    def flushMultilineBuffer(self):
        w = self.multiline_buffer
        self.multiline_buffer = ""
        return w

    def expandDynamicFileVars(self, w):
        """Expands strings of the form `{[FILEPATH]}` by replacing them with the contents of FILE1 if it is found and readable."""
        if not (self.getOption("dynamic_file_vars")):
            return w

        pattern = r"\{\[(.*?)\]\}"
        matches = re.findall(pattern, w)

        for match in matches:
            var = "{[" + match + "]}"
            # users will probably mess around with this so we wanna be real careful
            common_error_msg = "error: Unable to expand '" + var + "'. "
            try:
                content = open(match, "r").read()
                w = w.replace(var, content)
            except FileNotFoundError:
                printerr(common_error_msg + "File not found.")
            except PermissionError:
                printerr(common_error_msg + "Operation not permitted.")
            except IsADirectoryError:
                printerr(common_error_msg + "'" + match + "' is a directory.")
            except UnicodeDecodeError:
                printerr(common_error_msg + "Unable to decode the file.")
            except IOError:
                printerr(common_error_msg + "Bad news: IO error.")
            except OSError:
                printerr(common_error_msg + "Operating system has signaled an error.")
            except:
                printerr(
                    common_error_msg
                    + "An unknown exception has occurred. Here's the full traceback."
                )
                printerr(traceback.format_exc())
        return w

    def modifyInput(prog, w):
        """Takes user input (w), returns pair of (modified user input, and a hint to give to the ai."""
        if (
            prog.isContinue() and prog.continue_with == ""
        ):  # user entered /cont or equivalent
            if prog.getMode().startswith("chat"):
                # prevent AI from talking for us
                if prog.showStory().endswith("\n"):
                    return ("", prog.adjustForChat("")[1])
            return ("", "")

        if (
            prog.continue_with != ""
        ):  # user input has been replaced with something else, e.g. a transcription
            w = prog.popContinueString()

        # dynamic vars must come before other vars for security reasons
        w = prog.expandDynamicFileVars(w)
        w = prog.session.expandVars(w)
        (w, ai_hint) = prog.adjustForChat(w)

        # tool_hint = agency.makeToolInstructionMsg() if prog.getOption("use_tools") else ""

        # user may also provide a hint. unclear how to best append it, we put it at the end
        user_hint = prog.session.expandVars(prog.getOption("hint"))
        if user_hint and prog.getOption("warn_hint"):
            printerr(
                "warning: Hint is set. Try /raw to see what you're sending. Use /set hint '' to disable the hint, or /set warn_hint False to suppress this message."
            )

        return (w, ai_hint + user_hint)

    def getSystemTokenCount(self):
        """Returns the number of tokens in system msg. The value is cached per session. Note that this adds +1 for the BOS token."""
        if self._systemTokenCount is None:
            # self._systemTokenCount = len(self.getBackend().tokenize(self.session.getSystem())) + 1
            self._systemTokenCount = (
                len(self.getBackend().tokenize(self.showSystem())) + 1
            )
        return self._systemTokenCount

    def getTemplate(self):
        return self.template

    def getRawTemplate(self):
        return RawTemplate()

    def showSystem(self):
        # vars contains system_msg and others that may or may not be replaced in the template
        vars = self.session.getVars().copy()
        if (
            self.getOption("use_tools")
            and (tool_hint := self.getOption("tools_hint")) != ""
        ):
            vars["system_msg"] += "\n" + tools_hint
        return self.getTemplate().header(**vars)

    def showStory(self, story_folder=None, append_hint=True) -> str:
        """Returns the current story as a unformatted string, injecting the current prompt template."""
        # new and possibly FIXME: we need to add another hint from agency.makeToolInstructionMsg when using tools, so we disable the hint here
        if self.getOption("use_tools"):
            append_hint = False

        if story_folder is None:
            sf = self.session.stories
        else:
            sf = story_folder
        if self.isContinue():
            # user hit enter and wants ai to keep talking. this is kind of like using the entire last reply as a hint -> no templating needed
            return self.getRawTemplate().body(
                sf.get(), append_hint, **self.session.getVars()
            )
        return self.getTemplate().body(sf.get(), append_hint, **self.session.getVars())

    def formatStory(self, story_folder=None, with_color=False):
        """Pretty print the current story (or a provided one) in a nicely formatted way. Returns pretty story as a string."""
        if story_folder is None:
            sf = self.session.stories
        else:
            sf = story_folder

        ws = []
        for item in sf.get().getData():
            if type(w := item.content) != str:
                continue
            if item.role == "assistant":
                ws.append(
                    (self.getAIColorFormatter() + self.getDisplayFormatter()).format(w)
                )
                continue
            ws.append(self.getDisplayFormatter().format(w))
        return "\n".join(ws)

    def buildPrompt(self, hint=""):
        """Takes an input string w and returns the full history (including system msg) + w, but adjusted to fit into the context given by max_context_length. This is done in a complicated but very smart way.
        returns - A string ready to be sent to the backend, including the full conversation history, and guaranteed to carry the system msg.
        """
        # problem: the llm can only process text equal to or smaller than the context window
        # dumb solution (ds): make a ringbuffer, append at end, throw away the beginning until it fits into context window
        # problem with dumb solution: the system msg gets thrown out and the AI forgets the basics of who it is
        # slightly less dumb solution (slds): keep the system_msg at all costs, throw first half of the rest away when context is exceeded this is llama.cpp solution, but only if you supply n_keep = tokens of system_msg koboldcpp does this too, but they seem to be a bit smarter about it and make it more convenient. Advantage of this approach is that you will make better use of the cache, since at least half of the prompt after the system msg is guaranteed to be in cache.
        # problem with slds: This can cut off the story at awkward moments, especially if it's in the middle of sentence or prompt format relevant tokens, which can really throw some models off, especially in chat mode where we rely on proper formatting a lot
        # ghostbox (brilliant) solution (gbs): use metadata in the story history to semantically determine good cut-off points. usually, this is after an AI message, since those are more often closing-the-action than otherwise. Use template files to ensure syntactic correctness (e.g. no split prompt format tokens).
        # The problem with this (gbs) is that it is not making as good use of the cache, since if an earlier part of the prompt changed everything after it gets invalidated. However prompt eval is the easy part. Clearly though, there is a trade off. Maybe give user a choice between slds and gbs as trade off between efficiency vs quality?
        # Also: this is currently still quite dumb, actually, since we don't take any semantics into account
        # honorable mention of degenerate cases: If the system_msg is longer than the context itself, or users pull similar jokes, it is ok to shit the bed and let the backend truncate the prompt.
        self._smartShifted = False  # debugging
        backend = self.getBackend()

        w = self.showStory() + hint
        gamma = self.getOption("max_context_length")
        k = self.getSystemTokenCount()
        n = self.getOption("max_length")
        # budget is the number of tokens we may spend on story history
        budget = gamma - (k + n)

        if budget < 0:
            # shit the bed
            return self.showSystem() + w

        if not (self.getOption("smart_context")):
            # currently we just dump FIXME: make this version keep the system prompt at least
            return self.showSystem() + w

        # now we need a smart story history that fits into budget
        sf = self.session.stories.copyFolder(only_active=True)
        while len(
            backend.tokenize(self.showStory(story_folder=sf) + hint)
        ) > budget and not (sf.empty()):

            # drop some items from the story, smartly, and without changing original
            self._smartShifted = True
            item = sf.get().pop(0)
            # FIXME: this can be way better, needs moretesting!
        return self.showSystem() + self.showStory(story_folder=sf) + hint

    def adjustForChat(self, w):
        """Takes user input w and returns a pair (w1, v) where w1 is the modified user input and v is a hint for the AI to be put at the end of the prompt."""
        v = ""
        if self.getMode() == "chat":
            w = mkChatPrompt(self.getOption("chat_user")) + w
            v = mkChatPrompt(self.session.getVar("chat_ai"), space=False)
        return (w, v)

    def communicate(self, prompt_text, stream_callback=None):
        """Sends prompt_text to the backend and returns results."""
        backend = self.getBackend()
        payload = self.makeGeneratePayload(prompt_text)
        self._lastPrompt = prompt_text
        if self.getOption("warn_trailing_space"):
            if prompt_text.endswith(" "):
                printerr(
                    "warning: Prompt ends with a trailing space. This messes with tokenization, and can cause the model to start its responses with emoticons. If this is what you want, you can turn off this warning by setting 'warn_trailing_space' to False."
                )

        # FIXME: this is only necessary until llamacpp implements streaming tool calls
        if self.getOption("stream") and not ("tools" in payload.keys()):
            # FIXME: this is the last hacky bit about formatting
            if self.getOption("chat_show_ai_prompt") and self.getMode().startswith(
                "chat"
            ):
                self.print(
                    self.session.getVar("chat_ai") + ": ",
                    end="",
                    flush=True,
                    interrupt=False,
                )
            if backend.generateStreaming(
                payload,
                lambda token, only_once=uuid.uuid4(): self._streamCallback(
                    token, user_callback=stream_callback, only_once=only_once
                ),
            ):
                printerr("error: " + backend.getLastError())
                return ""
            backend.waitForStream()
            self.setLastJSON(backend.getLastJSON())
            return self.flushStreamQueue()
        else:
            result = backend.handleGenerateResult(backend.generate(payload))
            self.setLastJSON(backend.getLastJSON())

        if not (result):
            printerr(
                "error: Backend yielded no result. Reason: " + backend.getLastError()
            )
            self.verbose(
                "Additional information (last request): \n"
                + json.dumps(backend.getLastRequest(), indent=4)
            )
            return ""
        return result

    def interact(
        self,
        w: str,
        user_generation_callback=None,
        generation_callback=None,
        stream_callback=None,
        blocking=False,
        timeout=None,
    ) -> None:
        """This is as close to a main loop as we'll get. w may be any string or user input, which is sent to the backend. Generation(s) are then received and handled. Certain conditions may cause multiple generations. Strings returned from the backend are passed to generation_callback.
        :param w: Any input string, which will be processed and may be modified, eventually being passed to the backend.
        :param generation_callback: A function that takes a string as input. This will receive generations as they arrive from the backend. Note that a different callback handles streaming responses. If streaming is enabled and this function prints, you will print twice. Hint: check for getOption('stream') in the lambda.
        :return: Nothing. Use the callback to process the results of an interaction, or use interactBlocking.
        """
        # internal state does not change during an interaction
        if generation_callback is None:
            generation_callback = self._print_generation_callback

        self.freeze()

        def loop_interact(w):
            self._stop_generation.clear()
            communicating = True
            self._busy.set()
            (modified_w, hint) = self.modifyInput(w)
            self.addUserText(modified_w)
            while communicating:
                # this only runs more than once if there is auto-activation, e.g. with tool use
                if (generated_w := self.communicate(
                    self.buildPrompt(hint), stream_callback=stream_callback
                )) == "":
                    # empty string usually means something went wrong.
                    # communicate will have already printed to stderr
                    # it is important that we don't loop forever here though, so we bail
                    communicating = False



                # if the generated string has tool calls, we apply them here
                tool_results, tool_call = self.applyTools(
                    generated_w, json=self.lastResult
                )
                
                output = ""
                if tool_results != []:
                    # need to add both the calls and result to the history
                    self.session.stories.get().addMessages([tool_call] + tool_results)
                    (w, hint) = ("", "")
                    if self.getOption("verbose"):
                        output = json.dumps(tool_results)
                else:
                    if self.getMode() != "chat":
                        output = self.addAIText(generated_w)
                    else:
                        # formatting is a bit broken in chat mode. Actually chat mode is a bit broken
                        output = generated_w
                    communicating = False

            # done communicating
            if user_generation_callback is not None:
                user_generation_callback(output)
            generation_callback(output)
            self.unfreeze()
            self._lastInteraction = time_ms()
            self._busy.clear()            

        t = threading.Thread(target=loop_interact, args=[w])
        t.start()
        if blocking:
            t.join(timeout=timeout)
        if self.getOption("log_time"):
            printerr(showTime(self, []))

    def interactBlocking(self, w: str, timeout=None) -> str:
        temp = ""

        def f(v):
            nonlocal temp
            temp = v

        self.interact(w, user_generation_callback=f, timeout=timeout, blocking=True)
        return temp

    def _stopInteraction(self):
        """Stops an ongoing generation, regardless of wether it is streaming, blocking, or async.
        This is a low level function. Consider using stopAll instead.
        Note: Currently only works for streaming generations."""
        streaming.stop_streaming.set()
        self._stop_generation.set()

    def stopAll(self):
        """Stops ongoing generation and interrupts the TTS.
        The side effects of stopping generation depend on the method used, i.e. streaming generation will yield some partially generated results, while a blocking generation may be entirely discarded.
        Note: Currently only works for streaming generations."""
        self._stopInteraction()
        self.stopTTS()

    def hasImages(self):
        return bool(self.images) and self._images_dirty

    def loadImage(self, url, id):
        url = os.path.expanduser(url.strip())
        if not (os.path.isfile(url)):
            printerr("warning: Could not load image '" + url + "'. File not found.")
            return

        self.images[id] = {"url": url, "data": loadImageData(url)}
        self._images_dirty = True

    def setLastJSON(self, json_result):
        self.lastResult = json_result
        try:
            current_tokens = str(self.lastResult["usage"]["total_tokens"])
        except:
            # fail silently, it's not that important
            current_tokens = "?"
            
        self.session.setVar("current_tokens", current_tokens)

    def backup(self):
        """Returns a data structure that can be restored to return to a previous state of the program."""
        # copy strings etc., avoid copying high resource stuff or tricky things, like models and subprocesses
        return (self.session.copy(), copy.deepcopy(self.options))

    def restore(self, backup):
        (session, options) = backup
        self.session = session
        for k, v in options.items():
            self.setOption(k, v)

    def _stopHTTP(self):
        self.http_server.close()

    def _initializeHTTP(self):
        """Spawns a simple web server on its own thread.
        This will only serve the html/ folder included in ghostbox, along with the js files. This includes a minimal UI, and capabilities for streaming TTS audio and transcribing from user microphone input.
        Note: By default, this method will override terminal, audio and tts websock addresses. Use --no-http_override to suppress this behaviour.
        """
        import http.server
        import socketserver

        host, port = self.getOption("http_host"), self.getOption("http_port")

        # take care of terminal, audio and tts
        if self.getOption("http_override"):
            config = {
                "audio_websock_host": host,
                "audio_websock_port": port + 1,
                "audio_websock": True,
                "tts_websock_host": host,
                "tts_websock_port": port + 2,
                "tts_websock": True,
                "websock_host": host,
                "websock_port": port + 100,
                "websock": True,
            }

            for opt, value in config.items():
                self.setOption(opt, value)

        handler = partial(
            http.server.SimpleHTTPRequestHandler,
            directory=ghostbox.get_ghostbox_html_path(),
        )

        def httpLoop():
            with socketserver.TCPServer((host, port), handler) as httpd:
                printerr(
                    f"Starting HTTP server. Visit http://{host}:{port} for the web interface."
                )
                self.http_server = httpd
                httpd.serve_forever()

        self.http_thread = threading.Thread(target=httpLoop)
        self.http_thread.daemon = True
        self.http_thread.start()

    def initializeWebsock(self):
        """Starts a simple websocket server that sends and receives messages, behaving like a terminal client."""
        printerr("Initializing websocket server.")
        self.websock_server_running.set()
        self.websock_thread = threading.Thread(
            target=self._runWebsockServer, daemon=True
        )
        self.websock_thread.start()

        self.websock_regpl_thread = threading.Thread(
            target=regpl, args=[self, self._websockPopMsg], daemon=True
        )
        self.websock_regpl_thread.start()

        # handle printerr
        # FIXME: currently, we are not buffering while we send to a websock server, oh well
        util.printerr_callback = lambda w: self.websockSend(self.stderr_token + w)

    def stopWebsock(self):
        printerr("Stopping websocket server.")
        self.websock_running.clear()
        self.websock_clients = []
        self._printerr_buffer = ["Resetting stderr buffer."]
        util.printerr_callback = self._initial_printerr_callback

    def websockSend(self, msg: str) -> None:
        from websockets import ConnectionClosedError

        for i in range(len(self.websock_clients)):
            client = self.websock_clients[i]
            try:
                client.send(msg)
            except ConnectionClosedError:
                printerr(
                    "error: Unable to send message to "
                    + str(client.remote_address)
                    + ": Connection closed."
                )
                del self.websock_clients[i]

    def _websockPopMsg(self) -> str:
        """Pops a message of the internal websocket queue and returns it.
        This function blocks until a message is found on the queue."""
        while self.websock_server_running.is_set():
            try:
                return self.websock_msg_queue.get(timeout=1)
            except Empty:
                # timeout was hit
                # future me: don't remove this: get will super block all OS signals so we have to occasionally loop around or program will be unresponsive
                continue
            except:
                print(
                    "Exception caught while blocking. Shutting down gracefully. Below is the full exception."
                )
                printerr(traceback.format_exc())
                self.stopWebsock()

    def _runWebsockServer(self):
        import websockets
        import websockets.sync.server as WS

        def handler(websocket):
            remote_address = websocket.remote_address
            # printerr("[WEBSOCK] Got connection from " + str(remote_address))
            # update the new client with console log
            for msg in self._printerr_buffer:
                try:
                    websocket.send(self.stderr_token + msg)
                except ConnectionClosedError:
                    printerr(
                        "error: Connection with "
                        + str(remote_address)
                        + " closed during initial history update."
                    )
                    return

            self.websock_clients.append(websocket)
            try:
                while self.websock_server_running.is_set():
                    msg = websocket.recv()
                    if type(msg) == str:
                        self.websock_msg_queue.put(msg)
            except websockets.exceptions.ConnectionClosed:
                pass
            finally:
                self.websock_clients.remove(websocket)

        self.websock_server_running.set()
        self.websock_server = WS.serve(
            handler, self.getOption("websock_host"), self.getOption("websock_port")
        )
        printerr(
            "WebSocket server running on ws://"
            + self.getOption("websock_host")
            + ":"
            + str(self.getOption("websock_port"))
        )
        self.websock_server.serve_forever()

    def _startCLIPrinter(self) -> None:
        """Checks wether we are busy. If we are busy and then aren't, we print the CLI prompt."""
        # we don't want to use the self.print, nor printerr for this as both of them have side effects
        # this really is just for terminal users
        print_cli = lambda prefix: print(
            prefix + self.showCLIPrompt(), file=sys.stderr, end="", flush=True
        )

        def cli_printer():
            while self.running:
                self._busy.wait()
                while self._busy.is_set():
                    # don't blow up potato cpu
                    # user can wait for their coveted cli for 10ms
                    time.sleep(0.1)
                    continue

                if self.getOption("quiet"):
                    continue
                
                # bingo
                if self._triggered:
                    # this means someone wrote a command etc so user pressed enter and we don't need a newline
                    print_cli(prefix="")
                    self._triggered = False
                    continue

                # AI generated a bunch of text. there may or may not be a newline. llama actually has this in the result json but we roll our own
                if self._stdout_ringbuffer.endswith("\n"):
                    print_cli()
                else:
                    print_cli(prefix="\n")

        self._cli_printer_thread = threading.Thread(target=cli_printer, daemon=True)
        self._cli_printer_thread.start()

    def triggerCLI(self, check: Optional[str]=None) -> None:
        """Signals that the user has pressed enter, usually executing a command.
        The entire purpose of this is to track wether we need to print a newline before printing the CLI prompt.
        :param check: If none, is checked for a newline at the end, determining wether to print a newline."""
        if check is None:
            self._triggered = True
        else:
            self._triggered = True if check.endswith("\n") else False
            
        # off to the races lulz
        self._busy.set()
        self._busy.clear()


def main():
    just_fix_windows_console()
    tagged_parser = makeTaggedParser(backends.default_params)
    parser = tagged_parser.get_parser()
    args = parser.parse_args()
    prog = Plumbing(
        options=args.__dict__,
        initial_cli_prompt=args.cli_prompt,
        tags=tagged_parser.get_tags(),
    )
    setup_plumbing(prog, args)

    if (prompt := prog.getOption("prompt")) is not None:

        def input_once():
            prog.running = False
            return prompt

        regpl(prog, input_function=input_once)
    else:
        regpl(prog)
    

def setup_plumbing(prog: Plumbing, args: Namespace=Namespace()) -> None:
    # the following is setup, though it is subtly different from Plumbing.init, so beware        
    if userConfigFile():
        prog.setOption("user_config", userConfigFile())
        printerr(loadConfig(prog, [userConfigFile()], override=False))

    if prog.getOption("config_file"):
        printerr(loadConfig(prog, [prog.options["config_file"]]))


    try:
        # this can fail if called from api 
        if "-u" in sys.argv or "--chat_user" in sys.argv:
            prog.setOption("chat_user", args.chat_user)
    except AttributeError:
        # we don't have arg.xxx -> fine
        pass
    
    if prog.getOption("character_folder"):
        printerr(newSession(prog, []))

    if prog.getOption("hide"):
        hide(prog, [])

    if prog.getOption("tts"):
        prog.tts_flag = True
        
    if prog.getOption("image_watch"):
        del prog.options["image_watch"]
        prog.setOption("image_watch", True)


    if prog.getOption("http"):
        del prog.options["http"]
        prog.setOption("http", True)
        
        
    if prog.getOption("websock"):
        del prog.options["websock"]
        prog.setOption("websock", True)


    if prog.getOption("audio"):
        del prog.options["audio"]
        prog.setOption("audio", True)


        
def regpl(prog: Plumbing, input_function: Callable[[], str] = input) -> None:
    """Read user input, evaluate, generate LLM response, print loop."""
    skip = False
    prog.triggerCLI()
    prog._busy.clear()


    # check if someone started the cli without a char and try to be helpful
    if not(prog.getOption("character_folder")):
        printerr("warning: Running ghostbox without a character folder. Use `/start <charname>` to actually load an AI character.\n  For example `/start ghostbox-helper` will load a helpful AI that can explain ghostbox to you.\n  Other choices are: " + ", ".join(all_chars(prog)))
        
    while prog.running:
        last_state = prog.backup()
        try:
            # have to do TTS here for complex reasons; flag means to reinitialize tts, which can happen e.g. due to voice change
            if prog.tts_flag:
                prog.tts_flag = False
                prog.options["tts"] = False
                printerr(toggleTTS(prog, []))

            if prog.initial_print_flag:
                prog.initial_print_flag = False
                print("\n\n" + prog.formatStory(), end="")

            w = input_function()

            # check for multiline
            # this works different wether we have multiline mode enabled, or are doing ad-hoc multilines
            if prog.getOption("multiline"):
                # multiline mode
                if w != prog.getOption("multiline_delimiter"):
                    prog.bufferMultilineInput(w)
                    continue
                else:
                    # :-1 for trailing newline
                    w = prog.flushMultilineBuffer()[:-1]
            else:
                # ad hoc multilines
                if w.endswith("\\") and not (w.endswith("\\\\")):
                    prog.bufferMultilineInput(w)
                    continue
                elif prog.isMultilineBuffering():
                    w = prog.flushMultilineBuffer() + w

            # for convenience when chatting
            if w == "":
                # New: changed from
                # w = "/cont"
                # to the below because /cont wasn't used much and continue doesn't work very well with OAI API which is getting more prevalent
                prog.stopAll()
                prog.triggerCLI()
                continue

            # expand session vars, so we can do e.g. /tokenize {{system_msg}}
            w = prog.session.expandVars(w)

            for cmd, f in cmds:
                # FIXME: the startswith is dicey because it now makes the order of cmds defined above relevant, i.e. longer commands must be specified before shorter ones.
                if w.startswith(cmd):
                    # execute a / command
                    v = f(prog, w.split(" ")[1:])
                    printerr(v)
                    prog.triggerCLI(check=v)
                    if not (prog.isContinue()):
                        # skip means we don't send a prompt this iteration, which we don't want to do when user issues a command, except for the /continue command
                        skip = True
                    break  # need this to not accidentally execute multiple commands like /tts and /ttsdebug

            if skip:
                skip = False
                continue

            # this is the main event

            # for CLI use, we want a new generation to first stop all ongoing generation and TTS
            prog.stopAll()
            prog.interact(w, generation_callback=prog._print_generation_callback)

            prog.resetContinue()
        except KeyboardInterrupt:
            prog.running = False
            sys.exit
        except:
            printerr("error: Caught unhandled exception in main()")
            printerr(traceback.format_exc())
            try:
                prog.restore(last_state)
            except:
                printerr(
                    "error: While trying to recover from an exception, another exception was encountered. This is very bad."
                )
                printerr(traceback.format_exc())
                printerr(saveStoryFolder(prog, []))
                sys.exit()
            printerr("Restored previous state.")


if __name__ == "__main__":
    main()

```

## output_formatter.py

```python
from abc import ABC, abstractmethod
from functools import *
from ghostbox.util import *
from colorama import Fore, Style

class OutputFormatter(ABC):
    """An interface for formatting the chat history. The OutputFormatter interface is used both for formatted text that is presented to the user, as well as formatting text to send to the backend, e.g. to prepend 'Bob:' style chat prompts.
OutputFormatters work on both AI and human text and ensure a certain style, without restricting the LLM's grammar outright. Because the LLM will pick up on your style to varying degrees, depending on various factors, the following law must hold for OutputFormatters:

    ```
f = OutPutFormatter()
f.format(w) == f.format(f.format(w))
```

for all strings w. In other words, f is idempotent under repeated application. We might get a string 'How are you, Sally?' which the formatter turns into 'Bob: How are you, Sally?', but we don't want this to turn into 'Bob: Bob: How are you, Sally?' in case the LLM starts to prepend 'Bob: ' itself."""

    @abstractmethod
    def format(self, w):
        pass

    def compose(xs):
        """Takes a list of objects supporting the OutputFormatter interface and returns a ComposedFormatter that is the result of their composition."""
        return reduce(ComposedFormatter, xs, IdentityFormatter())

    def __add__(self, other):
        return ComposedFormatter(self, other)
    
    def sequence(xs, w):
        """Takes a list of OutputFormatters xs and applies them to w in sequence. Tip: Read the list from left to right and imagine applying each formatter to the string in succession."""
        return reduce(lambda v, f: f.format(v), xs, w)
    
class IdentityFormatter(OutputFormatter):
    """This formatter returns its input unchanged."""
    def __init__(self):
        pass
    
    def format(self, w):
        return w

class ComposedFormatter(OutputFormatter):
    """Allows composition of two given OutputFormatters.
In general, the following laws hold

```
a = OutputFormatter() #i.e. a has superclass OutputFormatter
e = IdentityFormatter()
    c = ComposedFormatter(a, e)

c.format(w) == a.format(w)
    ```

    for any given string w. In other words, IdentityOutputFormatter is the identity element for composition of formatters. Also

    ```
    a = OutputFormatter()
    b = OutputFormatter()
    c1 = ComposedFormatter(a, b)
    c2 = ComposedFormatter(b, a)

    c1.format(w) != c2.format(w)
    ```

    at least not for all a and b. So in general, composition of formatters does not commute."""

    def __init__(self, a, b):
        self.a = a
        self.b = b

    def format(self, w):
        return self.a.format(self.b.format(w))
    
class NicknameFormatter(OutputFormatter):
    """This formatter prepends chat names, according to a given decorator. By default, names are decorated with colon, as in 'Bob: '."""
    
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self.decorator_func=decorator_func
        self.nickname = nickname

    def setNickname(self, nickname):
        self.nickname = nickname
        return self
        
    def format(self, w):
        def f(v, line, nick=self.decorator_func(self.nickname)):
            if line.startswith(nick):
                return v + "\n" + line
            else:
                return v + "\n" + nick + line
        return reduce(f, w.split("\n"), "")[1:]

    def unformat(self, w):
        def f(v, line, nick=self.decorator_func(self.nickname)):
            if line.startswith(nick):
                return v + "\n" + line[len(nick):]
            else:
                return v + "\n" + line
        return reduce(f, w.split("\n"), "")[1:]

class NicknameRemover(OutputFormatter):
    """Removes prepended nicknames according to a supplied decorator. By default, this removes stuff like 'Bob: ', and it matches it a little fuzzy to account for spaces."""
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self._f = NicknameFormatter(nickname, decorator_func)
        
    def format(self, w):
        return self._f.unformat(w)
    
class IncompleteSentenceCleaner(OutputFormatter):
    """Removes incomplete sentences at the end of text."""

    def __init__(self,     stopchars = '! . ? ; `'.split(" ")):
        self.stopchars = stopchars
        
    def format(self, w):
        if w == "":
            return w


        exclusions = []
        # exclude stuff like '1.', '2.' etc
        exclusions += list(re.finditer(r"\d\.", w))

        skip = False
        stopchars = self.stopchars
        for i in range(len(w)-1, -1, -1):
            for match in exclusions:
                if i > match.start() and i < match.end():
                    skip = True

            if skip:
                skip = False
                continue
            if w[i] in stopchars:
                break

        if i == 0:
            return ""
        return w[:i+1]

class WhitespaceFormatter(OutputFormatter):
    """Removes whitespace at the beginning and end of text."""

    def format(self, w):
        return w.strip()
    
class LonelyPromptCleaner(OutputFormatter):
    """Removes lonely occurrences of trailing chat prompts. Like trailing 'Bob: ' etc, since these are hard to spot with incomplete sentence cleaning. You can supply another decorator besides the default '<nick>: ' pattern. Also note that cleaning is a little fuzzy when it comes to spaces."""
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self.decorator_func = decorator_func
        self.nickname = nickname

    def setNickname(self, nickname):
        self.nickname = nickname
        return self
        
    def format(self, w):
        prompt = self.decorator_func(self.nickname)
        ws = w.split("\n")
        return "\n".join(filter(lambda v: not(v in prompt), ws))


class ChatFormatter(OutputFormatter):
    """General purpose chat formatter with some extra functionality. Will prepend nicknames according to a decorator, and cleans up some bogus responses."""
    def __init__(self, nickname, decorator_func=lambda w: w + ": "):
        self.decorator_func = decorator_func
        self.nickname = nickname

    def setNickname(self, nickname):
        self.nickname = nickname

    def format(self, w):
        return OutputFormatter.sequence([IncompleteSentenceCleaner(),
                                         LonelyPromptCleaner(self.nickname, decorator_func=self.decorator_func),
                                         NicknameFormatter(self.nickname, decorator_func=self.decorator_func)],
                                        w)


class ColorFormatter(OutputFormatter):
    """Adds a given color and style to the output. Color and style can be provided as strings, e.g. 'red' and 'bright'm which will be converted to ANSI terminal codes."""
    def __init__(self, color, style="default"):
        super().__init__()
        self.color = stringToColor(color)
        self.style = stringToStyle(style)

    def format(self, w):
        return wrapColorStyle(w, self.color, self.style)
        
DoNothing = IdentityFormatter() # more descriptive name                
DefaultResponseCleaner = OutputFormatter.compose([IncompleteSentenceCleaner()])
CleanResponse = DefaultResponseCleaner
DefaultResponseCleaner.__doc__ =     """Does minimum cleanup. Intended for AI responses."""


    

```

## pftemplate.py

```python
import os, glob
from abc import ABC, abstractmethod
from functools import *
from ghostbox.util import *

class PFTemplate(ABC):
    """Abstract base class for prompt format templates, used to turn Story objects into properly formatted strings."""

    @abstractmethod
    def header(self, system_msg, **kwargs):
        pass

    @abstractmethod
    def body(self, story, append_hint=True, **kwargs):
        pass

    @abstractmethod
    def strip(self, w):
        pass

    @abstractmethod
    def stops(self):
        """Returns a list of strings that may stop generation. This is intended for EOS delimiters, like <|im_end|> etc."""
        pass
    
class FilePFTemplate(PFTemplate):
    """Simple, customizable prompt format templates based on loading dictionaries with certain files.

Files expected:
    system - Will be prepended to every prompt. It should contain '{{system_msg}}', which will be replaced by the actual content of the system prompt when the header method is called.
    begin_user - Contains string that will be prepended to user messages. Be sure to include newlines, if you want them
    end_user - Contains string that will be appended to user message.
    begin_assistant - Contains string that will be prepended to generated AI message. This may be the same as begin_user, or it may differ.
    end_assistant - Contains string that will be appended to the generated AI message.
    stop_lines - Contains strings that will cause generation to stop, seperated by newlines
    hint - Contains a special string that is sometimes appended at the end of a user message. It should contain a string that guides the AI's compleetion, e.g. this may just be '<|im_start|>assistant\n' in the case of chat-ml, which will heavily discourage the LLM from speaking for the user. This is only appended when append_hint is True in the body method.

The following files are optional:
    begin_system - Special tokens to be prepended to a system message within the chat history, e.g. a tool or function call result. This will be prepended to message with role=system in the chat history.
    end_system - Special tokens to be appended to a system message within the chat history, e.g. a tool or function call result. This will be appended to message with role=system in the chat history.
    If optional files are missing but their conditions are met (e.g. a message  with role=system), the template will default to something reasonable (e.g. begin_assistant)

All methods accept additional **kwargs, which contain replacements for double curly braced strings in the story content and system message. Things like '{{char_name}}' etc.
Example:
    ```
from ghostbox.Story import *
s = Story()
s.addUserText("The {{adjective}}, brown fox jumps over the lazy hedgehog!")
t = FilePFTemplate("templates/chat-ml")
print(t.body(s, append_hint=True, adjective="quick"))
```

Output:
    
```
The quick, brown fox jumps over the lazy hedgehog!<|im_end|><|im_start|>assistant

```
"""    

    var_decorator = lambda w: "{{" + w + "}}"
    
    def __init__(self, dir):
        self.dir = dir
        self._loadFiles()

    def _loadFiles(self):
        if not(os.path.isdir(self.dir)):
            raise FileNotFoundError("Could not find path " + self.dir)

        allfiles = glob.glob(self.dir + "/*")
        for filepath in allfiles:
            filename = os.path.split(filepath)[1]
            if os.path.isfile(filepath):
                self.__dict__[filename] = open(filepath, "r").read()
                
    def header(self, system_msg, **kwargs):
        return replaceFromDict(self.system.replace("{{system_msg}}", system_msg), kwargs, key_func=FilePFTemplate.var_decorator)

    def body(self, story, append_hint=True, **kwargs):
        def build(w, item):
            # you could do this more modular but why? this way users see the files and the template scheme is obvious. I bet this covers 99% of actual use cases for LLM
            content = replaceFromDict(item.content, kwargs, key_func=FilePFTemplate.var_decorator)
            if item.role == "user":
                return w + self.begin_user + content + self.end_user
            elif item.role == "assistant":
                return w + self.begin_assistant + content + self.end_assistant
            elif item.role == "system":
                # this is a bit more hairy
                begin = self.__dict__.get("begin_system", self.begin_assistant)
                end = self.__dict__.get("end_system", self.end_assistant)
                return w + begin + content + end
            # throw if people use weird or no roles with this template
            raise ValueError(item.role + " is not a valid role for this template.")
        if append_hint:
            hint = replaceFromDict(self.hint, kwargs, key_func=FilePFTemplate.var_decorator)
        else:
            hint = ""
        return reduce(build, story.getData(), "") + hint
    def stops(self):
        return self.stop_lines.split("\n")
        
    
    def strip(self, w):
        #FXIME: only preliminary for testing like this
        targets = [self.begin_user, self.begin_assistant, self.end_user, self.end_assistant]
        return reduce(lambda v, target: v.replace(target, ""), targets, w)
        


class RawTemplate(PFTemplate):
    """This is a dummy template that doesn't do anything. Perfect if you want to experiment."""
    def header(self, system_msg, **kwargs):
        return system_msg


    def body(self, story, append_hint=True, **kwargs):
        return "\n".join([item.content for item in story.getData()
                          if type(item.content) == str])
            
    def stops(self):
        return []
    
    def strip(self, w):
        return w

```

## session.py

```python
import os, glob
from copy import deepcopy
from ghostbox.util import *
from ghostbox.StoryFolder import *
from ghostbox.agency import *

class Session(object):
    special_files = "chat_ai config.json tools.py".split(" ") 
    
    def __init__(self, dir=None, chat_user="", chat_ai="", additional_keys=[], tools_forbidden=[]):
        self.dir = dir
        self.fileVars = {"chat_user" : chat_user, "chat_ai" : chat_ai, "system_msg" : "", "current_tokens": "0"}
        self.stories = StoryFolder()
        self.tools = []
        self.tools_file = ""
        self.tools_module = None
        if self.dir is not None:
            self._init(additional_keys, tools_forbidden)

    def copy(self):
        # can't deepcopy a module
        tmp = self.tools_module
        self.tools_module = None
        new = deepcopy(self)
        # module becomes a singleton
        self.tools_module = tmp
        new.tools_module = tmp
        return new
    
        
            
    def merge(self, other):
        """Merges some things from a session object other into itself. This generally means keeping story etc, of self, but possibly adding fileVars from other, including overriding our own."""
        self.fileVars = self.fileVars | other.fileVars
        self.dir = other.dir
            
    def hasVar(self, var):
        return var in self.fileVars
    
    def getVar(self, var, default=None):            
        if var not in self.fileVars:
            if default is None:
                printerr("warning: session.getVar: Key not defined '" + var + "'. Did you forget to create " + self.dir + "/" + var + "?")
                return ""                
            else:
                return default
        return self.expandVars(self.fileVars[var])

    def setVar(self, name, value):
        self.fileVars[name] = value
    
    def getVars(self):
        return {k : self.expandVars(v) for (k, v) in self.fileVars.items()}
    
    def getSystem(self):
        return self.getVar("system_msg")


    def expandVars(self, w, depth=3):
        """Expands all variables of the form {{VAR}} in a given string w, if VAR is a key in fileVars. By default, will recursively expand replacements to a depth of 3."""
        for i in range(0, depth):
            w_new = replaceFromDict(w, self.fileVars, lambda k: "{{" + k + "}}")
            if w == w_new:
                break
            w = w_new
        return w_new
    
        
    
    def _init(self, additional_keys=[], tools_forbidden=[]):
        if not(os.path.isdir(self.dir)):
            raise FileNotFoundError("Could not find path " + self.dir)

        allfiles = glob.glob(self.dir + "/*") + additional_keys
        for filepath in allfiles:
            filename = os.path.split(filepath)[1]
            if os.path.isfile(filepath) and filename not in self.special_files:
                #self.fileVars[filename] = self.expandVars(open(filepath, "r").read())
                self.fileVars[filename] = open(filepath, "r").read()
                # this is useful but too verbose
                #printerr("Found " + filename)
            elif filename == "tools.py":
                self.tool_file = filepath                
                (self.tools, self.tools_module) = makeTools(filepath, display_name=os.path.basename(self.dir) + "_tools", tools_forbidden=tools_forbidden)



        init_msg = self.getVar("initial_msg", "")
        if init_msg:
            self.stories.get().addAssistantText(init_msg)

    def callTool(self, name, params):
        if name not in [tool.function.name for tool in self.tools]:
            return
        try:
            f = getattr(self.tools_module, name)
        except:
            printerr("warning: Failed to call tool '" + name + "': Not found in module '" + self.tools_file + "'.")
            printerr(traceback.format_exc())
            return

        # we have to build a function call somewhat laboriously because the order of arguments is not guaranteed        
        try:
            pargs = [params[arg] for arg in getPositionalArguments(f)]
        except KeyError:
            printerr("warning: Couldn't call tool '" + name + "': Required positional parameter missing.")
            printerr(traceback.format_exc())
            return
        
        kwargs = {k : v for (k, v) in params.items() if k in getOptionalArguments(f)}

        # here goes nothing
        try:
            result = f(*pargs, **kwargs)
        except:
            printerr("warning: Caught exception when calling tool '" + name + "'. Here's a dump of the arguments:")
            printerr(json.dumps(params, indent=4))
            printerr("\nAnd here is the full exception:")
            printerr(traceback.format_exc())
            return
        return result
    

            


```

## StoryFolder.py

```python
import jsonpickle, copy
from ghostbox.Story import *

class StoryFolder(object):
    """Thin wrapper around a list of Story objects."""
    def __init__(self, json_data=None):
        self.stories = [Story()]
        self.index = 0 # points to where to append next
        if json_data:
            self.stories = jsonpickle.loads(json_data) # throw if illegal json
            # FIXME: this will crash and burn if json is bogus, but oh well

    def empty(self):
        return self.stories[self.index] == []
            
    def get(self):
        return self.stories[self.index]
    
    def newStory(self):
        self.stories.append(Story())
        self.index = len(self.stories) - 1

    def reset(self) -> None:
        """Reset storyfolder to an empty state."""
        self.stories = [Story()]
        self.index = 0
        
    def cloneStory(self, index=-1):
        if index == -1:
            # -1  means currrent story
            index = self.index

        l = len(self.stories)
        if index >= 0 and index < l:
            self.stories.append(copy.deepcopy(self.stories[index]))
            self.index = l

    def copyFolder(self, only_active=False):
        sf = StoryFolder()
        if only_active:
            sf.stories = copy.deepcopy(self.stories[self.index:self.index+1])
        else:
            sf.stories = copy.deepcopy(self.stories)
            sf.index = self.index
        return sf
        
    def _shiftStory(self, i):
        l = len(self.stories)
        newIndex = self.index + i
        if newIndex >= l:
            return 1

        if newIndex < 0:
            return -1

        self.index = newIndex
        return 0

    def nextStory(self):
        return self._shiftStory(1)

    def previousStory(self):
        return self._shiftStory(-1)
    
    def toJSON(self):
        return jsonpickle.dumps(self.stories)
    
    def shiftTo(self, i):
        l = len(self.stories)
        if i >= l or i < 0:
            return "Index out of range."
        self.index = i
        return ""
    

```

## Story.py

```python
from typing import *
from pydantic import BaseModel
from ghostbox.definitions import *

class Story(object):
    """A story is a thin wrapper around a list of ChatMessages."""
    
    data: List[ChatMessage] = [] 

    def addUserText(self, w: str, image_id:Optional[int]=None, **kwargs) -> None:
        new_data = ChatMessage(role = "user", content = w, image_id=image_id, **kwargs)
        self.data.append(new_data)

    def addAssistantText(self, w: str, **kwargs):
        self.data.append(ChatMessage(role="assistant", content= w, **kwargs))

    def addRawJSON(self, json: Dict[str, Any]) -> None:
        """Try to parse a raw json dictionary as a ChatMessage and then append it to the story.
        This will throw if the parsing fails."""
        self.data.append(ChatMessage(**json))
        
    def addRawJSONs(self, json_list: List[Dict[str, Any]]) -> None:
        """Add one or more python dictionaries that will be interpreted as ChatMessages and appended to the story.
                         If any of the passed dictionaries don't conform to the ChatMessage schema, you will get a pydantic ValidationError."""
        self.data.extend([ChatMessage(**item)
                          for item in json_list])

    def addMessage(self, msg: ChatMessage) -> None:
        """Appends a chat message to the story."""
        self.data.append(msg)
        
    def addMessages(self, msgs: List[ChatMessage]) -> None:
        """Appends ChatMessages to the story."""
        self.data.extend(msgs)
        
    def addSystemText(self, w: str, **kwargs) -> None:
        self.data.append(ChatMessage(role="system", content= w, **kwargs))

    def extendAssistantText(self, w: str) -> None:
        """Alters the latest found message in the story that is by the assistant, and extends it with w. If no such message exists, it adds w as an assistant message to the story."""
        for i in range(-1, -1*(len(self.data)+1), -1):
            # go through msgs from back to front
            msg = self.data[i]
            if msg.role == "assistant":
                if msg.content is None:
                    # this case is too weird, we just skip empty content
                    continue
                elif type(msg) == str:
                    # easy case
                    msg.content += w
                    return
                else:
                    # the content is complex -> a list of images + text or smth
                    for cmsg in msg.content:
                        if cmsg.type == "text":
                            cmsg.content += w
                            return

        # if we reached this point, no assistant was found
        # we just append a new message
        self.addAssistantText(w)
                

                
                

            
    def extendLast(self, w:str) -> None:
        """Appends w to the last message. Does nothing if there are no messages."""
        if self.data == []:
            return
        
        msg = self.data[-1]
        if msg.content is None:
            # tricky case, I say we do nothing
            return
        elif type(msg.content) == str:
            msg.content += w
        else:
            # the message is complex
            for cmsg in msg.content:
                # this is a truly weird case, I guess we mirror the behaviour of extend assistant and just extend the first text field
                if cmsg.type == "text":
                    cmsg.content += w
                    return
                
            
            
    def getData(self) -> List[ChatMessage]:
        return self.data

    def drop(self, n:int=-1) -> bool:
        """Safely remove the nth story item from the story. Defaults to the last item. If there are no elements, of if n is out of range, this has no effect. Returns True if an item was removed."""
        if self.data == []:
            return False

        if n == -1:
            n = len(self.data) - 1
        
        if not(n in range(0, len(self.data))):
            return False
        self.pop(n)
        return True
    
    
    def pop(self, n:int=-1) -> ChatMessage:
        """Remove and return the last story item. If n is supplied, item at position n is removed and returned."""
        return self.data.pop(n)
            
    def dropUntil(self, predicate: Callable[[ChatMessage], bool]) -> bool:
        """Drops chat messages from the back of the list until predicate is True. Predicate takes a ChatMessage as argument. Returns true if predicate was true for an item."""
        while self.data != []:
            if predicate(self.data[-1]):
                return True
            self.pop()
        return False
        
    def to_json(self) -> List[Dict[str, Any]]:
        """Returns internal data as json models.
        Shorthand for mapping model_dump over a getData() call."""
        return [msg.model_dump() for msg in self.data]

```

## streaming.py

```python
import requests, json
#from requests_html import HTMLSession
from time import sleep
from threading import Thread, Event
from ghostbox.util import printerr

# FIXME:   # poor man's closure; somehow this isn't enough yet to warrant making a class
stop_streaming = Event()

def connect_to_endpoint(url, prompt, headers=""):
    try:
        #session = HTMLSession()
        session = requests.Session()
        r = session.post(url, json=prompt, stream=True, headers=headers)
        return r
    except Exception as e:
        printerr(f"Error connecting to {url}: {e}")
        return None



  
    
def process_sse_streaming_events(callback, done_flag, r):
    global stop_streaming
    for event in r.iter_lines():
        if stop_streaming.is_set():
            # FIXME: this being global will be an issue :/
            break
            
        if event:
            w = event.decode()
            if w == "data: [DONE]":
                # openai do this
                #break
                pass
            elif w.startswith("data: "):
                d = json.loads(w[6:])            
                callback(d)
            else:
                # this works usually if people aren't actually streaming, but maybe we should just crash
                printerr("warning: Malformed data in process_sse_streaming_events. Are you actually streaming?")
                d = json.loads(w)            
                callback(d)                

                
    done_flag.set()


def streamPrompt(callback, done_flag, url, json="", headers=""):
    global stop_streaming
    stop_streaming.clear()
    
    response = connect_to_endpoint(url, json, headers=headers)
    if response:
        thread = Thread(target=process_sse_streaming_events, args=(callback, done_flag, response))
        thread.start()
    return response

```

## transcribe.py

```python
import whisper
import time, os, sys, contextlib, threading
import wave
import tempfile
from ctypes import *
import pyaudio
from pydub import pyaudioop
import audioop
import math
from collections import deque
from queue import Queue
import websockets.sync.server as WS
import websockets
import numpy as np
from ghostbox.util import printerr


def loadModel(name="base.en"):
    return whisper.load_model(name)

def getWhisperTranscription(filename, model):
    result = model.transcribe(filename, fp16=False)
    return result["text"].strip()

# unfortunately pyaudio will give a bunch of error messages, which is very irritating for using it in a shell program, so we supress the msgs
@contextlib.contextmanager
def ignoreStderr():
    devnull = os.open(os.devnull, os.O_WRONLY)
    old_stderr = os.dup(2)
    sys.stderr.flush()
    os.dup2(devnull, 2)
    os.close(devnull)
    try:
        yield
    finally:
        os.dup2(old_stderr, 2)
        os.close(old_stderr)

class WhisperTranscriber(object):
    def __init__(self, model_name="base.en", silence_threshold=2500, input_func=None):
        """model_name is the name of a whisper model, e.g. 'base.en' or 'tiny.en'.
        silence_threshold is an integer value describing a decibel threshold at which recording starts in the case of continuous transcribing.
input_func is a 0-argument function or None. If not None, it is called before transcribing, though only with the one-shot 'transcribe' method, not with transcribeContinuously. You can use this to print to stdout, or play a sound or do anything to signal to the user that recording has started."""
        self.model = loadModel(model_name)
        self.silence_threshold = silence_threshold
        self.input_func = input_func

    def transcribeWithPrompt(self, input_msg="", input_func=None, input_handler=lambda w: w):
        """Records audio directly from the microphone and then transcribes it to text using Whisper, returning that transcription.
input_msg - String that will be shown at the prompt.
input_func - 0-argument callback function that will be called immediately before prompt. This will be called in addition to, and immediately after, WhisperTranscriber.input_func
input_handler - Function that takes the user supplied input string as argument. This is most often unused as the user just presses enter, but sometimes you may use this to check for /quit etc.
        Returns - String signifying the transcript of the recorded audio.
This function will record from the point it is called and until the user hits enter, as per the builtin input() function."""

        # Create a temporary file to store the recorded audio (this will be deleted once we've finished transcription)
        temp_file = tempfile.NamedTemporaryFile(suffix=".wav")

        sample_rate = 16000
        bits_per_sample = 16
        chunk_size = 1024
        audio_format = pyaudio.paInt16
        channels = 1

        def callback(in_data, frame_count, time_info, status):
            wav_file.writeframes(in_data)
            return None, pyaudio.paContinue

        # Open the wave file for writing
        wav_file = wave.open(temp_file.name, 'wb')
        wav_file.setnchannels(channels)
        wav_file.setsampwidth(bits_per_sample // 8)
        wav_file.setframerate(sample_rate)

        # Suppress ALSA warnings (https://stackoverflow.com/a/13453192)
        ERROR_HANDLER_FUNC = CFUNCTYPE(None, c_char_p, c_int, c_char_p, c_int, c_char_p)
        def py_error_handler(filename, line, function, err, fmt):
            return

        c_error_handler = ERROR_HANDLER_FUNC(py_error_handler)
        asound = cdll.LoadLibrary('libasound.so')
        asound.snd_lib_error_set_handler(c_error_handler)

        # Initialize PyAudio
        audio = None
        with ignoreStderr():
            audio = pyaudio.PyAudio()

        # Start recording audio
        stream = audio.open(format=audio_format,
                            channels=channels,
                            rate=sample_rate,
                            input=True,
                            frames_per_buffer=chunk_size,
                            stream_callback=callback)

        if self.input_func:
            self.input_func()
        if input_func:
            input_func()

        input_handler(input(input_msg))
        # Stop and close the audio stream
        stream.stop_stream()
        stream.close()
        audio.terminate()

        # Close the wave file
        wav_file.close()

        # And transcribe the audio to text (suppressing warnings about running on a CPU)
        result = getWhisperTranscription(temp_file.name, self.model)
        temp_file.close()
        return result

    def transcribeContinuously(self, callback=None, on_threshold=None, websock=False, websock_host="localhost", websock_port=5051):
        """Starts recording continuously, transcribing audio when a given volume threshold is reached.
        This function is non-blocking, but returns a ContinuousTranscriber object, which runs asynchronously and can be polled to get the latest transcription (if any).
        Alternatively or in addition to polling, you can allso supply a callback, which gets called whenever a string is transcribed with the string as argument.
        :param callback: Function taking a string as argument. Gets called on a successful transcription.
        :param on_threshold: A zero argument function that gets called whenever the audio threshold is crossed while recording.
        :param websock: Boolean flag to enable WebSocket recording.
        :param websock_host: The hostname to bind the websocket server to.
        :param websock_port: The listening port for the WebSocket connection.
        :return: A ContinuousTranscriber object."""
        return ContinuousTranscriber(self.model, self.silence_threshold, callback=callback, on_threshold=on_threshold, websock=websock, websock_host=websock_host, websock_port=websock_port)


class ContinuousTranscriber(object):
    def __init__(self, model, silence_threshold, callback=None, on_threshold=None, websock=False, websock_host="localhost", websock_port=5051):
        self.model = model
        self.callback = callback
        self.on_threshold = on_threshold
        self.silence_threshold = silence_threshold
        # sampel rate is in self._samplerate. This is a tricky value, as it gets set by the client in websock mode.
        self._set_samplerate(44100) 
        self.buffer = []
        self.running = False
        self.resume_flag = threading.Event()
        self.payload_flag = threading.Event()
        self.websock = websock
        self.websock_host = websock_host
        self.websock_port = websock_port
        self.websock_server = None
        #self.audio_buffer = b""
        self.audio_buffer = Queue()
        self._spawnThread()
        if self.websock:
            self._setup_websocket_server()                    

    def _handle_client(self, websocket):
        while self.running:
            try:
                packet = websocket.recv(1024)
                if type(packet) == str:
                    w = packet
                    if w.startswith("samplerate:"):
                        #printerr("[DEBUG] Setting " + w)
                        self._set_samplerate(int(w.split(":")[1]))
                    elif w == "END":
                        self.running = False
                        self.resume_flag.set()
                else:
                    #self.audio_buffer += packet
                    # FIXME: using queue here means chunks might be < 1024, we could use a bytearray in this loop to buffer until we have a chunk
                    # this only happens on buffer underrun though, e.g. during high network latency. It's ok to fail transcribing in such cases, this should be handled by record_on_detect
                    self.audio_buffer.put(packet)
            except websockets.exceptions.ConnectionClosed:
                self.running = False
                self.resume_flag.set()

                
    def _setup_websocket_server(self):
        def run_server():
            printerr("Starting websock server for audio transcription.")
            self.websock_server = WS.serve(self._handle_client, host=self.websock_host, port=self.websock_port)
            self.websock_server.serve_forever()

        server_thread = threading.Thread(target=run_server)
        server_thread.start()
                
    def _spawnThread(self):
        self.running = True
        self.resume_flag.set()
        self.payload_flag.clear()
        thread = threading.Thread(target=self._recordLoop, args=())
        thread.start()

    def _recordLoop(self):
        while self.running:
            self.resume_flag.wait()
            temp_file = tempfile.NamedTemporaryFile(suffix=".wav")
            if self.record_on_detect(temp_file.name, silence_threshold=self.silence_threshold):
                continue

            import shutil
            shutil.copy(temp_file.name, "/home/marius/etc/diagnostic.wav")
            self.buffer.append(getWhisperTranscription(temp_file.name, self.model))
            if self.callback:
                self.callback(self.buffer[-1])
            self.payload_flag.set()

    def pause(self):
        self.resume_flag.clear()

    def isPaused(self):
        return not(self.resume_flag.is_set())

    def resume(self):
        self.resume_flag.set()

    def stop(self):
        self.running = False
        self.resume()
        if self.websock and self.websock_server is not None:
            self.websock_server.shutdown()            

    def pop(self):
        """Returns a list of strings that were recorded and transcribed since the last time poll or pop was called.
        This function is non-blocking."""
        tmp = self.buffer
        self.buffer = [] #FIXME: race condition?
        return tmp

    def poll(self):
        """Returns a list of strings that were recorded and transcribed since the last time poll or pop was called.
This function will block until new input is recorded."""
        self.payload_flag.wait()
        self.payload_flag.clear()
        return self.pop()


    def _set_samplerate(self, samplerate):
        self._samplerate = samplerate

    def get_samplerate(self):
        return self._samplerate
        

    def record_on_detect(self, file_name, silence_limit=1, silence_threshold=2500, chunk=1024, prev_audio=1):
        """Records audio from the microphone or WebSocket and saves it to a file.
        Returns False on error or if stopped.
        Silence limit in seconds. The max amount of seconds where
        only silence is recorded. When this time passes the
        recording finishes and the file is delivered.

        The silence threshold intensity that defines silence
        and noise signal (an int. lower than THRESHOLD is silence).
        Previous audio (in seconds) to prepend. When noise
        is detected, how much of previously recorded audio is
        prepended. This helps to prevent chopping the beginning
        of the phrase."""

        rate = self.get_samplerate()
        # FIXME: this is necessary until I find out how to send stereo audio from javascript, otherwise we get chipmunk sound
        CHANNELS = 2 if not(self.websock) else 1
        FORMAT = pyaudio.paInt16
        with ignoreStderr():
            p = pyaudio.PyAudio()
        stream = None
        if not self.websock:
            stream = p.open(format=p.get_format_from_width(2),
                            channels=CHANNELS,
                            rate=rate,
                            input=True,
                            output=False,
                            frames_per_buffer=chunk)
        listen = True
        started = False
        rel = rate / chunk
        frames = []
        prev_audio = deque(maxlen=int(prev_audio * rel))
        slid_window = deque(maxlen=int(silence_limit * rel))
        while listen:
            if not(self.running) or self.isPaused():
                return True

            data = None
            if self.websock:
                data = self.audio_buffer.get()
                #if len(self.audio_buffer) >= chunk:
                    #data = np.frombuffer(self.audio_buffer[:chunk], dtype=np.int16)
                    #self.audio_buffer = self.audio_buffer[chunk:]
            else:
                data = stream.read(chunk)

            if data is not None:
                slid_window.append(math.sqrt(abs(audioop.avg(data, 4))))

            if sum([x > silence_threshold for x in slid_window]) > 0:
                if not started:
                    # recording starts here
                    started = True
                    if self.on_threshold is not None:
                        self.on_threshold()
            elif started:
                started = False
                listen = False
                prev_audio = deque(maxlen=int(0.5 * rel))

            if started and data is not None:
                frames.append(data)
            elif data is not None:
                prev_audio.append(data)

        if not self.websock:
            stream.stop_stream()
            stream.close()
            p.terminate()

        wf = wave.open(file_name, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(pyaudio.get_sample_size(FORMAT))
        wf.setframerate(rate)
        wf.writeframes(b''.join(list(prev_audio)))
        wf.writeframes(b''.join(frames))
        wf.close()
        return False


    #debug notes
    # https://community.openai.com/t/playing-audio-in-js-sent-from-realtime-api/970917/8
    

```

## tts_backends.py

```python
import traceback, appdirs, os, wget
from abc import ABC, abstractmethod
from functools import *
from typing import *
from ghostbox.util import *
from ghostbox.definitions import *

class IgnoreValueError(ValueError):
    def __init__(self, ignored_value, **kwargs):
        super().__init__(**kwargs)
        self.ignored_value = ignored_value
        
        
def assert_downloaded(filepath: str, download_url: str) -> None:
    """Makes sure a file FILEPATH exists, downloading it from DOWNLOAD_URL if necessary."""
    if os.path.isfile(filepath):
        return
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    printerr("Downloading " + os.path.basename(filepath) + " from " + download_url)
    wget.download(download_url, out=filepath)
    printerr("\nSuccessfully saved to " + filepath)
    
import nltk.data
nltk.download('punkt_tab')
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')


class TTSBackend(ABC):
    """Abstract interface to a TTS model, like xtts2 (now derelict), tortoise, or zonos."""

    @abstractmethod
    def __init__(self, config: Dict[str, Any]={}) -> None:
        pass

    @abstractmethod
    def tts_to_file(self, text: str, file_path: str, language:str = "en", speaker_file:str = "") -> None:
        """Given a message, writes the message spoken as audio to a wav file."""
        pass

    @abstractmethod
    def split_into_sentences(self, text:str) -> List[str]:
        """Returns a list of sentences, where a 'sentence' is any string the TTS backend wants to process as a chunk.
        The default implementation splits on common punctuation marks."""
        return tokenizer.tokenize(text)

    @abstractmethod
    def configure(self, **kwargs) -> None:
        """Set parameters specific to a TTS model."""
        pass

    @abstractmethod
    def get_voices(self) -> List[str]:
        """Returns a list of all voices supported by the model.
        This may be empty or inexhaustive for some models, e.g. if files need to be provided for cloning."""
        pass


    @abstractmethod
    def get_config(self) -> Dict[str, Any]:
        pass
def dump_config(backend: TTSBackend) -> List[str]:
    if "config" not in backend.__dict__:
        return []
    return ["    " + key + "\t" + str(value) for key, value in backend.config.items()]


    
        
class XTTSBackend(TTSBackend):
    """Bindings for the xtts2 model, which is currently (2025) in license limbo and should not be used for production purposes.
This immplementation remains here as a reference implementation."""

    def __init__(self, config: Dict[str, Any]={}):
        super().__init__()
        # fail importing these early
        import torch        
        from TTS.api import TTS
        device = "cuda" if torch.cuda.is_available() else "cpu"
        printerr("Using " + device)        
        self.tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)        

    def tts_to_file(self, text: str, file_path: str, language:str = "en", speaker_file:str = "") -> None:
        printerr("`" + text + "`")        
        self.tts.tts_to_file(text=text, speaker_wav=speaker_file, language=language, file_path=file_path)
        
    def split_into_sentences(self, text:str) -> List[str]:
        return self.tts.synthesizer.split_into_sentences(msg)        

    def configure(self, **kwargs) -> None:
        super().configure(**kwargs)

    def get_voices(self) -> List[str]:
        return []

    def get_config(self):
        return {}
    
class ZonosBackend(TTSBackend):
    """Bindings for the zonos v0.1 model. See https://github.com/Zyphra/Zonos"""

    def __init__(self, config: Dict[str, Any]={}):
        super().__init__(config=config)
        self._speakers = {}        
        # default config
        self.config = {"zonos_model": "Zyphra/Zonos-v0.1-transformer",
                       "pitch_std" : 200.0,
                       "seed" : 420}
        self.config |= self.get_default_emotions()
        self._model_fallback = self.config["zonos_model"]
        self.configure(**config)
        self._init()

    def _init(self) -> None:
        printerr("Initializing zonos TTS model " + self.config["zonos_model"] + ".")        
        # fail importing these early
        import torch
        import torchaudio
        from zonos.model import Zonos
        from zonos.conditioning import make_cond_dict
        from zonos.utils import DEFAULT_DEVICE as device
        printerr("Using " + str(device))        
        #self._model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-hybrid", device=device)
        #self._model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device=device)

        try:
            self._model = Zonos.from_pretrained(self.config["zonos_model"], device=device)
        except:
            if self.config["zonos_model"] != self._model_fallback:
                printerr("warning: Couldn't load model. Retrying with fallback. Below is the full traceback.")
                printerr(traceback.format_exc())                            
                self.configure(zonos_model=self._model_fallback)
                self._init()
                return
            printerr("error: Couldn't load model. Panicing, as there is no more fallback. Below is the full traceback. Goodbye.")
            printerr(traceback.format_exc())                            

    def configure(self, **kwargs) -> None:
        for key, value in kwargs.items():
            # we only configure options that are in the default config
            if key not in self.config:
                continue

            # some special cases
            if key == "zonos_model":
                if value == "transformer":
                    self.config["zonos_model"] = "Zyphra/Zonos-v0.1-transformer"
                elif value == "hybrid":
                    self.config["zonos_model"] = "Zyphra/Zonos-v0.1-hybrid"
                else:
                    self.config["zonos_model"] = value
            else:
                self.config[key] = value
        
    def tts_to_file(self, text: str, file_path: str, language:str = "en-us", speaker_file:str = "") -> None:
        import torch
        import torchaudio
        from zonos.conditioning import make_cond_dict
        
        if text == "":
            return
        
        printerr("`" + text + "`")
        
        # we want to support the 'en' code because xtts uses it
        if language == 'en':
            language = "en-us"

        if speaker_file not in self._speakers:
            self._create_speaker(speaker_file)
        speaker = self._speakers[speaker_file]

        torch.manual_seed(self.config["seed"])
        cond_dict = make_cond_dict(text=text, speaker=speaker, language=language, emotion=list(self.get_config_emotions().values()), pitch_std=self.config["pitch_std"])
        conditioning = self._model.prepare_conditioning(cond_dict)
        codes = self._model.generate(conditioning)
        wavs = self._model.autoencoder.decode(codes).cpu()
        torchaudio.save(file_path, wavs[0], self._model.autoencoder.sampling_rate)
        
    def split_into_sentences(self, text:str) -> List[str]:
        ws = super().split_into_sentences(text)
        # debug
        #print(str(ws))
        return ws
    

    def get_default_emotions(self) -> Dict[str, float]:
        names = "happiness sadness disgust fear surprise anger other neutral".split(" ")
        emotions = [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077]
        return dict(list(zip(names, emotions)))
        return 

    def get_config_emotions(self) -> Dict[str, float]:
        names = self.get_default_emotions().keys()
        return {name: self.config[name] for name in names}
    
    def _create_speaker(self, speaker_file):
        import torchaudio        
        wav, sampling_rate = torchaudio.load(speaker_file)
        self._speakers[speaker_file] = self._model.make_speaker_embedding(wav, sampling_rate)

    def get_voices(self) -> List[str]:
        return []


    def get_config(self):
        return self.config
    
class KokoroBackend(TTSBackend):
    """Bindings for the indomitable kokoro tts https://github.com/hexgrad/kokoro ."""


    def __init__(self, config: Dict[str, Any]={}) -> None:
        super().__init__(config=config)
        self._default_onnx_file = "kokoro-v1.0.onnx"
        self._default_voice_file = "voices-v1.0.bin"
        self._init()

    def _init(self) -> None:
        printerr("Initializing kokoro.")
        # import even if unused just to fail early
        import soundfile as sf
        from kokoro_onnx import Kokoro
        # FIXME: ok there is a problem upstream with kokoro and gpu support. problem is it tends to default to CPU
        # we need to make sure that user did
        # pip install kokoro_onnx[gpu]
        # which is annoying, additionally, we require
        #export ONNX_PROVIDER=CUDAExecutionProvider
        # then it will run with cuda, but apparently the latest cuda libs aren't supported. wip.
        # update: it actually works with
        # pacman -S cudnn
        # still, it's all a bit hairy.

        self._model = Kokoro(self._get_onnx_path(), self._get_voice_path())

    def _data_dir(self) -> str:
        return appdirs.AppDirs("ghostbox-tts").user_data_dir

    def _get_onnx_path(self) -> str:
        onnx_path = self._data_dir() + "/" + self._default_onnx_file
        # FIXME: maybe host these yourself
        assert_downloaded(onnx_path, "https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx")
        return onnx_path

    def _get_voice_path(self) -> str:
        voice_file = appdirs.AppDirs("ghostbox-tts").user_data_dir + "/" + self._default_voice_file
        assert_downloaded(voice_file, "https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin")
        return voice_file
    
    def tts_to_file(self, text: str, file_path: str, language:str = "en-us", speaker_file:str = "") -> None:
        """Given a message, writes the message spoken as audio to a wav file."""
        import soundfile as sf
        if text == "":
            return
        
        printerr("`" + text + "`")        
        if language == "en":
            language = "en-us"

        # FIXME: bit of a hack that exists simply because ghostbox always passes the absolute path to voices, which is meaningless for kokoro
        speaker_file = os.path.basename(speaker_file)
        
        try:
            samples, sample_rate = self._model.create(
                text, voice=speaker_file, speed=1.0, lang=language
            )
            sf.write(file_path, samples, sample_rate)
        except ValueError as e:
            # this happens when kokoro doesn't like a string, e.g. "---".
            # rather than filtering all of these, we just throw and continue in the main loop
            raise IgnoreValueError(text)
        except:
            # this happens e.g. when a wrong voice is picked. we exit to avoid infinite loop with the main thread retries.
                printerr(traceback.format_exc())
                sys.exit()

    def split_into_sentences(self, text:str) -> List[str]:
        """Returns a list of sentences, where a 'sentence' is any string the TTS backend wants to process as a chunk.
        The default implementation splits on common punctuation marks."""
        # kokoro can deal with long sentences without degrading output.
        # but it can end up reserving too much vram.
        # I'm not certain of this but I also think that it does a better job the more material it has.
        # so we compromise
        if len(text) > 300:
            return super().split_into_sentences(text)
        return [text.strip()]

    def configure(self, **kwargs) -> None:
        """Set parameters specific to a TTS model."""
        pass

    def get_config(self):
        return {}
    
    def get_voices(self) -> List[str]:
        return self._model.get_voices()
    

```

## tts_output.py

```python
import traceback, threading, wave, time
from abc import ABC, abstractmethod
from functools import *
from typing import *
from queue import Queue, Empty
from ghostbox.util import *
from ghostbox.definitions import *

class TTSOutput(ABC):
    """Manages output of TTS sound."""

    @abstractmethod
    def __init__(self, **kwargs):
        pass

    @abstractmethod
    def enqueue(self, filename: str, volume: float=1.0) -> None:
        """Enqeueu a wave file for playback. Start playback immediately if nothing is playing.
        This function is non-blocking."""
        pass

    def stop(self) -> None:
        """Instantly interrups and stops any ongoing playback. This method is thread safe."""
        pass

    @abstractmethod
    def shutdown(self) -> None:
        """Gracefully shut down output module, finishing playback of all enqueued files.
                Calling any of the methods after this one is undefined behaviour."""
        pass

class DefaultTTSOutput(TTSOutput):
    """Local TTS sound output using pyaudio."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # fail early if pyaudio isn't available
        import pyaudio
        printerr("Using pyaudio for local playback.")
        self.stop_flag = threading.Event()
        self._queue = Queue()
        self.pyaudio = pyaudio.PyAudio()

        def play_worker():
            while self.running or not(self._queue.empty()):
                # don't remove the loop or timeout or else thread will not be terminated through signals
                try:
                    filename = self._queue.get(timeout=1)
                except Empty:
                    continue
                    
                # _play will block until stop is called or playback finishes
                self._play(filename)

        self.running = True
        self.worker = threading.Thread(target=play_worker)
        self.worker.start()

    def enqueue(self, filename, volume: float= 1.0) -> None:
        self.volume = volume

        self._queue.put(filename)
        
    def _play(self, filename: str, volume: float= 1.0) -> None:
        import pyaudio

        wf = wave.open(filename, 'rb')
        chunk = 1024        
        p = self.pyaudio
        stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),
                        channels=wf.getnchannels(),
                        rate=wf.getframerate(),
                        output=True,
                        frames_per_buffer=chunk)


        # Play the audio data
        self.stop_flag.clear()
        while             data := wf.readframes(chunk):
            if self.stop_flag.isSet():
                break
            stream.write(data)

        stream.stop_stream()
        stream.close()

    def stop(self) -> None:
        """Instantly interrupts and stops all playback."""
        self.stop_flag.set()
        self._queue.queue.clear() # yes

    def shutdown(self) -> None:
        """Gracefully shut down output module, finishing playback of all enqueued files."""
        self.running = False
        super().shutdown()
        
class WebsockTTSOutput(TTSOutput):
    def __init__(self, host="localhost", port=5052, **kwargs):
        # fail early        
        import websockets.sync.server
        
        super().__init__(**kwargs)
        self.clients = []
        self.stop_flag = threading.Event()
        self.go_flag = threading.Event()
        self.go_flag.set()
        self.server_running = threading.Event()
        self._queue = Queue()
        self.host = host
        self.port = port
        
        # start websock server
        self.server_thread = threading.Thread(target=self._run_server)
        self.server_thread.daemon = True
        self.server_thread.start()
        printerr("WebSocket TTS output initialized.")


        def play_worker():
            while self.server_running.isSet() or not(self._queue.empty()):
                # don't remove the loop or timeout or else thread will not be terminated through signals
                try:
                    filename = self._queue.get(timeout=1)
                except Empty:
                    continue
             
                # _play will block until stop is called or playback finishes
                self._play(filename)


        # start queue worker
        self.worker_thread = threading.Thread(target=play_worker)
        # deliberately not a daemon so we deal with EOF properly
        self.worker_thread.start()
        
    def _run_server(self):
        import websockets
        import websockets.sync.server as WS

        def handler(websocket):
            remote_address = websocket.remote_address
            printerr("[WEBSOCK] Got connection from " + str(remote_address))
            self.clients.append(websocket)
            try:
                while self.server_running.isSet():
                    msg = websocket.recv()
                    #print(msg)
                    if msg == "done":
                        # current sound has finished playing
                        self.go_flag.set()
                        
            except websockets.exceptions.ConnectionClosed:
                pass
            finally:
                self.clients.remove(websocket)
                # FIXME: this doesn't work with multiple clients. see play()
                self.go_flag.set()
                printerr("[WEBSOCK] Closed connection with " + str(remote_address))                

        self.server_running.set() 
        self.server = WS.serve(handler, self.host, self.port)
        printerr("WebSocket server running on ws://" + self.host + ":" + str(self.port))
        self.server.serve_forever()

    def stop_server(self) -> None:
        self.server_running.clear()
        printerr("Halting websocket server.")


    def enqueue(self, filename: str, volume: float= 1.0) -> None:
        """Send a wave file   over the network, or enqueue it to be sent if busy.
        This method is non-blocking."""
        self._queue.put(filename)
    def _play(self, filename: str, volume: float = 1.0) -> None:
        """Sends a wave file ofer the network to all connected sockets."""
        from websockets import ConnectionClosedError
        printerr("[WEBSOCK] Playing with " + str(len(self.clients)) + " clients.")

        with open(filename, 'rb') as wf:
            # Read the entire file content
            #data = wf.readframes(wf.getnframes())
            data = wf.read()

            for client in self.clients:
                printerr("[WEBSOCK] Playing audio to " + str(client.remote_address) + " with file " + filename)
                try:
                    client.send(data, text=False)
                except ConnectionClosedError:
                        print("[WEBSOCK] error: Unable to send data to " + str(client.remote_address) + ": Connection closed.")
        # we block now, just as if we were playing the sound waiting for it to finish
        self.go_flag.clear()
        while True:
            # FIXME: this doesn't work 100% correctly with multiple clients (it might be ok though), but that is not the intended use case
            if self.go_flag.isSet():
                break
            time.sleep(0.1)

    def stop(self) -> None:
        """Instantly stop playback and clears the queue."""
        self._queue.queue.clear()        
        self.stop_flag.set()
        self.go_flag.clear()
        for client in self.clients:
            client.close()
        printerr("[WEBSOCK] TTS output stopped.")        

    def shutdown(self) -> None:
        """Shuts down the output module gracefully, waiting for playback/sending of all enqueue files to be finished."""
        while not(self._queue.empty()):
            time.sleep(0.1)
        self.stop()
        self._stop_server()
        super().shutdown()



```

## tts.py

```python
#!/usr/bin/env python
import argparse, traceback, sys, tempfile, ast, shutil
from ghostbox.definitions import TTSOutputMethod, TTSModel
from ghostbox.tts_util import *
from ghostbox.tts_state import *
from ghostbox.tts_backends import *
from ghostbox.tts_output import *
    
def main():
    program_name = sys.argv[0]
    parser = argparse.ArgumentParser(description= program_name + " - TTS program to consume text from stdin and speak it out/ save it as wav file.")
    #parser.add_argument("-f", '--filepath', type=str, default="", help="Filename to save accumulated spoken lines in. Output is in wav format.")
    parser.add_argument("--voices", action=argparse.BooleanOptionalAction, default=False, help="List all available voices for chosen model, then exit the program.")
    parser.add_argument("-q", "--quiet", action=argparse.BooleanOptionalAction, default=False, help="Do not play any audio.")
    parser.add_argument("-l", "--language", type=str, default="en", help="Language for the TTS output. Not all TTS models support all language, and many don't need this option.")
    parser.add_argument("-p", "--pause_duration", type=int, default=1, help="Duration of pauses after newlines. A value of 0 means no or minimal-duration pause.")
    parser.add_argument("-y", "--voice_sample", type=str, default="cloning.wav", help="Path to wav file used as a voice sample to clone.")
    parser.add_argument("-i", "--volume", type=float, default=1.0, help="Volume for the voice playback.")
    parser.add_argument("-s", "--seed", type=int, default=420, help="Random seed for voice models that use it.")
    parser.add_argument("--sound_dir", type=str, default="sounds", help="Directory where sound files are located to be played with #sound <SNDNAME>")
    parser.add_argument("-m", "--model", type=str, choices=[tm.name for tm in TTSModel], default=TTSModel.zonos.name, help="Text-to-speech model to use.")
    parser.add_argument("-o", "--output-method", type=str, choices=[om.name for om in TTSOutputMethod], default=TTSOutputMethod.default.name, help="How to play the generated speech.")
    parser.add_argument("--websock-host", type=str, default="localhost", help="The hostname to bind to when using websock as output method.")
    parser.add_argument("--websock-port", type=int, default=5052, help="The port to listen on for connections when using websock as output method.")
    # zonos specific
    parser.add_argument("--zonos_model", type=str, default="hybrid", help="The pretrained checkpoint to use with the Zonos TTS engine. Try picking 'transformer' or 'hybrid' for good defaults, otherwise consult the zonos project for more checkpoints. Tip: Hybrid seems to give better results than transformer, but requires the mamba-ssm and flash-attn pip packages and doesn't work on all GPUs.")
    args = parser.parse_args()


    import time, threading, os

    def initTTS(model: str, config: Dict[str, Any] = {}) -> TTSBackend:
        if model == TTSModel.xtts.name:
            return XTTSBackend()
        elif model == TTSModel.zonos.name:
            return ZonosBackend(config=config)
        elif model == TTSModel.kokoro.name:
            return KokoroBackend(config=config)
        raise ValueError("Not a valid TTS model: " + model + ". Valid choices are " + "\n  ".join([tm.name for tm in TTSModel]))

    def initOutputMethod(method: str, args) -> TTSOutput:
        if method == TTSOutputMethod.default.name:
            return DefaultTTSOutput()
        elif method == TTSOutputMethod.websock.name:
            return WebsockTTSOutput(host=args.websock_host, port=args.websock_port)
        raise ValueError("Not a valid output method: " + method + ". Valid choices are " + "\n  ".join([om.name for om in TTSOutputMethod]))

    # initialization happens here
    prog = TTSState(args)
    output_module = initOutputMethod(prog.args.output_method, prog.args)
    tts = initTTS(prog.args.model, config=vars(prog.args))
    # we have to put something on the message queue that signals EOF but isn't actually EOF
    # the tokens are only for the msg_queue, and can on principle never be a user request, since we strip leading and trailing whitespace
    eof_token = "  <EOF>  "
    silence_token = "  <silence>  "
    # thisis different from e.g. <clear>, which is currently the only special string that might actullay be user input. oh well

    # list voices if requested
    if args.voices:
        for voice_name in tts.get_voices():
            print(voice_name)
        sys.exit()


    config_options = dump_config(tts)
    if config_options != []:
        printerr("Dumping TTS config options. Set them with '/<OPTION> <VALUE>'. /ls to list again.")
        for w in config_options:
            printerr(w)

    from queue import Queue, Empty
    msg_queue = Queue()
    done = threading.Event()

    def input_loop():
        global done
        global prog
        global tts
        global output_module

        while True:
            try:
                w = input()
                if w == "<clear>":
                    with msg_queue.mutex:
                        output_module.stop()                                    
                        msg_queue.queue.clear()
                    prog.clearRetries()
                    continue
                elif w.strip() == "":
                    msg_queue.put(silence_token)
                elif w.startswith("/"):
                    vs = w[1:].split(" ")
                    option = vs[0]
                    if option == "ls":
                        for u in dump_config(tts):
                            printerr(u)
                        continue
                    elif option in tts.get_config().keys():
                        try:
                            value = ast.literal_eval(" ".join(vs[1:]))
                        except:
                            printerr("Couldn't set config option '" + vs[0] + "'. Error in value literal?")
                            continue
                        tts.configure(**{option:value})
                        continue

                # main event -> speak input msg w
                ws = tts.split_into_sentences(w)
                for chunk in ws:
                    msg_queue.put(chunk)
            except EOFError as e:
                printerr("EOF")
                msg_queue.put(eof_token)
                break
            except:
                print("Exception caught while blocking. Shutting down gracefully. Below is the full exception.")
                print(traceback.format_exc())                    
                time.sleep(3)
                done.set()
                break


    t = threading.Thread(target=input_loop)
    t.daemon = True
    t.start()


    # this is so muggels know to type stuff when they accidentally run ghostbox-tts standalone
    printerr("Good to go. Reading messages from standard input. Hint: Type stuff and it will be spoken.")
    while True:
        # here we handle text chunks that were placed on the msg_queue
        if done.is_set():
            break

        try:
            if prog.isRetrying():
                rawmsg = prog.popRetryMSG()
            else:
                # so fun fact
                # Queue.get blocks. you knew that, ofc
                # but did you know that it super blocks? that's right - it refuses to handle any signals send to the application, including sigint and sigkill
                # so we have to sporadically use a timeout and loop around. btw all of this is undocumented.
                # Thanks, Guido!
                rawmsg = msg_queue.get(timeout=1)
                if rawmsg == eof_token:
                    done.set()
                    continue
                elif rawmsg == silence_token:
                    if not(prog.args.quiet):
                        output_module.enqueue(prog.silence_filename())
                    continue
        except Empty:
            # timeout was hit
            continue
        except: #EOFError as e:
            print("Exception caught while blocking. Shutting down gracefully. Below is the full exception.")
            print(traceback.format_exc())        
            #prog.handleMixins()
            #print("EOF encountered. Closing up.")
            time.sleep(3)
            os._exit(1)

        (msg, cont, err) = prog.processMsg(rawmsg)
        print(err)
        if cont:
            continue

        output_file = prog.temp_wav_file()
        try:
            tts.tts_to_file(text=msg, speaker_file=prog.getVoiceSampleFile(), file_path=output_file.name)
        except IgnoreValueError as e:
            # this happens on some bad values that are hard to filter but harmless.
            # e.g. "---" for text in kokoro
            printerr("Ignored `" + e.ignored_value + "`.")
            continue
        except ZeroDivisionError:
            print("Caught zero division error. Ignoring.")
            # this happens when the tts is asked to process whitespace and produces a wav file in 0 seconds :) nothing to worry about
            continue
        except AssertionError as e:
            print(str(e) + "\nwarning: Caught assertion error on msg: " + msg)
            prog.retry(msg)
            continue # we retry the msg that was too long

        if prog.args.quiet:
            continue

        # this filecopy is horrible but it is necessary because
        # even with ful program synchronization, the filesystem might not play ball
        # in any case without this, the wavefile created on this thread wouldn't show up on the other one
        newfilename = tempfile.mkstemp(suffix=".wav")[1]
        shutil.copy(output_file.name, newfilename)

        # queue and play
        output_module.enqueue(newfilename, volume=prog.args.volume)




    # this will let all enqueued files finish playing
    output_module.shutdown()
    prog.cleanup()

if __name__ == "__main__":
    main()
    

```

## tts_state.py

```python
import os, subprocess, tempfile
from moviepy.editor import *
import ghostbox
from ghostbox.tts_util import *
from queue import Queue

class TTSState(object):
    def __init__(self, args):
        self.args = args
        if args.model == "xtts":
            self._default_samplerate = "24000"
        else:
            self._default_samplerate = "44100"
            # FIXME: accumulation temporarily disabled
            args.filepath = ""
        if args.filepath != "":
            # user wants to keep acc file
            self._keep_acc = True
        else:
            self._keep_acc = False
        self.accfile = getAccumulatorFile(args.filepath)
        self.accfile.close()
        self._empty_filename = ghostbox.get_ghostbox_data("empty." + self._default_samplerate + ".wav")
        self._silence_filename = ghostbox.get_ghostbox_data("silence.1." + self._default_samplerate + ".wav")
        subprocess.run(["cp", self._empty_filename, self.accfile.name])        
        self.tmpfile = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        self.tmpfile.close()
        self.mixins = [] # list of (musicfilename, timestampe)
        self.retry_queue = Queue()
        self.temp_wav_files = []
        
        self.tagFunctions = {
            "sound" : self._soundTag,
            "mixin" : self._mixinTag}

    def temp_wav_file(self):
        f = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        # without fsync the file isn't guaranteed to be available to other threads
        # even if you .close() it
        # python is so great it just works /s
        os.fsync(f)
        f.close()
        self.temp_wav_files.append(f)
        return f

    def silence_filename(self) -> str:
        return self._silence_filename
    
    def cleanup(self):
        for tf in self.temp_wav_files:
            os.remove(tf.name)
        
        os.remove(self.tmpfile.name)
        if not(self._keep_acc):
            os.remove(self.accfile.name)
            
        
        
    def getVoiceSampleFile(self):
        return self.args.voice_sample
        
    def processMsg(self, msg):
        # returns (newMsg, continue_bool, errormsg)

        if msg == "":
            return ("", True, "")

        msg = fixDivZero(msg)
        
        (tag, tagArgs) = maybeGetTag(msg)
        if tag in self.tagFunctions:
            return self.tagFunctions[tag](tagArgs)

        return (msg, False, "")

    def _soundTag(self, argv):
        if argv == []:
            return ("", True, "")

        sndFile = self.args.sound_dir + "/" + " ".join(argv)
        if not(os.path.isfile(sndFile)):
            return ("", False, "Warning: Could not find sound file: " + sndFile)

        self.accumulateSound(sndFile)
        return ("", True, "")

    def _mixinTag(self, argv):
        if argv == []:
            return ("", True, "")        

        mixfile = argv[0]
        clip = AudioFileClip(self.accfile)
        self.mixins.append((mixfile, clip.duration))
        return ("", True, "")

    def handleMixins(self):
        # adds background music etc. to the accumulated file
        clips = [AudioFileClip(self.accfile.name)]
        for (mixfile, timestamp) in self.mixins:
            clips.append(AudioFileClip(mixfile).with_start(timestamp))

        outclip = CompositeAudioClip(clips)
        # this is due to a bug (I think) in moviepy with fps not being defined
        if "fps" not in outclip.__dict__:
            outclip.fps = 44100
        outclip.write_audiofile(self.tmpfile.name)
        subprocess.run(["cp", self.tmpfile.name, self.accfile.name])        
        
    def accumulateSound(self, sndFile):
        subprocess.run(["sox", self.accfile.name, sndFile, self.tmpfile.name])
        subprocess.run(["cp", self.tmpfile.name, self.accfile.name])
        
    def addPause(self):
        if self.args.pause_duration == 0:
            return

        for n in range(0, self.args.pause_duration):
            self.accumulateSound(self._silence_filename)

    def isRetrying(self):
        return not(self.retry_queue.empty())
                
    def retry(self, msg):
        # called when e.g. a messsage is too long, which we can only know after the fact (thanks api designers)
        # first attempt of fixing is by removing all quotation marks. This seems to confuse the tokenizer, when quotation marks aren'tr balanced
        w = msg.replace('"', "")
        if w != msg:
            self.retry_queue.put(w)
            return
        # most really long run-on sentences are due to many commas. We find a comma in the middle and split the string in two
        ws = msg.split(",")
        if len(ws) > 1:
            i = (len(ws) // 2) - 1
            v1 = ",".join(ws[0:i])
            v2 = ws[i] + "." + ",".join(ws[i:])
            self.retry_queue.put(v1)
            self.retry_queue.put(v2)
            return

        # now the gloves are off. Brutally split the string in half.
        i = len(msg) // 2
        self.retry_queue.put(msg[:i])
        self.retry_queue.put(msg[i:])
        return

    def popRetryMSG(self):
        return self.retry_queue.get()
    
    def clearRetries(self):
        with self.retry_queue.mutex:
            self.retry_queue.queue.clear()
            
        
        

```

## tts_util.py

```python
import subprocess, tempfile, os
import datetime
#from TTS.tts.layers.xtts.tokenizer import split_sentence

def getAccumulatorFile(filepath=""):
    if filepath:
        f = open(filepath, "w")
    else:
        f = tempfile.NamedTemporaryFile(suffix=datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S") + ".wav", delete=False)
    return f

def maybeGetTag(w):
    # returns pair of (tag, tagargs) as (string,. list of strings)
    delim = "#"
    if w.startswith(delim):
        ws = w.split(" ")
        tag = ws[0][len(delim):]
        args = ws[1:]
        return (tag, args)
    return ("", [])
        


    



def dir2(x):
    for k in dir(x):
        if k.startswith("_"):
            continue
        print(str(k))


def fixDivZero(w):
    # so, weirdly, the TTS will crash when it encounters xml tags like <test> or <begin> or really anythingin <> brackets. (division by zero crash)
    # easy fix is just to replace the symbols with spelled out words, since at this point we only care about the spoken part anyway. Fucks up languages other than english, but oh well
    return w.replace("<", " less than ").replace(">", " greater than ")



```

## util.py

```python
import os, getpass, shutil, base64, requests, re, csv, glob, time

# removed tortoise dependency because it will require torch, which import multiprocess, which won't work with renpy
# FIXME: not a big deal because tortoise and all tts are spawned with subprocess. However, we will have to find a better way to get the voices.
# import tortoise.utils.audio

from colorama import Fore, Back, Style
import appdirs
import sys
from functools import *


def getErrorPrefix():
    return " # "


def stringToColor(w):
    """Maps normal strings like 'red' to ANSI control codes using colorama package."""
    w = w.upper()
    for color in Fore.__dict__.keys():
        if w == color:
            return Fore.__dict__[color]
    return Fore.RESET


def stringToStyle(w):
    """Maps normal strings to ANSI control codes for style, like 'bright' etc. using colorama."""
    w = w.upper()
    for s in Style.__dict__.keys():
        if w == s:
            return Style.__dict__[s]
    return Style.RESET_ALL


def wrapColorStyle(w, color, style):
    return style + color + w + Fore.RESET + Style.RESET_ALL


# this can be used to modify printerr behaviour. It can be a function that accepts one argument -> the to be printed string
printerr_callback = None
printerr_disabled = False

def printerr(w, prefix=getErrorPrefix(), color=Fore.GREEN):
    global printerr_disabled
    if printerr_disabled:
        return
    
    if w == "":
        return

    if w.startswith("error:"):
        color = Fore.RED

    if w.startswith("warning:"):
        color = Fore.YELLOW

    # prepend all lines with prefix
    ws = w.split("\n")
    new_w = ("\n" + prefix).join(ws)
    formatted_w = color + prefix + new_w + Fore.RESET
    print(formatted_w, file=sys.stderr)
    if printerr_callback is not None:
        printerr_callback(color + w + Fore.RESET)


def getArgument(argname, argv):
    ws = argv.split(argname)
    if len(ws) < 2:
        return None
    return ws[1].split(" ")[1]


def trimOn(stopword, w):
    return w.split(stopword)[0]


def trimChatUser(chatuser, w):
    if chatuser:
        return trimOn(mkChatPrompt(chatuser), trimOn(mkChatPrompt(chatuser).strip(), w))
    return w


def assertNotStartWith(assertion, w):
    # ensures w doesn't start with assertion
    l = len(assertion)
    if w.startswith(assertion):
        return w[l:]
    return w


def assertStartWith(assertion, w):
    # makes sure w starts with assertion. This is intended for making sure strings start with a chat prompt, i.e. Bob: bla bla bla, without duplicating it, as in Bob: Bob: bla bla
    if not (w.startswith(assertion)):
        return assertion + w
    return w


def mkChatPrompt(username, space=True):
    # turns USERNAME into USERNAME:, or, iuf we decide to change it, maybe <USERNAME> etc.
    if username == "":
        return ""
    if space:
        return username + ": "
    return username + ":"


def ensureColonSpace(usertxt, generatedtxt):
    """So here's the problem: Trailing spaces in the prompt mess with tokenization and force the backend to create emoticons, which isn't always what we want.
        However, for chat mode, trailing a colon (:) means the backend will immediately put a char behind it, which looks ugly. There doesn't seem to be a clean way to fix that, short of retraining the tokenizer. So we let it generate text behind the colon, and then add a space in this step manually. This is complicated by the way we split user and generated text.
        usertxt - User supplied text, which may end in something like 'Gary:'
    generatedtxt - Text generated by backend. This immediately follows usertxt for any given pormpt / backend interaction. It may or may not start with a newline.
        Returns - The new usertxt as a string, possibly with a newline added to it."""
    if generatedtxt.startswith(" "):
        return usertxt

    if usertxt.endswith(":"):
        return usertxt + " "
    return usertxt


def ensureFirstLineSpaceAfterColon(w):
    # yes its a ridiculous name but at least it's descriptive
    if w == "":
        return w

    if len(w) <= 2:
        if w == "::":
            return ": :"
        elif w.endswith(":"):
            return w + " "

    ws = w.split("\n")
    v = ws[0]
    for i in range(0, len(v) - 1):
        if v[i] == ":":
            if v[i + 1] == " ":
                return w
            else:
                ws[0] = v[: i + 1] + " " + v[i + 1 :]
                break
    return "\n".join(ws)


def filterPrompt(prompt, w):
    # filters out prompts like "Assistant: " at the start of a line, which can sometimes be generated by the LLM on their own
    return w.replace("\n" + prompt, "\n")


def filterLonelyPrompt(prompt, w):
    # this will filter out prompts like "Assistant: ", but only if it's the only thing on the line. This can happen after trimming. Also matches the prompt a little fuzzy, since sometimes only part of the prompt remains.
    ws = w.split("\n")
    return "\n".join(filter(lambda v: not (v in prompt), ws))


def discardFirstLine(w):
    return "\n".join(w.split("\n")[1:])


def filterLineBeginsWith(target, w):
    """Returns w with all lines that start with target removed. Matches target a little fuzzy."""
    acc = []
    targets = [target, " " + target, target + " "]
    for line in w.split("\n"):
        dirty = False
        for t in targets:
            if line.startswith(t):
                dirty = True
        if not (dirty):
            acc.append(line)
    return "\n".join(acc)


def saveFile(filename, w, depth=0):
    # saves w in filename, but won't overwrite existing files, appending .new; returns the successful filename, if at all possible
    if depth > 10:
        return ""  # give up

    if os.path.isfile(filename):
        parts = filename.split(".")
        if len(parts) > 1:
            newfilename = ".".join([parts[0], "new"] + parts[1:])
        else:
            newfilename = filename + ".new"
        return saveFile(newfilename, w, depth=depth + 1)

    f = open(filename, "w")
    f.write(w)
    f.flush()
    return filename


def stripLeadingHyphens(w):
    # FIXME: this is hacky
    w = w.split("=")[0]

    if w.startswith("--"):
        return w[2:]

    if w.startswith("-"):
        return w[1:]

    return w


def userConfigFile(force=False):
    # return location of ~/.ghostbox.conf.json, or "" if not found
    userconf = "ghostbox.conf"
    path = appdirs.user_config_dir() + "/" + userconf
    if os.path.isfile(path) or force:
        return path
    return ""


def userCharDir():
    return appdirs.user_data_dir() + "/ghostbox/chars"


def ghostboxdir():
    return appdirs.user_data_dir() + "/ghostbox"


def userTemplateDir():
    return ghostboxdir() + "/templates"


def inputChoice(msg, choices):
    while True:
        w = input(msg)
        if w in choices:
            return w


def userSetup():
    if not (os.path.isfile(userConfigFile())):
        print("Creating config file " + userConfigFile(force=True))
        f = open(userConfigFile(force=True), "w")
        f.write('{\n"chat_user" : "' + getpass.getuser() + '"\n}\n')
        f.flush()

    if not (os.path.isdir(userCharDir())):
        print("Creating char dir " + userCharDir())
        os.makedirs(userCharDir())

    # try copying some example chars
    chars = "dolphin dolphin-kitten joshu minsk scribe command-r".split(" ")
    copyEntity("char", "chars", chars)
    templates = "chat-ml alpaca raw mistral user-assistant-newline vacuna command-r llama3 phi3-instruct".split(
        " "
    )
    copyEntity("template", "templates", templates)


def copyEntity(entitynoun, entitydir, entities):
    chars = entities
    choice = ""
    try:
        for char in chars:
            chardir = ghostboxdir() + "/" + entitydir + "/" + char
            if os.path.isdir(chardir) and choice != "a":
                choice = inputChoice(
                    chardir + " exists. Overwrite? (y/n/a): ", "y n a".split(" ")
                )
                if choice == "n":
                    continue
            print("Installing " + entitynoun + " " + char)
            shutil.copytree(entitydir + "/" + char, chardir, dirs_exist_ok=True)
    except:
        print("Warning: Couldn't copy example " + entitynoun + "s.")


def getJSONGrammar():
    return r"""root   ::= object
value  ::= object | array | string | number | ("true" | "false" | "null") ws

object ::=
  "{" ws (
            string ":" ws value
    ("," ws string ":" ws value)*
  )? "}" ws

array  ::=
  "[" ws (
            value
    ("," ws value)*
  )? "]" ws

string ::=
  "\"" (
    [^"\\] |
    "\\" (["\\/bfnrt] | "u" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes
  )* "\"" ws

number ::= ("-"? ([0-9] | [1-9] [0-9]*)) ("." [0-9]+)? ([eE] [-+]? [0-9]+)? ws

# Optional space: by convention, applied in this grammar after literal chars when allowed
ws ::= ([ \t\n] ws)?
"""


def loadImageData(image_path):
    with open(image_path, "rb") as image_file:
        base64_bytes = base64.b64encode(image_file.read())
        return base64_bytes


def packageImageDataLlamacpp(data_base64, id):
    return {"data": data_base64.decode("utf-8"), "id": id}


# def repackImages(images):
#    """Takes the plumbing.images object and makes sure its base64 encoded."""
#    return [{"data" : data_base64.decode("utf-8"), "id" : id}


def mkImageEmbeddingString(image_id):
    return "[img-" + str(image_id) + "]"


def maybeReadInt(w):
    try:
        n = int(w)
    except:
        return None
    return n


def isImageFile(file):
    # good enuff
    return file.endswith(".png")


def getImageExtension(url, default="png"):
    ws = url.split(".")
    if len(ws) < 2:
        return default
    return ws[-1]


def dirtyGetJSON(url):
    r = requests.get(url)
    if r.status_code == 200:
        return r.json()
    return {}


def replaceFromDict(w, d, key_func=lambda k: k):
    def replace_and_check(v, pair):
        v_new = v.replace(key_func(pair[0]), pair[1])
        if type(v_new) != str:
            return str(v_new)
        return v_new
        
    return reduce(replace_and_check, d.items(), w)


ansi_escape = re.compile(r"\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])")


def stripANSI(w):
    return ansi_escape.sub("", w)


def getLayersFile():
    return appdirs.user_config_dir() + "/llm_layers"


def loadLayersFile():
    """Returns a list of dictionaries, one for each row in the layers file."""
    f = open(getLayersFile(), "r")
    return list(csv.DictReader(filter(lambda row: row[0] != "#", f), delimiter="\t"))


def envFromDict(d):
    """Returns a kstandard shell environment with variables added from the provided dictionary d."""
    return os.environ | {k: str(v) for (k, v) in d.items()}


def explodeIncludeDir(include, extradir):
    """So that we can turn 'char/' into ['/include/path/char/', 'char/'] etc."""
    include = os.path.normpath(include)
    extradir = os.path.normpath(extradir)
    return [include, extradir, include + "/" + extradir]


def ultraglob(include_dirs, specific_dir):
    # everyone hates nested listcomprehensions
    acc = []
    for include_dir in include_dirs:
        acc += [
            glob.glob(dir + "/*")
            for dir in explodeIncludeDir(include_dir, specific_dir)
        ]
    return reduce(lambda xs, ys: xs + ys, acc, [])


def getVoices(prog):
    """Returns a list of strings with voice names for a given Program object."""
    pollyvoices = "Lotte, Maxim, Ayanda, Salli, Ola, Arthur, Ida, Tomoko, Remi, Geraint, Miguel, Elin, Lisa, Giorgio, Marlene, Ines, Kajal, Zhiyu, Zeina, Suvi, Karl, Gwyneth, Joanna, Lucia, Cristiano, Astrid, Andres, Vicki, Mia, Vitoria, Bianca, Chantal, Raveena, Daniel, Amy, Liam, Ruth, Kevin, Brian, Russell, Aria, Matthew, Aditi, Zayd, Dora, Enrique, Hans, Danielle, Hiujin, Carmen, Sofie, Gregory, Ivy, Ewa, Maja, Gabrielle, Nicole, Filiz, Camila, Jacek, Thiago, Justin, Celine, Kazuha, Kendra, Arlet, Ricardo, Mads, Hannah, Mathieu, Lea, Sergio, Hala, Tatyana, Penelope, Naja, Olivia, Ruben, Laura, Takumi, Mizuki, Carla, Conchita, Jan, Kimberly, Liv, Adriano, Lupe, Joey, Pedro, Seoyeon, Emma, Niamh, Stephen".split(
        ", "
    )
    vs = []
    if prog.getOption("tts_program") == "ghostbox-tts-polly":
        for voice in pollyvoices:
            vs.append(voice)
    elif prog.getOption("tts_program") == "ghostbox-tts-tortoise":
        # vs = list(tortoise.utils.audio.get_voices(extra_voice_dirs=list(filter(bool, [prog.getOption("tts_voice_dir")]))))
        # FIXME: find another way to get the list of voices
        vs = []
    else:
        # for file in ultraglob(prog.getOption("include"), prog.getOption("tts_voice_dir")):
        vs = [
            os.path.split(file)[1]
            for file in glob.glob(prog.tryGetAbsVoiceDir() + "/*")
            if os.path.isfile(file)
        ]
    return vs


def time_ms():
    return round(time.time() * 1000)


def compose2(f, g):
    return lambda x: f(g(x))

```

